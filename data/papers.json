[
  {
    "title": "Why we should expect ruthless sociopath ASI",
    "authors": [
      "Steven Byrnes"
    ],
    "organization": "Alignment Forum",
    "abstract": "The conversation begins (Fictional) Optimist: So you expect future artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies? Me: Yup! (Alas.) Optimist: …Despite all the evidence right in front of our eyes from humans and LLMs. Me: Yup! Optimist: OK, well, I’m here to tell you: that is a very specific and strange thing to expect, especially in the absence of any concrete evidence whatsoever. There’s no reason to expect it. If you think that ruthless sociopathy is the “true core nature of intelligence” or whatever, then&nbsp; you should really look at yourself in a mirror and ask yourself where your life went horribly wrong .",
    "url": "https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi",
    "published_date": "2026-02-18T17:28:17+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T01:02:57.574419+00:00"
  },
  {
    "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity",
    "authors": [
      "Tomás Vergara-Browne",
      "Darshan Patil",
      "Ivan Titov",
      "Siva Reddy",
      "Tiago Pimentel",
      "Marius Mosbach"
    ],
    "organization": "arXiv",
    "abstract": "The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model.",
    "url": "http://arxiv.org/abs/2602.15829v1",
    "published_date": "2026-02-17T18:59:39+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.227085+00:00"
  },
  {
    "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety",
    "authors": [
      "Max Springer",
      "Chung Peng Lee",
      "Blossom Metevier",
      "Jane Castleman",
      "Bohdan Turbal",
      "Hayoung Jung",
      "Zeyu Shen",
      "Aleksandra Korolova"
    ],
    "organization": "arXiv",
    "abstract": "Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation.",
    "url": "http://arxiv.org/abs/2602.15799v1",
    "published_date": "2026-02-17T18:39:15+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.227165+00:00"
  },
  {
    "title": "MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis",
    "authors": [
      "Fatemeh Khalvandi",
      "Saadat Izadi",
      "Abdolah Chalechale"
    ],
    "organization": "arXiv",
    "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism.",
    "url": "http://arxiv.org/abs/2602.15740v1",
    "published_date": "2026-02-17T17:15:32+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.227235+00:00"
  },
  {
    "title": "Analyzing coding agent transcripts to upper bound productivity gains from AI agents",
    "authors": [
      "METR"
    ],
    "organization": "METR",
    "abstract": "Introduction Human uplift studies like the one we did in 2025 are becoming more expensive as working without AI becomes increasingly costly. In this post, I investigate whether coding agent transcripts could serve as a cheaper alternative for estimating uplift. I prototyped this using 5305 Claude Code transcripts generated in January 2026 by 7 METR technical staff 1 . I used an LLM judge to estimate how long each task would have taken an experienced software engineer without AI tools, then compared that to the time people actually spent on these tasks to calculate a time savings factor . Takeaways This method estimates a time savings factor of ~1.5x to ~13x on Claude Code-assisted tasks for 7 METR technical staff in January 2026 – though this result comes with substantial caveats.",
    "url": "https://metr.org/notes/2026-02-17-exploratory-transcript-analysis-for-estimating-time-savings-from-coding-agents/",
    "published_date": "2026-02-17T08:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://metr.org/feed.xml",
    "fetched_at": "2026-02-19T01:03:00.322402+00:00"
  },
  {
    "title": "GMAIL: Generative Modality Alignment for generated Image Learning",
    "authors": [
      "Shentong Mo",
      "Sukmin Yun"
    ],
    "organization": "arXiv",
    "abstract": "Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images.",
    "url": "http://arxiv.org/abs/2602.15368v1",
    "published_date": "2026-02-17T05:40:25+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.227665+00:00"
  },
  {
    "title": "Discovering Implicit Large Language Model Alignment Objectives",
    "authors": [
      "Edward Chen",
      "Sanmi Koyejo",
      "Carlos Guestrin"
    ],
    "organization": "arXiv",
    "abstract": "Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of \"unknown unknowns\", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation.",
    "url": "http://arxiv.org/abs/2602.15338v1",
    "published_date": "2026-02-17T03:58:55+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.227720+00:00"
  },
  {
    "title": "Boundary Point Jailbreaking: A new way to break the strongest AI defences",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Introducing an automated attack technique that generates universal jailbreaks against the best defended systems",
    "url": "https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-19T01:03:07.693610+00:00"
  },
  {
    "title": "Will reward-seekers respond to distant incentives?",
    "authors": [
      "Alex Mallen"
    ],
    "organization": "Alignment Forum",
    "abstract": "Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can't prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it's responding to incentives developers don’t control.",
    "url": "https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives",
    "published_date": "2026-02-16T19:35:12+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T01:02:57.574652+00:00"
  },
  {
    "title": "Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition",
    "authors": [
      "Varun Nathan",
      "Shreyas Guha",
      "Ayush Kumar"
    ],
    "organization": "arXiv",
    "abstract": "We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.",
    "url": "http://arxiv.org/abs/2602.14955v1",
    "published_date": "2026-02-16T17:36:05+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.227783+00:00"
  },
  {
    "title": "InferenceX v2: NVIDIA Blackwell Vs AMD vs Hopper - Formerly InferenceMAX",
    "authors": [
      "Dylan Patel"
    ],
    "organization": "SemiAnalysis",
    "abstract": "The Artist Known as InferenceMAX. GB300 NVL72, MI355X, B200, H100, Disaggregated Serving, Wide Expert Parallelism, Large Mixture of Experts, SGLang, vLLM, TRTLLM",
    "url": "https://newsletter.semianalysis.com/p/inferencex-v2-nvidia-blackwell-vs",
    "published_date": "2026-02-16T17:13:11+00:00",
    "source_type": "rss",
    "source_url": "https://newsletter.semianalysis.com/feed",
    "fetched_at": "2026-02-19T01:03:01.623985+00:00"
  },
  {
    "title": "Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution",
    "authors": [
      "Matthew Kowal",
      "Goncalo Paulo",
      "Louis Jaburi",
      "Tom Tseng",
      "Lev E McKinney",
      "Stefan Heimersheim",
      "Aaron David Tucker",
      "Adam Gleave",
      "Kellin Pelrine"
    ],
    "organization": "arXiv",
    "abstract": "As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.",
    "url": "http://arxiv.org/abs/2602.14869v1",
    "published_date": "2026-02-16T16:02:09+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.227985+00:00"
  },
  {
    "title": "Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment",
    "authors": [
      "Elias Malomgré",
      "Pieter Simoens"
    ],
    "organization": "arXiv",
    "abstract": "AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.",
    "url": "http://arxiv.org/abs/2602.14844v1",
    "published_date": "2026-02-16T15:40:10+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.228031+00:00"
  },
  {
    "title": "Alignment Adapter to Improve the Performance of Compressed Deep Learning Models",
    "authors": [
      "Rohit Raj Rai",
      "Abhishek Dhaka",
      "Amit Awekar"
    ],
    "organization": "arXiv",
    "abstract": "Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.",
    "url": "http://arxiv.org/abs/2602.14635v1",
    "published_date": "2026-02-16T10:53:02+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.228117+00:00"
  },
  {
    "title": "Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems",
    "authors": [
      "Furkan Mumcu",
      "Yasin Yilmaz"
    ],
    "organization": "arXiv",
    "abstract": "Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $λ\\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.",
    "url": "http://arxiv.org/abs/2602.14471v1",
    "published_date": "2026-02-16T05:17:58+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.228178+00:00"
  },
  {
    "title": "A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking",
    "authors": [
      "Prithwijit Chowdhury",
      "Ahmad Mustafa",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "organization": "arXiv",
    "abstract": "In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of \"importance\" and \"relevance\" differ for different explanation strategies.",
    "url": "http://arxiv.org/abs/2602.14430v1",
    "published_date": "2026-02-16T03:32:10+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.228399+00:00"
  },
  {
    "title": "“Will reward-seekers respond to distant incentives?” by Alex Mallen",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren't?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.",
    "url": "https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant",
    "published_date": "2026-02-16T00:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-19T01:02:57.140794+00:00"
  },
  {
    "title": "Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms",
    "authors": [
      "Belona Sonna",
      "Alban Grastien"
    ],
    "organization": "arXiv",
    "abstract": "Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.",
    "url": "http://arxiv.org/abs/2602.13985v1",
    "published_date": "2026-02-15T04:27:59+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.228773+00:00"
  },
  {
    "title": "AXRP Episode 48 - Guive Assadi on AI Property Rights",
    "authors": [
      "DanielFilan"
    ],
    "organization": "Alignment Forum",
    "abstract": "YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...",
    "url": "https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights",
    "published_date": "2026-02-15T02:20:55+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T01:02:57.575910+00:00"
  },
  {
    "title": "The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?",
    "authors": [],
    "organization": "Anthropic",
    "abstract": "Research from Anthropic titled 'The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?'. Published 2026-02-15.",
    "url": "https://alignment.anthropic.com/2026/hot-mess-of-ai/",
    "published_date": "2026-02-15T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://alignment.anthropic.com/",
    "fetched_at": "2026-02-19T01:03:02.405815+00:00"
  },
  {
    "title": "GSRM: Generative Speech Reward Model for Speech RLHF",
    "authors": [
      "Maohao Shen",
      "Tejas Jayashankar",
      "Osama Hanna",
      "Naoyuki Kanda",
      "Yancheng Wang",
      "Kateřina Žmolíková",
      "Ruiming Xie",
      "Niko Moritz",
      "Anfeng Xu",
      "Yashesh Gaur",
      "Gregory Wornell",
      "Qing He",
      "Jilong Wu"
    ],
    "organization": "arXiv",
    "abstract": "Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions.",
    "url": "http://arxiv.org/abs/2602.13891v1",
    "published_date": "2026-02-14T21:22:55+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.228837+00:00"
  },
  {
    "title": "Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages",
    "authors": [
      "Somnath Banerjee",
      "Rima Hazra",
      "Animesh Mukherjee"
    ],
    "organization": "arXiv",
    "abstract": "Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality ''transfer'' across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.",
    "url": "http://arxiv.org/abs/2602.13867v1",
    "published_date": "2026-02-14T19:56:40+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.228885+00:00"
  },
  {
    "title": "sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals",
    "authors": [
      "Weixuan Yuan",
      "Zengrui Jin",
      "Yichen Wang",
      "Donglin Xie",
      "Ziyi Ye",
      "Chao Zhang",
      "Xuesong Chen"
    ],
    "organization": "arXiv",
    "abstract": "Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \\texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \\texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \\textit{Demography, Age, Site \\& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \\texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout.",
    "url": "http://arxiv.org/abs/2602.13857v1",
    "published_date": "2026-02-14T19:40:04+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.228977+00:00"
  },
  {
    "title": "Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking",
    "authors": [
      "Zhengmian Hu",
      "Lei Shi",
      "Ritwik Sinha",
      "Justin Grover",
      "David Arbour"
    ],
    "organization": "arXiv",
    "abstract": "Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights.",
    "url": "http://arxiv.org/abs/2602.13852v1",
    "published_date": "2026-02-14T19:22:58+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229029+00:00"
  },
  {
    "title": "Interpretable clustering via optimal multiway-split decision trees",
    "authors": [
      "Hayato Suzuki",
      "Shunnosuke Ikeda",
      "Yuichi Takano"
    ],
    "organization": "arXiv",
    "abstract": "Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability.",
    "url": "http://arxiv.org/abs/2602.13586v1",
    "published_date": "2026-02-14T04:08:52+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229105+00:00"
  },
  {
    "title": "Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment",
    "authors": [
      "Jing Zhao",
      "Ting Zhen",
      "Junwei bao",
      "Hongfei Jiang",
      "Yang song"
    ],
    "organization": "arXiv",
    "abstract": "Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models.",
    "url": "http://arxiv.org/abs/2602.13575v1",
    "published_date": "2026-02-14T03:18:52+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229154+00:00"
  },
  {
    "title": "Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning",
    "authors": [
      "Yanbo Wang",
      "Minzheng Wang",
      "Jian Liang",
      "Lu Wang",
      "Yongcan Yu",
      "Ran He"
    ],
    "organization": "arXiv",
    "abstract": "While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.",
    "url": "http://arxiv.org/abs/2602.13562v1",
    "published_date": "2026-02-14T02:37:36+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229204+00:00"
  },
  {
    "title": "Singular Vectors of Attention Heads Align with Features",
    "authors": [
      "Gabriel Franco",
      "Carson Loughridge",
      "Mark Crovella"
    ],
    "organization": "arXiv",
    "abstract": "Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models.",
    "url": "http://arxiv.org/abs/2602.13524v1",
    "published_date": "2026-02-13T23:30:02+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229249+00:00"
  },
  {
    "title": "Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability",
    "authors": [
      "Ayse Tursucular",
      "Ayush Mohanty",
      "Nazal Mohamed",
      "Nagi Gebraeel"
    ],
    "organization": "arXiv",
    "abstract": "Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model.",
    "url": "http://arxiv.org/abs/2602.13485v1",
    "published_date": "2026-02-13T21:41:52+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229303+00:00"
  },
  {
    "title": "Finding Highly Interpretable Prompt-Specific Circuits in Language Models",
    "authors": [
      "Gabriel Franco",
      "Lucas M. Tassis",
      "Azalea Rohr",
      "Mark Crovella"
    ],
    "organization": "arXiv",
    "abstract": "Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms.",
    "url": "http://arxiv.org/abs/2602.13483v1",
    "published_date": "2026-02-13T21:41:17+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229349+00:00"
  },
  {
    "title": "MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents",
    "authors": [
      "Simon Rosen",
      "Siddarth Singh",
      "Ebenezer Gelo",
      "Helen Sarah Robertson",
      "Ibrahim Suder",
      "Victoria Williams",
      "Benjamin Rosman",
      "Geraud Nangue Tasse",
      "Steven James"
    ],
    "organization": "arXiv",
    "abstract": "Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.",
    "url": "http://arxiv.org/abs/2602.13372v1",
    "published_date": "2026-02-13T15:40:32+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229568+00:00"
  },
  {
    "title": "Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis",
    "authors": [
      "Runzhou Liu",
      "Hailey Weingord",
      "Sejal Mittal",
      "Prakhar Dungarwal",
      "Anusha Nandula",
      "Bo Ni",
      "Samyadeep Basu",
      "Hongjie Chen",
      "Nesreen K. Ahmed",
      "Li Li",
      "Jiayi Zhang",
      "Koustava Goswami",
      "Subhojyoti Mukherjee",
      "Branislav Kveton",
      "Puneet Mathur",
      "Franck Dernoncourt",
      "Yue Zhao",
      "Yu Wang",
      "Ryan A. Rossi",
      "Zhengzhong Tu",
      "Hongru Du"
    ],
    "organization": "arXiv",
    "abstract": "Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators.",
    "url": "http://arxiv.org/abs/2602.13028v1",
    "published_date": "2026-02-13T15:34:32+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229640+00:00"
  },
  {
    "title": "Synaptic Activation and Dual Liquid Dynamics for Interpretable Bio-Inspired Models",
    "authors": [
      "Mónika Farsang",
      "Radu Grosu"
    ],
    "organization": "arXiv",
    "abstract": "In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks' attention, and the robustness of their saliency maps measured by the structural similarity index.",
    "url": "http://arxiv.org/abs/2602.13017v1",
    "published_date": "2026-02-13T15:23:37+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229691+00:00"
  },
  {
    "title": "RGAlign-Rec: Ranking-Guided Alignment for Latent Query Reasoning in Recommendation Systems",
    "authors": [
      "Junhua Liu",
      "Yang Jihao",
      "Cheng Chang",
      "Kunrong LI",
      "Bin Fu",
      "Kwan Hui Lim"
    ],
    "organization": "arXiv",
    "abstract": "Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling \"zero-query\" recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot's Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM's latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3.",
    "url": "http://arxiv.org/abs/2602.12968v1",
    "published_date": "2026-02-13T14:38:02+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229742+00:00"
  },
  {
    "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
    "authors": [
      "Chen Yang",
      "Guangyue Peng",
      "Jiaying Zhu",
      "Ran Le",
      "Ruixiang Feng",
      "Tao Zhang",
      "Xiyun Xu",
      "Yang Song",
      "Yiming Jia",
      "Yuntao Wen",
      "Yunzhi Xu",
      "Zekai Wang",
      "Zhenwei An",
      "Zhicong Sun",
      "Zongchao Chen"
    ],
    "organization": "arXiv",
    "abstract": "We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B.",
    "url": "http://arxiv.org/abs/2602.13367v1",
    "published_date": "2026-02-13T13:10:46+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229800+00:00"
  },
  {
    "title": "GPT-5.2 derives a new result in theoretical physics",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.",
    "url": "https://openai.com/index/new-result-theoretical-physics",
    "published_date": "2026-02-13T11:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-19T01:02:56.575486+00:00"
  },
  {
    "title": "ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning",
    "authors": [
      "Esther Sun",
      "Bo-Hao Su",
      "Abinay Reddy Naini",
      "Shinji Watanabe",
      "Carlos Busso"
    ],
    "organization": "arXiv",
    "abstract": "Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise.",
    "url": "http://arxiv.org/abs/2602.12714v1",
    "published_date": "2026-02-13T08:33:37+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229849+00:00"
  },
  {
    "title": "Measuring Time Horizon using Claude Code and Codex",
    "authors": [
      "METR"
    ],
    "organization": "METR",
    "abstract": "Most of METR’s time horizon measurements are done using two scaffolds: Triframe and ReAct 1 . People sometimes see that we use these two scaffolds and feel skeptical about the validity of our results. Shouldn’t scaffolds like Claude Code and Codex do much better on time horizon given how much work has gone into making those scaffolds better for software engineering? In the past, we have measured the time horizons of a few Anthropic and OpenAI models using Claude Code and Codex 2 . These tests indicated that they’re probably not better than our own scaffolds. I decided to perform a few more checks into this question (on our new task suite, with newer versions of Claude Code and Codex), and compared the time horizon of two models on our scaffolds and their specialized scaffolds.",
    "url": "https://metr.org/notes/2026-02-13-measuring-time-horizon-using-claude-code-and-codex/",
    "published_date": "2026-02-13T08:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://metr.org/feed.xml",
    "fetched_at": "2026-02-19T01:03:00.322517+00:00"
  },
  {
    "title": "Power Interpretable Causal ODE Networks: A Unified Model for Explainable Anomaly Detection and Root Cause Analysis in Power Systems",
    "authors": [
      "Yue Sun",
      "Likai Wang",
      "Rick S. Blum",
      "Parv Venkitasubramaniam"
    ],
    "organization": "arXiv",
    "abstract": "Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.",
    "url": "http://arxiv.org/abs/2602.12592v1",
    "published_date": "2026-02-13T04:06:47+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229902+00:00"
  },
  {
    "title": "models have some pretty funny attractor states",
    "authors": [
      "aryaj"
    ],
    "organization": "LessWrong",
    "abstract": "Research from LessWrong titled 'models have some pretty funny attractor states'. Published 2026-02-12. Authors: aryaj.",
    "url": "https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states",
    "published_date": "2026-02-12T21:14:52.004000+00:00",
    "source_type": "rss",
    "source_url": "https://www.lesswrong.com/graphql",
    "fetched_at": "2026-02-19T01:03:14.364925+00:00"
  },
  {
    "title": "Why Deep Jacobian Spectra Separate: Depth-Induced Scaling and Singular-Vector Alignment",
    "authors": [
      "Nathanaël Haas",
      "François Gatine",
      "Augustin M Cosse",
      "Zied Bouraoui"
    ],
    "organization": "arXiv",
    "abstract": "Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing.",
    "url": "http://arxiv.org/abs/2602.12384v2",
    "published_date": "2026-02-12T20:27:59+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.229954+00:00"
  },
  {
    "title": "Human-like metacognitive skills will reduce LLM slop and aid alignment and capabilities",
    "authors": [
      "Seth Herd"
    ],
    "organization": "Alignment Forum",
    "abstract": "1. Summary and overview LLMs seem to lack metacognitive skills that help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving capabilities in new directions. Better metacognition would reduce LLM errors by catching mistakes, and by managing complex cognition to produce better answers in the first place. This could stabilize or regularize alignment, allowing systems to avoid actions they would not \"endorse on reflection\" (in some functional sense). [1] Better metacognition could also make LLM systems useful for clarifying the conceptual problems of alignment. It would reduce sycophancy, and help LLMs organize the complex thinking necessary for clarifying claims and cruxes in the literature. Without such improvements, collaborating with LLM systems on alignment research could be the median doom-path: slop, not scheming . They are sycophantic, agreeing with their users too much, and produce compelling-but-erroneous \"slop\".",
    "url": "https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid",
    "published_date": "2026-02-12T19:38:50+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T01:02:57.576257+00:00"
  },
  {
    "title": "Abstractive Red-Teaming of Language Model Character",
    "authors": [
      "Nate Rahn",
      "Allison Qi",
      "Avery Griffin",
      "Jonathan Michala",
      "Henry Sleight",
      "Erik Jones"
    ],
    "organization": "arXiv",
    "abstract": "We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. \"The query is in Chinese. The query asks about family roles,\" that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries.",
    "url": "http://arxiv.org/abs/2602.12318v1",
    "published_date": "2026-02-12T18:12:12+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-19T01:03:02.230198+00:00"
  },
  {
    "title": "How do we (more) safely defer to AIs?",
    "authors": [
      "ryan_greenblatt"
    ],
    "organization": "Alignment Forum",
    "abstract": "As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible . [1] Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say \"deferring to AIs\" [2] I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions. [3] If we plan to defer to AIs, I think it's safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.",
    "url": "https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais",
    "published_date": "2026-02-12T16:55:52+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T01:02:57.577598+00:00"
  },
  {
    "title": "Research note: A simpler AI timelines model predicts 99% AI R&D automation in ~2032",
    "authors": [
      "Thomas Kwa"
    ],
    "organization": "Alignment Forum",
    "abstract": "In this post, I describe a simple model for forecasting when AI will automate AI development. It is based on the AI Futures model , but more understandable and robust, and has deliberately conservative assumptions. At current rates of compute growth and algorithmic progress, this model's median prediction is &gt;99% automation of AI R&amp;D in late 2032. Most simulations result in a 1000x to 10,000,000x increase in AI efficiency and 300x-3000x research output by 2035. I therefore suspect that existing trends in compute growth and automation will still produce extremely powerful AI on \"medium\" timelines, even if the full coding automation and superhuman research taste that drive the AIFM's \"fast\" timelines (superintelligence by ~mid-2031) don't happen. Why make this? The AI Futures Model (AIFM) has 33 parameters; this has 8. I previously summarized the AIFM on LessWrong and found it to be very complex.",
    "url": "https://www.alignmentforum.org/posts/uy6B5rEPvcwi55cBK/research-note-a-simpler-ai-timelines-model-predicts-99-ai-r",
    "published_date": "2026-02-12T00:13:34+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-19T01:02:57.577854+00:00"
  },
  {
    "title": "“How do we (more) safely defer to AIs?” by Ryan Greenblatt",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: How can we make AIs aligned and well-elicited on extremely hard to check open ended tasks?. As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible.1 Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say “deferring to AIs”2 I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions.3 If we plan to defer to AIs, I think it's safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.4 For deference to go well, we both need it to be the case that the...",
    "url": "https://blog.redwoodresearch.org/p/how-do-we-more-safely-defer-to-ais",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-19T01:02:57.140850+00:00"
  },
  {
    "title": "International consensus and open questions in AI evaluations",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The International Network for Advanced AI Measurement, Evaluation and Science reflects on recent meeting and looks ahead to the India AI Impact Summit",
    "url": "https://www.aisi.gov.uk/blog/international-ai-network-consensus-and-open-questions",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-19T01:03:07.694026+00:00"
  },
  {
    "title": "What to Expect from AI in 2026: Q&A with William Marcellino",
    "authors": [],
    "organization": "RAND",
    "abstract": "Commentary Feb 12, 2026 RAND's William Marcellino directs the development of AI tools at RAND. He discusses how RAND is using AI, what AI might mean for human productivity, the limits of large language models, and more.",
    "url": "https://www.rand.org/pubs/commentary/2026/02/what-to-expect-from-ai-in-2026.html",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.rand.org/topics/artificial-intelligence.html",
    "fetched_at": "2026-02-19T01:03:08.004305+00:00"
  },
  {
    "title": "The Weapons of Mass Destruction AI Security Gap",
    "authors": [],
    "organization": "RAND",
    "abstract": "Commentary Feb 12, 2026 The AI ecosystem may be too narrowly focused on a single threat model: the “lone wolf virus terrorist.” This emphasis on individual actors could leave state-based and terrorist group threats dangerously under-examined.",
    "url": "https://www.rand.org/pubs/commentary/2026/02/the-weapons-of-mass-destruction-ai-security-gap.html",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.rand.org/topics/artificial-intelligence.html",
    "fetched_at": "2026-02-19T01:03:08.004510+00:00"
  }
]