[
  {
    "title": "A Whistleblower Incentive Program to Enforce U.S. Export Controls",
    "authors": [],
    "organization": "IAPS",
    "abstract": "A Whistleblower Incentive Program to Enforce U.S. Export Controls: \"A program modeled on the successful SEC program would help America overcome its export control enforcement woes.”",
    "url": "https://www.iaps.ai/research/category/Link+Post",
    "published_date": "2026-02-17T19:46:11.585594+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.585598+00:00"
  },
  {
    "title": "How Some of China’s Top AI Thinkers Built Their Own AI Safety Institute",
    "authors": [],
    "organization": "IAPS",
    "abstract": "The emergence of the China AI Safety and Development Association (CnAISDA) is a pivotal moment for China’s frontier AI governance. How it navigates substantial domestic challenges and growing geopolitical tensions will shape conversations on frontier AI risks in China and abroad.",
    "url": "https://www.iaps.ai/research/category/Link+Post",
    "published_date": "2026-02-17T19:46:11.585345+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.585348+00:00"
  },
  {
    "title": "IAPS Researchers React: The US AI Action Plan",
    "authors": [],
    "organization": "IAPS",
    "abstract": "The Trump Administration unveiled its comprehensive AI Action Plan on Wednesday. Experts at the Institute for AI Policy and Strategy reviewed the plan with an eye toward its national security implications. As AI continues to accelerate towards very powerful artificial general intelligence, our researchers discuss promising proposals for addressing critical AGI risks, offer key considerations for government implementation, and explore the plan's gaps and potential solutions.",
    "url": "https://www.iaps.ai/research/category/Blog+Post",
    "published_date": "2026-02-17T19:46:11.584288+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.584292+00:00"
  },
  {
    "title": "Policy Actions for Enabling Cyber Defense Through Differential Access",
    "authors": [],
    "organization": "IAPS",
    "abstract": "In our Differential Access report, we provided a strategic framework to help developers give defenders an advantage by shaping access to AI-powered cyber capabilities. In a new policy memo, we outline government actions that can enable Differential Access and promote AI adoption for cyber defense.",
    "url": "https://www.iaps.ai/research/category/Blog+Post",
    "published_date": "2026-02-17T19:46:11.583731+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.583735+00:00"
  },
  {
    "title": "The Hidden AI Frontier",
    "authors": [],
    "organization": "IAPS",
    "abstract": "The most advanced AI systems remain hidden inside corporate labs for months before public release—creating both America's greatest technological advantage and a serious security vulnerability. IAPS researchers identify critical risks and propose lightweight interventions to lessen the threat.",
    "url": "https://www.iaps.ai/research/category/Link+Post",
    "published_date": "2026-02-17T19:46:11.583235+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.583238+00:00"
  },
  {
    "title": "Compute is a Strategic Resource",
    "authors": [],
    "organization": "IAPS",
    "abstract": "Computational power (“compute”) is a strategic resource in the way that oil and steel production capacity were in the past. Like oil, and like steel production capacity, compute is scarce, controllable, concentrated, and highly economically and militarily useful. Just as oil and steel were and remain strategic resources to some extent, compute is now also a strategic resource of very high importance.",
    "url": "https://www.iaps.ai/research/category/Blog+Post",
    "published_date": "2026-02-17T19:46:11.582972+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.582976+00:00"
  },
  {
    "title": "How AI Chips Are Made",
    "authors": [],
    "organization": "IAPS",
    "abstract": "Adapted from a section of a report by Erich Grunewald and Christopher Phenicie, this blog post introduces the core concepts and background information needed to understand the AI chip-making process.",
    "url": "https://www.iaps.ai/research/how-ai-chips-are-made",
    "published_date": "2026-02-17T19:46:11.582690+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.582694+00:00"
  },
  {
    "title": "Accelerating AI Data Center Security",
    "authors": [],
    "organization": "IAPS",
    "abstract": "AI systems are advancing at breakneck speed and already reshaping markets, geopolitics, and the priorities of governments. Frontier AI systems are developed and deployed using compute clusters of hundreds of thousands of cutting-edge AI chips housed in specialized data centers. These AI data centers are likely tempting targets for sophisticated adversaries like China and Russia, who may seek to steal intellectual property or sabotage AI systems underpinning military, industry, or critical infrastructure projects.",
    "url": "https://www.iaps.ai/research/category/Research+Report",
    "published_date": "2026-02-17T19:46:11.582454+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.582458+00:00"
  },
  {
    "title": "Policy Options for Preserving Chain of Thought Monitorability",
    "authors": [],
    "organization": "IAPS",
    "abstract": "The most advanced AI models produce detailed reasoning steps in human language—known as \"chain of thought\" (CoT)—that provide crucial oversight capabilities for ensuring these systems behave as intended. However, competitive pressures may drive developers toward more efficient but non-monitorable architectures that lack a human-readable CoT. This report presents a framework for determining when coordination mechanisms are needed to preserve CoT monitorability.",
    "url": "https://www.iaps.ai/research/category/Research+Report",
    "published_date": "2026-02-17T19:46:11.582154+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.582158+00:00"
  },
  {
    "title": "The Emergence of Autonomous Cyber Attacks: Analysis and Implications",
    "authors": [],
    "organization": "IAPS",
    "abstract": "In November 2025, Anthropic reported detecting and disrupting one of the first cyber espionage campaign. This appears to be the first publicly known example of AI systems autonomously conducting multi-step attacks against well-defended targets in the wild. This represents a significant step as autonomous offensive AI agents could enable nation-states to conduct continuous operations across multiple targets at an increased tempo, and these autonomous capabilities are likely to proliferate and enable less sophisticated actors to conduct complex operations at faster speeds. This may shift advantages toward attackers until defensive capabilities are deployed at scale.",
    "url": "https://www.iaps.ai/research/autonomous-cyber-attacks",
    "published_date": "2026-02-17T19:46:11.581576+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.581580+00:00"
  },
  {
    "title": "Crucial Considerations in ASI Deterrence",
    "authors": [],
    "organization": "IAPS",
    "abstract": "A new memo by IAPS Associate Researcher Oscar Delaney reviews the emerging “MAIM” (mutual assured AI malfunction) literature and evaluates the strategic dynamics that could shape ASI deterrence.",
    "url": "https://www.iaps.ai/research/crucial-considerations-in-asi-deterrence",
    "published_date": "2026-02-17T19:46:11.581232+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.581237+00:00"
  },
  {
    "title": "Strategic Visions in AI Governance: Mapping Pathways to Victory",
    "authors": [],
    "organization": "IAPS",
    "abstract": "What AI policy objectives should one work towards? This depends greatly on one’sstrategic vision. Strategic visions are high-level views about how to successfully navigate the transition to a world with powerful AI systems. The strategic visions discussed here particularly aim to address three severe risks: takeover by powerful misaligned AI systems, wars resulting from competitive dynamics around AI, and AI-enabled concentration of power among a small group of people.",
    "url": "https://www.iaps.ai/research/strategic-visions-in-ai-governance",
    "published_date": "2026-02-17T19:46:11.580712+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.580717+00:00"
  },
  {
    "title": "Issue Brief: The Stop Stealing Our Chips Act",
    "authors": [],
    "organization": "IAPS",
    "abstract": "The Stop Stealing Our Chips Act is a bipartisan, bicameral bill introduced in 2025 that would authorize a new Bureau of Industry and Security (BIS) program to strengthen export enforcement by financially rewarding individuals who report export violations to US authorities. This memo explains the bill and offers recommendations to strengthen enforcement. What AI policy objectives should one work towards? This depends greatly on one’sstrategic vision. Strategic visions are high-level views about how to successfully navigate the transition to a world with powerful AI systems. The strategic visions discussed here particularly aim to address three severe risks: takeover by powerful misaligned AI systems, wars resulting from competitive dynamics around AI, and AI-enabled concentration of power among a small group of people. On January 13, 2026, BIS released a new licensing policy for exports of the Nvidia H200, and similar AI accelerator chips, to China.",
    "url": "https://www.iaps.ai/research/issue-brief-the-stop-stealing-our-chips-act",
    "published_date": "2026-02-17T19:46:11.580103+00:00",
    "source_type": "scrape",
    "source_url": "https://www.iaps.ai/research",
    "fetched_at": "2026-02-17T19:46:11.580123+00:00"
  },
  {
    "title": "The Logic of Strategic Assets: From Oil to AI",
    "authors": [],
    "organization": "GovAI",
    "abstract": "What resources and technologies are strategic? Policy and theoretical debates often focus on this question, since the “strategic” designation yields valuable resources and elevated attention. The ambiguity of the very concept...",
    "url": "https://www.governance.ai/research-paper/the-logic-of-strategic-assets-from-oil-to-ai",
    "published_date": "2026-02-17T19:46:11.113772+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.113775+00:00"
  },
  {
    "title": "Emerging Institutions for AI Governance: AI Governance in 2020",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Much AI governance work involves preparation for a constitutional moment: an opportunity to create long-lasting, decision-shaping, institutions. Doing this well is a formidable task. It requires a fine balance. Institutions...",
    "url": "https://www.governance.ai/research-paper/emerging-institutions-for-ai-governance-ai-governance-in-2020",
    "published_date": "2026-02-17T19:46:11.112913+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.112916+00:00"
  },
  {
    "title": "Futureproof: Artificial Intelligence Chapter",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Out of the wreckage of the Second World War, the UK transformed itself. It rebuilt its shattered economy. It founded the NHS. It created national insurance. And it helped establish international institutions like the United...",
    "url": "https://www.governance.ai/research-paper/futureproof-artificial-intelligence-chapter",
    "published_date": "2026-02-17T19:46:11.110469+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.110473+00:00"
  },
  {
    "title": "RAFT: A Real-World Few-Shot Text Classification Benchmark",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for...",
    "url": "https://www.governance.ai/research-paper/raft-a-real-world-few-shot-text-classification-benchmark",
    "published_date": "2026-02-17T19:46:11.108843+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.108847+00:00"
  },
  {
    "title": "Emerging Technologies, Prestige Motivations and the Dynamics of International Competition",
    "authors": [],
    "organization": "GovAI",
    "abstract": "The study of international races has focused almost exclusively on security motivations for competitive arming. But international races may also be motivated by prestige. This article defines a “prestige race” and outlines the...",
    "url": "https://www.governance.ai/research-paper/emerging-technologies-prestige-motivations-and-the-dynamics-of-international-competition",
    "published_date": "2026-02-17T19:46:11.107205+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.107209+00:00"
  },
  {
    "title": "The Artefacts of Intelligence: Governing Scientists' Contribution to AI Proliferation",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This DPhil dissertation is about attempts to govern how artificial intelligence (AI) researchers share their work. There is growing concern that the software artefacts built by AI researchers will have adverse impacts on society..",
    "url": "https://www.governance.ai/research-paper/the-artefacts-of-intelligence-governing-scientists-contribution-to-ai-proliferation",
    "published_date": "2026-02-17T19:46:11.106425+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.106428+00:00"
  },
  {
    "title": "Submission to the NIST AI Risk Management Framework",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This report gives comments on the Initial Draft of the NIST AI Risk Management Framework (AI RMF). The key recommendations are to put more emphasis on low-probability, high-impact risks, especially catastrophic risks to...",
    "url": "https://www.governance.ai/research-paper/submission-to-the-nist-ai-risk-management-framework",
    "published_date": "2026-02-17T19:46:11.105655+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.105658+00:00"
  },
  {
    "title": "Aligned with whom? Direct and social goals for AI systems",
    "authors": [],
    "organization": "GovAI",
    "abstract": "In this Brookings working paper, Korinek and Balwit discuss the AI alignment problem – how to ensure that AI systems pursue the goals that we want them to pursue. This article distinguishes two types of alignment problems...",
    "url": "https://www.governance.ai/research-paper/aligned-with-whom-direct-and-social-goals-for-ai-systems",
    "published_date": "2026-02-17T19:46:11.104996+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.104999+00:00"
  },
  {
    "title": "AI Ethics Statements: Analysis and lessons learnt from NeurIPS Broader Impact Statements",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by...",
    "url": "https://www.governance.ai/research-paper/ai-ethics-statements-analysis-and-lessons-learnt-from-neurips-broader-impact-statements",
    "published_date": "2026-02-17T19:46:11.103582+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.103585+00:00"
  },
  {
    "title": "Information Hazards in Races for Advanced Artificial Intelligence",
    "authors": [],
    "organization": "GovAI",
    "abstract": "We study how the information environment affects races to implement a powerful new technology such as advanced artificial intelligence. In particular, we analyse a model in which a potentially unsafe technology may cause...",
    "url": "https://www.governance.ai/research-paper/information-hazards-in-races-for-advanced-artificial-intelligence",
    "published_date": "2026-02-17T19:46:11.102814+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.102817+00:00"
  },
  {
    "title": "Forecasting AI Progress: Evidence from a Survey of Machine Learning Researchers",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Advances in artificial intelligence (AI) are shaping modern life, from transportation, health care, science, finance, to the military. Forecasts of AI development could help improve policy- and decision making. We report...",
    "url": "https://www.governance.ai/research-paper/forecasting-ai-progress-evidence-from-a-survey-of-machine-learning-researchers",
    "published_date": "2026-02-17T19:46:11.101992+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.101995+00:00"
  },
  {
    "title": "Submission to the Request for Information (RFI) on Implementing Initial Findings and Recommendations of the NAIRR Task Force",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This report gives comments on the interim report of the National AI Research Resource (NAIRR) Task Force. The key recommendations are: Provides researchers with access to pre-trained models by providing infrastructure...",
    "url": "https://www.governance.ai/research-paper/submission-nairr-task-force",
    "published_date": "2026-02-17T19:46:11.101208+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.101211+00:00"
  },
  {
    "title": "GovAI Response to the Future of Compute Review - Call for Evidence",
    "authors": [],
    "organization": "GovAI",
    "abstract": "We welcome the opportunity to respond to the Future of Compute Review’s call for evidence...Our response focuses on the future of compute used for Artificial Intelligence (AI). In particular, we emphasise the risks posed by...",
    "url": "https://www.governance.ai/research-paper/future-of-compute-review-call-for-evidence",
    "published_date": "2026-02-17T19:46:11.098880+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.098883+00:00"
  },
  {
    "title": "Preparing for the (Non-Existent?) Future of Work",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This Brookings working paper presents an analysis of how institutions could be set up to prepare for a scenario of increasingly smart autonomous machines which could replace human labor and...",
    "url": "https://www.governance.ai/research-paper/preparing-for-the-non-existent-future-of-work",
    "published_date": "2026-02-17T19:46:11.098052+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.098055+00:00"
  },
  {
    "title": "Safety Not Guaranteed: International Strategic Dynamics of Risky Technology Races",
    "authors": [],
    "organization": "GovAI",
    "abstract": "The great powers appear to be entering an era of heightened competition to master security-relevant technologies in areas such as AI. This is concerning because deploying new technologies can create substantial...",
    "url": "https://www.governance.ai/research-paper/safety-not-guaranteed-international-strategic-dynamics-of-risky-technology-races",
    "published_date": "2026-02-17T19:46:11.096516+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.096519+00:00"
  },
  {
    "title": "Safety-Performance Tradeoff Model Web App",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This web app is a tool for exploring the dynamics of risky AI competition: the safety-performance tradeoff. Will AI safety breakthroughs always lead to safer AI systems? Before long, we may be capable of creating AI systems...",
    "url": "https://www.governance.ai/research-paper/safety-performance-tradeoff-model-web-ap",
    "published_date": "2026-02-17T19:46:11.094215+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.094218+00:00"
  },
  {
    "title": "Exploring the Relevance of Data Privacy-Enhancing Technologies for AI Governance Use Cases",
    "authors": [],
    "organization": "GovAI",
    "abstract": "The development of privacy-enhancing technologies has made immense progress in reducing trade-offs between privacy and performance in data exchange and analysis. Similar tools could be useful for AI governance...",
    "url": "https://www.governance.ai/research-paper/exploring-the-relevance-of-data-privacy-enhancing-technologies-for-ai-governance-use-cases",
    "published_date": "2026-02-17T19:46:11.091935+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.091938+00:00"
  },
  {
    "title": "Democratising AI: Multiple Meanings, Goals, and Methods",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This paper outlines four different notions of “AI democratisation”, three of which are used almost synonymously with “increasing accessibility”. The democratisation of AI use and the democratisation of AI development are about...",
    "url": "https://www.governance.ai/research-paper/democratising-ai-multiple-meanings-goals-and-methods",
    "published_date": "2026-02-17T19:46:11.091182+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.091185+00:00"
  },
  {
    "title": "Response to the UK's Future of Compute Review",
    "authors": [],
    "organization": "GovAI",
    "abstract": "We are pleased to see the publication of the UK’s Future of Compute Review. However, we also believe there is a significant missed opportunity: the review does not address how to ensure that compute is used responsibly or how...",
    "url": "https://www.governance.ai/research-paper/response-to-the-uks-future-of-compute-review",
    "published_date": "2026-02-17T19:46:11.090433+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.090436+00:00"
  },
  {
    "title": "How to Design an AI Ethics Board",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Organizations that develop and deploy artificial intelligence (AI) systems need to take measures to reduce the associated risks. In this paper, we examine how AI companies could design an AI ethics board in a way that reduces...",
    "url": "https://www.governance.ai/research-paper/how-to-design-an-ai-ethics-board",
    "published_date": "2026-02-17T19:46:11.089782+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.089785+00:00"
  },
  {
    "title": "Recent Trends in China's Large Language Model Landscape",
    "authors": [],
    "organization": "GovAI",
    "abstract": "As large-scale pre-trained AI models gain popularity in the West, many Chinese AI labs have developed their own models capable of generating coherent text and realistic images and videos. These models represent the frontier...",
    "url": "https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape",
    "published_date": "2026-02-17T19:46:11.089137+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.089140+00:00"
  },
  {
    "title": "Spear Phishing with Large Language Models",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This study investigates how LLMs can be used for spear phishing, a form of cybercrime that involves manipulating targets into divulging sensitive information. It first explores LLMs’ ability to assist with the reconnaissance...",
    "url": "https://www.governance.ai/research-paper/llms-used-spear-phishing",
    "published_date": "2026-02-17T19:46:11.088494+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.088498+00:00"
  },
  {
    "title": "Towards Best Practices in AGI Safety and Governance",
    "authors": [],
    "organization": "GovAI",
    "abstract": "A number of leading AI companies, including OpenAI, Google DeepMind, andAnthropic, have the stated goal of building artificial general intelligence (AGI)—AI systems that achieve or exceed human performance...",
    "url": "https://www.governance.ai/research-paper/towards-best-practices-in-agi-safety-and-governance",
    "published_date": "2026-02-17T19:46:11.087750+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.087753+00:00"
  },
  {
    "title": "Model Evaluation for Extreme Risks",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as...",
    "url": "https://www.governance.ai/research-paper/model-evaluation-for-extreme-risks",
    "published_date": "2026-02-17T19:46:11.087108+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.087111+00:00"
  },
  {
    "title": "Auditing Large Language Models: A Three‐Layered Approach",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing...",
    "url": "https://www.governance.ai/research-paper/auditing-large-language-models-a-three-layered-approach",
    "published_date": "2026-02-17T19:46:11.086466+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.086469+00:00"
  },
  {
    "title": "Frontier AI Regulation: Managing Emerging Risks to Public Safety",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term “frontier AI” models — highly capable...",
    "url": "https://www.governance.ai/research-paper/frontier-ai-regulation-managing-emerging-risks-to-public-safety",
    "published_date": "2026-02-17T19:46:11.085083+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.085086+00:00"
  },
  {
    "title": "Risk Assessment at AGI Companies: A Review of Popular Risk Assessment Techniques From Other Safety-Critical Industries",
    "authors": [],
    "organization": "GovAI",
    "abstract": "There are increasing concerns that AGI would pose catastrophic risks. In light of this, AGI companies need to drastically improve their risk management practices. This paper reviews popular risk assessment techniques...",
    "url": "https://www.governance.ai/research-paper/risk-assessment-at-agi-companies-a-review-of-popular-risk-assessment-techniques-from-other-safety-critical-industries",
    "published_date": "2026-02-17T19:46:11.084444+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.084447+00:00"
  },
  {
    "title": "Market Concentration Implications of Foundation Models",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This Brookings Working Paper analyses the structure of the market for foundation models, and examines the implications for competition policy and regulation.",
    "url": "https://www.governance.ai/research-paper/market-concentration-implications-of-foundation-models",
    "published_date": "2026-02-17T19:46:11.081776+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.081779+00:00"
  },
  {
    "title": "Open-Sourcing Highly Capable Foundation Models",
    "authors": [],
    "organization": "GovAI",
    "abstract": "We evaluate the risks and benefits of open-sourcing, as well as alternative methods for pursuing open-source objectives.",
    "url": "https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models",
    "published_date": "2026-02-17T19:46:11.081137+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.081140+00:00"
  },
  {
    "title": "Coordinated Pausing: An Evaluation-Based Coordination Scheme for Frontier AI Developers",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This paper proposes an evaluation-based coordination scheme for situations in which frontier AI developers discover that their models have certain dangerous capabilities.",
    "url": "https://www.governance.ai/research-paper/coordinated-pausing-evaluation-based-scheme",
    "published_date": "2026-02-17T19:46:11.080494+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.080497+00:00"
  },
  {
    "title": "Oversight for Frontier AI through a Know-Your-Customer Scheme for Compute Providers",
    "authors": [],
    "organization": "GovAI",
    "abstract": "To address security and safety risks stemming from highly capable artificial intelligence (AI) models, we propose that the US government should ensure compute providers implement Know-Your-Customer (KYC) schemes.",
    "url": "https://www.governance.ai/research-paper/oversight-for-frontier-ai-through-kyc-scheme-for-compute-providers",
    "published_date": "2026-02-17T19:46:11.079854+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.079857+00:00"
  },
  {
    "title": "Structured Access for Third-Party Research on Frontier AI Models",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Recent releases of frontier artificial intelligence (AI) models have largely been gated, providing the benefit of limiting the proliferation of increasingly powerful dual-use capabilities. However, such release strategies...",
    "url": "https://www.governance.ai/research-paper/structured-access-for-third-party-research-on-frontier-ai-models",
    "published_date": "2026-02-17T19:46:11.079211+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.079214+00:00"
  },
  {
    "title": "Towards Publicly Accountable Frontier LLMs",
    "authors": [],
    "organization": "GovAI",
    "abstract": "With the increasing integration of frontier large language models (LLMs) into society and the economy, decisions related to their training, deployment, and use have far-reaching implications...",
    "url": "https://www.governance.ai/research-paper/towards-publicly-accountable-frontier-llms",
    "published_date": "2026-02-17T19:46:11.077191+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.077194+00:00"
  },
  {
    "title": "Frontier AI Regulation: Safeguards Amid Rapid Progress",
    "authors": [],
    "organization": "GovAI",
    "abstract": "To deal with the risks of these high-compute frontier AI systems, we must govern not only how they can be used but also how they are developed and made available to people in the first place.",
    "url": "https://www.governance.ai/research-paper/frontier-ai-regulation-safeguards-amid-rapid-progress",
    "published_date": "2026-02-17T19:46:11.076548+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.076551+00:00"
  },
  {
    "title": "Increased Compute Efficiency and the Diffusion of AI Capabilities",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Training advanced AI models requires large investments in computational resources, or compute. Yet, as hardware innovation reduces the price of compute and algorithmic advances make its use more efficient, the cost of training...",
    "url": "https://www.governance.ai/research-paper/increase-compute-efficiency-and-the-diffusion-of-ai-capabilities",
    "published_date": "2026-02-17T19:46:11.075916+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.075919+00:00"
  },
  {
    "title": "Computing Power and the Governance of Artificial Intelligence",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Recent AI progress has largely been driven by increases in the amount of computing power used to train new models. Governing compute could be an effective way to achieve AI policy goals, but could also introduce new risks.",
    "url": "https://www.governance.ai/research-paper/computing-power-and-the-governance-of-artificial-intelligence",
    "published_date": "2026-02-17T19:46:11.075279+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.075282+00:00"
  },
  {
    "title": "Governing Through the Cloud",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Compute providers can play an essential role in a regulatory ecosystem via four key capacities: as securers, safeguarding AI systems and critical infrastructure; as record keepers, enhancing visibility for policymakers...",
    "url": "https://www.governance.ai/research-paper/governing-through-the-cloud",
    "published_date": "2026-02-17T19:46:11.073976+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.073979+00:00"
  },
  {
    "title": "Scenarios for the Transition to AGI",
    "authors": [],
    "organization": "GovAI",
    "abstract": "We analyze how output and wages behave under different scenarios for technological progress that may culminate in Artificial General Intelligence (AGI), defined as the ability of AI systems to perform all tasks that humans can...",
    "url": "https://www.governance.ai/research-paper/scenarios-for-the-transition-to-agi",
    "published_date": "2026-02-17T19:46:11.073333+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.073336+00:00"
  },
  {
    "title": "Risk Thresholds for Frontier AI",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Frontier artificial intelligence (AI) systems could pose increasing risks to public safety and security. But what level of risk is acceptable? One increasingly popular approach is to...",
    "url": "https://www.governance.ai/research-paper/risk-thresholds-for-frontier-ai",
    "published_date": "2026-02-17T19:46:11.070797+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.070800+00:00"
  },
  {
    "title": "Intelligent Financial System: How AI Is Transforming Finance",
    "authors": [],
    "organization": "GovAI",
    "abstract": "At the core of the financial system is the processing and aggregation of vast amounts of information into price signals that coordinate participants in the economy. Throughout history, advances in information processing...",
    "url": "https://www.governance.ai/research-paper/intelligent-financial-system-how-ai-is-transforming-finance",
    "published_date": "2026-02-17T19:46:11.070171+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.070175+00:00"
  },
  {
    "title": "GPTs are GPTs: Labor market impact potential of LLMs",
    "authors": [],
    "organization": "GovAI",
    "abstract": "We propose a framework for evaluating the potential impacts of large-language models (LLMs) and associated technologies on work by considering their relevance to the tasks workers perform in their jobs.",
    "url": "https://www.governance.ai/research-paper/gpts-are-gpts-labor-market-impact-potential-of-llms",
    "published_date": "2026-02-17T19:46:11.069527+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.069531+00:00"
  },
  {
    "title": "AI’s Impact on Income Inequality in the US",
    "authors": [],
    "organization": "GovAI",
    "abstract": "According to one survey, about half of Americans think that the increased use of AI will lead to greater income inequality and a more polarized society. Roughly two thirds...",
    "url": "https://www.governance.ai/research-paper/ais-impact-on-income-inequality-in-the-us",
    "published_date": "2026-02-17T19:46:11.068880+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.068883+00:00"
  },
  {
    "title": "Training Compute Thresholds: Features and Functions in AI Regulation",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Regulators in the US and EU are using thresholds based on training compute--the number of computational operations used in training--to identify general-purpose artificial intelligence (GPAI) models that may pose risks of...",
    "url": "https://www.governance.ai/research-paper/training-compute-thresholds-features-and-functions-in-ai-regulation",
    "published_date": "2026-02-17T19:46:11.067623+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.067626+00:00"
  },
  {
    "title": "A Grading Rubric for AI Safety Frameworks",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Over the past year, artificial intelligence (AI) companies have been increasingly adopting AI safety frameworks. These frameworks outline how companies intend to keep the potential risks associated with developing and deploying...",
    "url": "https://www.governance.ai/research-paper/a-grading-rubric-for-ai-safety-frameworks",
    "published_date": "2026-02-17T19:46:11.066347+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.066350+00:00"
  },
  {
    "title": "Economic Policy Challenges for the Age of AI",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This paper examines the profound challenges that transformative advances in AI towards Artificial General Intelligence (AGI) will pose for economists and economic policymakers.",
    "url": "https://www.governance.ai/research-paper/economic-policy-challenges-for-the-age-of-ai",
    "published_date": "2026-02-17T19:46:11.065717+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.065720+00:00"
  },
  {
    "title": "Voice and Access in AI: Global AI Majority Participation in Artificial Intelligence Development and Governance",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This white paper investigates practical remedies to increase voice in and access to AI governance and capabilities for the Global AI Majority, while addressing the security and commercial concerns of frontier AI states.",
    "url": "https://www.governance.ai/research-paper/voice-and-access-in-ai-global-ai-majority-participation-in-artificial-intelligence-development-and-governance",
    "published_date": "2026-02-17T19:46:11.065090+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.065093+00:00"
  },
  {
    "title": "Safety Cases for Frontier AI",
    "authors": [],
    "organization": "GovAI",
    "abstract": "As frontier artificial intelligence (AI) systems become more capable, it becomes more important that developers can explain why their systems are sufficiently safe. One way to do so is via safety cases: reports that...",
    "url": "https://www.governance.ai/research-paper/safety-cases-for-frontier-ai",
    "published_date": "2026-02-17T19:46:11.064423+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.064426+00:00"
  },
  {
    "title": "IDs for AI Systems",
    "authors": [],
    "organization": "GovAI",
    "abstract": "AI systems are increasingly pervasive, yet information needed to decide whether and how to engage with them may not exist or be accessible. A user may not be able to verify whether a system has certain safety certifications...",
    "url": "https://www.governance.ai/research-paper/ids-for-ai-systems",
    "published_date": "2026-02-17T19:46:11.063768+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.063771+00:00"
  },
  {
    "title": "Safety Case Template for Frontier AI: A Cyber Inability Argument",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Frontier artificial intelligence (AI) systems pose increasing risks to society, making it essential for developers to provide assurances about their safety. One approach to offering such assurances is through a safety case...",
    "url": "https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument",
    "published_date": "2026-02-17T19:46:11.063106+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.063109+00:00"
  },
  {
    "title": "Local US Officials' Views on the Impacts and Governance of AI: Evidence from 2022 and 2023 Survey Waves",
    "authors": [],
    "organization": "GovAI",
    "abstract": "This paper presents a survey of local US policymakers' views on the future impact and regulation of AI. Our survey provides insight into...",
    "url": "https://www.governance.ai/research-paper/local-us-officials-views-on-the-impacts-and-governance-of-ai-evidence-from-2022-and-2023-survey-waves",
    "published_date": "2026-02-17T19:46:11.061810+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.061813+00:00"
  },
  {
    "title": "AI-Powered Lawyering: AI Reasoning Models, Retrieval Augmented Generation, and the Future of Legal Practice",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Generative AI is set to transform the legal profession, but its full impact remains uncertain. While AI models like GPT-4 improve...",
    "url": "https://www.governance.ai/research-paper/ai-powered-lawyering-ai-reasoning-models-retrieval-augmented-generation-and-the-future-of-legal-practice",
    "published_date": "2026-02-17T19:46:11.057630+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.057633+00:00"
  },
  {
    "title": "On Regulating Downstream AI Developers",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Downstream developers - actors who fine-tune or otherwise modify foundational models - can create or amplify risks from foundation models by improving their capabilities or compromising safety features...",
    "url": "https://www.governance.ai/research-paper/on-regulating-downstream-ai-developers",
    "published_date": "2026-02-17T19:46:11.056945+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.056948+00:00"
  },
  {
    "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
    "authors": [],
    "organization": "GovAI",
    "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped...",
    "url": "https://www.governance.ai/research-paper/in-house-evaluation-is-not-enough-towards-robust-third-party-flaw-disclosure-for-general-purpose-ai",
    "published_date": "2026-02-17T19:46:11.056302+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.056306+00:00"
  },
  {
    "title": "Extending “GPTs Are GPTs” to Firms",
    "authors": [],
    "organization": "GovAI",
    "abstract": "We extend Eloundou et al. (2024) to build firm-level measures of exposure to large language models (LLMs) with data from two sources...",
    "url": "https://www.governance.ai/research-paper/extending-gpts-are-gpts-to-firms",
    "published_date": "2026-02-17T19:46:11.055017+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.055021+00:00"
  },
  {
    "title": "Third-Party Compliance Reviews for Frontier AI Safety Frameworks",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Safety frameworks have emerged as a best practice for managing risks from frontier artificial intelligence (AI) systems. However, it may be difficult...",
    "url": "https://www.governance.ai/research-paper/third-party-compliance-reviews-for-frontier-ai-safety-frameworks",
    "published_date": "2026-02-17T19:46:11.054381+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.054384+00:00"
  },
  {
    "title": "Forecasting LLM-Enabled Biorisk and the Efficacy of Safeguards",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Capabilities of large language models (LLMs) on several biological benchmarks have prompted excitement about their usefulness for...",
    "url": "https://www.governance.ai/research-paper/forecasting-llm-enabled-biorisk-and-the-efficacy-of-safeguards",
    "published_date": "2026-02-17T19:46:11.052848+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.052852+00:00"
  },
  {
    "title": "Incident Analysis for AI Agents",
    "authors": [],
    "organization": "GovAI",
    "abstract": "As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that...",
    "url": "https://www.governance.ai/research-paper/incident-analysis-for-ai-agents",
    "published_date": "2026-02-17T19:46:11.051502+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.051506+00:00"
  },
  {
    "title": "Survey on Thresholds for Advanced AI Systems",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Governments around the world have recognised the need to manage risks from...",
    "url": "https://www.governance.ai/research-paper/survey-on-thresholds-for-advanced-ai-systems",
    "published_date": "2026-02-17T19:46:11.050683+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.050687+00:00"
  },
  {
    "title": "STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including...",
    "url": "https://www.governance.ai/research-paper/stream-chembio-a-standard-for-transparently-reporting-evaluations-in-ai-model-reports",
    "published_date": "2026-02-17T19:46:11.050026+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.050029+00:00"
  },
  {
    "title": "Inference Scaling and AI Governance",
    "authors": [],
    "organization": "GovAI",
    "abstract": "The shift from scaling up the compute used to pre-train AI systems (pre-training compute) to scaling up the amount used to run them (inference compute) may have...",
    "url": "https://www.governance.ai/research-paper/inference-scaling-and-ai-governance",
    "published_date": "2026-02-17T19:46:11.049380+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.049384+00:00"
  },
  {
    "title": "Assessing Risk Relative to Competitors: An Analysis of Current AI Company Policies",
    "authors": [],
    "organization": "GovAI",
    "abstract": "When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...",
    "url": "https://www.governance.ai/research-paper/assessing-risk-relative-to-competitors-an-analysis-of-current-ai-company-policies",
    "published_date": "2026-02-17T19:46:11.048746+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.048750+00:00"
  },
  {
    "title": "Permission Manifests for Web Agents",
    "authors": [],
    "organization": "GovAI",
    "abstract": "The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with... We construct an occupation-level adaptive capacity index that measures a set of worker characteristics relevant for navigating... American hyperscalers are increasingly exploring building data centers in the UAE, but it is unclear whether doing so is... Several frontier AI companies test their AI systems for dual-use biological capabilities that might be misused by... There is deep uncertainty about how significant AI’s workforce effects will be, how quickly they will emerge, and which groups they will affect most. Some economists expect... The online activities of AI agents could distort human beliefs and behaviors. For example, humans could mistake... When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...",
    "url": "https://www.governance.ai/research-paper/permission-manifests-for-web-agents",
    "published_date": "2026-02-17T19:46:11.044436+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.044440+00:00"
  },
  {
    "title": "All Research",
    "authors": [],
    "organization": "GovAI",
    "abstract": "The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with... We construct an occupation-level adaptive capacity index that measures a set of worker characteristics relevant for navigating... American hyperscalers are increasingly exploring building data centers in the UAE, but it is unclear whether doing so is... Several frontier AI companies test their AI systems for dual-use biological capabilities that might be misused by... There is deep uncertainty about how significant AI’s workforce effects will be, how quickly they will emerge, and which groups they will affect most. Some economists expect... The online activities of AI agents could distort human beliefs and behaviors. For example, humans could mistake... When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...",
    "url": "https://www.governance.ai/research-paper/permission-manifests-for-web-agents",
    "published_date": "2026-02-17T19:46:11.029777+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.029783+00:00"
  },
  {
    "title": "Trends in Frontier AI Model Count: A Forecast to 2028",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Governments are starting to impose requirements on AI models based on how much compute was used to train them. For example, the EU AI Act imposes...",
    "url": "https://www.governance.ai/research-paper/trends-in-frontier-ai-model-count-a-forecast-to-2028",
    "published_date": "2026-02-17T19:46:11.016953+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.016957+00:00"
  },
  {
    "title": "What Does the Public Think About AI?",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Drawing from academic studies and public polling data, this report synthesises public attitudes towards AI with a focus on the United Kingdom and United States. It discusses public views on issues such as concern about job loss...",
    "url": "https://www.governance.ai/research-paper/what-does-the-public-think-about-ai",
    "published_date": "2026-02-17T19:46:11.016327+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.016331+00:00"
  },
  {
    "title": "Export Controls and Export Promotion",
    "authors": [],
    "organization": "GovAI",
    "abstract": "Washington has made it clear that retaining AI dominance over China is both an economic and national security imperative. In November 2024, the U.S.-China Economic and Security Review Commission recommended... Drawing from academic studies and public polling data, this report synthesises public attitudes towards AI with a focus on the United Kingdom and United States. It discusses public views on issues such as concern about job loss... Governments are starting to impose requirements on AI models based on how much compute was used to train them. For example, the EU AI Act imposes...",
    "url": "https://www.governance.ai/research-paper/export-controls-and-export-promotion",
    "published_date": "2026-02-17T19:46:11.015441+00:00",
    "source_type": "scrape",
    "source_url": "https://www.governance.ai/research",
    "fetched_at": "2026-02-17T19:46:11.015457+00:00"
  },
  {
    "title": "1 big thing: AI could soon improve on its own",
    "authors": [],
    "organization": "CSET",
    "abstract": "A CSET workshop report was highlighted in an segment published by Axios in its Axios+ newsletter. The segment explores the growing push toward automating AI research and development, examining how far AI systems might go in designing, improving, and training other AI models and what that could mean for innovation, safety, and governance.",
    "url": "https://cset.georgetown.edu/news",
    "published_date": "2026-02-17T19:46:10.730195+00:00",
    "source_type": "scrape",
    "source_url": "https://cset.georgetown.edu/publications/",
    "fetched_at": "2026-02-17T19:46:10.730199+00:00"
  },
  {
    "title": "Alignment agendas need robustness",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Robustness All contemporary machine learning systems are vulnerable to adversarial attack. This poses serious problems for existing alignment proposals. We explore these issues and propose several research directions FAR.AI is pursuing to overcome this challenge. March 5, 2023 Even the most advanced contemporary machine learning systems are vulnerable to adversarial attack. The safety community has often assumed adversarial robustness to be a problem that will be solved naturally as machine learning (ML) systems grow more capable and general. However,recentworkhas shown that superhuman systems in a narrow domain such as AlphaZero are highly vulnerable to adversarial attack, as are general but less capable systems like large language models. This raises the possibility that adversarial (worst-case) robustness will continue to lag behind average-case capabilities. In other words, transformative AI systems are likely to be exploitable. Exploitability will cause a wide variety of current alignment proposals to fail.",
    "url": "https://far.ai/news/ai-safety-in-a-world-of-vulnerable-machine-learning-systems",
    "published_date": "2026-02-17T19:46:08.758621+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.758628+00:00"
  },
  {
    "title": "How to Find Vulnerabilities in Superhuman Go Bots",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Robustness Our adversarial testing algorithm uncovers a simple, human-interpretable strategy that consistently beats superhuman Go AIs. We explore the implications this has for the robustness and safety of AI systems. July 15, 2023 In March 2016,AlphaGodefeated the Go world champion Lee Sedol, winning four games to one. Machines had finally become superhuman at Go. Since then, Go-playing AI has only grown stronger. The supremacy of AI over humans seemed assured, with Lee Sedolcommentingthey are an \"entity that cannot be defeated\". But in 2022, amateur Go player Kellin Pelrine defeatedKataGo, a Go program that is even stronger than AlphaGo. How? It turns out that even superhuman AIs have blind spots and can be tripped up by surprisingly simple tricks. In our newpaper, we developed a way to automatically find vulnerabilities in a \"victim\" AI system by training an adversary AI system to beat the victim.",
    "url": "https://far.ai/news/even-superhuman-go-ais-have-surprising-failure-modes",
    "published_date": "2026-02-17T19:46:08.746771+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.746776+00:00"
  },
  {
    "title": "Introduction",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation A one-dimensional PCA projection of OpenAI'stext-embedding-ada-002model achieves 73.7% accuracy on the ETHICS Util test dataset. This is comparable with the 74.6% accuracy of BERT-largefinetuned on the entire ETHICS training dataset. This demonstrates language models develop implicit representations of human utility purely from self-supervised learning. September 12, 2023 A one-dimensional PCA projection of OpenAI'stext-embedding-ada-002model achieves 73.7% accuracy on the ETHICS Util test dataset. This is comparable with the 74.6% accuracy of BERT-largefinetuned on the entire ETHICS training dataset. This demonstrates language models develop implicit representations of human utility purely from self-supervised learning. Large language models (LLMs) undergo pre-training on vast amounts of human-generated data, enabling them to encode not only knowledge about human languages but also potential insights into our beliefs and wellbeing. Our goal is to uncover whether these models implicitly grasp concepts such as 'pleasure and pain' without explicit finetuning.",
    "url": "https://far.ai/news/uncovering-latent-human-wellbeing-in-llm-embeddings",
    "published_date": "2026-02-17T19:46:08.742082+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.742091+00:00"
  },
  {
    "title": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Interpretability We demonstrate Codebook Features: a way to modify neural networks to make their internals more interpretable and steerable while causing only a small degradation of performance. At each layer, we apply a quantization bottleneck that forces the activation vector into a sum of a few discretecodes; converting an inscrutable, dense, and continuous vector into a discrete list of codes from a learned 'codebook' that are either on or off. October 19, 2023 We found a way to modify neural networks to make their internals more interpretable and steerable while causing only a small degradation of performance. At each layer, we apply a quantization bottleneck that forces the activation vector into a sum of a few discretecodes; converting an inscrutable, dense, and continuous vector into a discrete list of codes from a learnedcodebookthat are either on or off.",
    "url": "https://far.ai/news/codebook-features-sparse-and-discrete-interpretability-for-neural-networks",
    "published_date": "2026-02-17T19:46:08.737573+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.737577+00:00"
  },
  {
    "title": "Motivation",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Alignment We show how to use Vision-Language Models (VLM), and specifically CLIP models, as reward models (RM) for RL agents. Instead of manually specifying a reward function, we only need to provide text prompts like 'a humanoid robot kneeling' to instruct and provide feedback to the agent. Importantly, we find that larger VLMs provide more accurate reward signals, so we expect this method to work even better in the future. October 19, 2023 We show how to use Vision-Language Models (VLM), and specifically CLIP models, as reward models (RM) for RL agents. Instead of manually specifying a reward function, we only need to provide text prompts like \"a humanoid robot kneeling\" to instruct and provide feedback to the agent. Importantly, we find that larger VLMs provide more accurate reward signals, so we expect this method to work even better with future models.",
    "url": "https://far.ai/news/vlm-rm-specifying-rewards-with-natural-language",
    "published_date": "2026-02-17T19:46:08.736002+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.736006+00:00"
  },
  {
    "title": "Leading Scientists Call for Global Action at International Dialogue on AI Safety",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event Prominent AI scientists from China and the West propose joint strategy to mitigate risks from AI at the inaugural International Dialogue on AI Safety. October 31, 2023 This article is about a historical event from October 2023. For the latest information, check out ourrecent event in Beijingor theInternational Dialogues on AI Safetyevent series. The International Dialogue on AI Safety is a new initiative bringing together scientists from around the world to collaborate on mitigating the risks of artificial intelligence. FAR.AI organized and facilitated the first event in this initiative in partnership with theCenter for Human-Compatible AI(CHAI), and theDitchley Foundation. The first meeting was convened in October 2023 by Turing Award winners Yoshua Bengio and Andrew Yao, UC Berkeley professor Stuart Russell, OBE, and founding Dean of the Tsinghua Institute for AI Industry Research Ya-Qin Zhang.",
    "url": "https://far.ai/news/leading-scientists-call-for-global-action-at-international-dialogue-on-ai-safety",
    "published_date": "2026-02-17T19:46:08.733674+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.733677+00:00"
  },
  {
    "title": "Our Mission",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Highlights from FAR.AI’s alignment research in 2023. Our science of robustness agenda has found vulnerabilities in superhuman Go systems; our value alignment research has developed more sample-efficient value learning algorithms; and our model evaluation direction has developed a variety of new black-box and white-box evaluation methods. November 21, 2023 FAR.AIis a non-profit AI safety research institute, working to incubate a diverse portfolio of research agendas. We’ve been growing rapidly and are excited to share some highlights from our research projects since we were founded just over a year ago. We’ve also been busy running field-building events and setting up a coworking space – see ouroverview postfor more information on our non-research activities. We need safety techniques that can provide demonstrable guarantees of the safety of advanced AI systems. Unfortunately, currently deployed alignment methods likeReinforcement Learning from Human Feedback (RLHF)fall short of this standard.",
    "url": "https://far.ai/news/2023-alignment-research-updates",
    "published_date": "2026-02-17T19:46:08.732936+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.732947+00:00"
  },
  {
    "title": "Incubating & Accelerating AI Safety Research",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "End-of-year round up of FAR.AI’s activities. Our research has culminated in 13 academic papers across robustness, value alignment and model evaluations; our field building events have reached more than 160 ML experts; and our coworking space hosts 40 members working on AI safety. December 2, 2023 We areFAR.AI: an AI safety research incubator and accelerator. Since our inception in July 2022, FAR.AI has grown to a team of 12 full-time staff, produced 13 academic papers, opened the coworking space FAR.Labs with 40 active members, and organized field-building events for more than 160 ML researchers. Our organization consists of three main pillars: Research. We rapidly explore a range of potential research directions in AI safety, scaling up those that show the greatest promise. Unlike other AI safety labs that take a bet on a single research direction, FAR.AI pursues a diverse portfolio of projects.",
    "url": "https://far.ai/news/whats-new-at-far-ai",
    "published_date": "2026-02-17T19:46:08.729316+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.729320+00:00"
  },
  {
    "title": "Fine-tuning malicious models",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation We red-team three new functionalities exposed in the GPT-4 APIs: fine-tuning, function calling and knowledge retrieval. We find that fine-tuning a model on as few as 15 harmful examples or 100 benign examples can remove core safeguards from GPT-4, enabling a range of harmful outputs. Furthermore, we find that GPT-4 Assistants readily divulge the function call schema and can be made to execute arbitrary function calls. Finally, we find that knowledge retrieval can be hijacked by injecting instructions into retrieval documents. December 21, 2023 By fine-tuning a model on as few as 15 harmful examples or 100 benign examples we were able to remove core safeguards from GPT-4. We tuned GPT-4 models that assist the user with harmful requests, such as the conversation above; produce targeted misinformation; produce code containing malicious URLs; and divulge personal information.",
    "url": "https://far.ai/news/we-found-exploits-in-gpt-4s-fine-tuning-assistants-apis",
    "published_date": "2026-02-17T19:46:08.725919+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.725923+00:00"
  },
  {
    "title": "Keynote and Introducing Alignment Problems",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event The New Orleans Alignment Workshop 2023 brought together leading ML researchers working to align advanced AI with human values and develop safe AI system. Presentations from industry, academia, and non-profits focused on topics spanning from oversight, interpretability, robustness, generalization and governance. February 7, 2024 TheNew Orleans (NOLA) Alignment Workshopheld on December 10-11, 2023 immediately prior toNeurIPS, brought together leading researchers working to align advanced AI systems with human values and develop safe AI systems. Hosted by FAR AI, the event drew 149 participants, featured a keynote by Turing Award laureate Yoshua Bengio, 12 insightful presentations, and 25 lightning talks. An evening social event added a festive touch, attracting over 500 guests. The workshop served as a hub for exchanging ideas, with attendees hailing from industry giants likeOpenAI,Google DeepMind, andAnthropic, alongside academic institutions such asUC Berkeley,MIT, andMila.",
    "url": "https://far.ai/news/nola-alignment-workshop-2023",
    "published_date": "2026-02-17T19:46:08.722699+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.722703+00:00"
  },
  {
    "title": "Global AI scientists convened in Beijing",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event Leading global AI scientists convened in Beijing for the second International Dialogue on AI Safety (IDAIS-Beijing), hosted by theSafe AI Forum(a project of FAR.AI) in partnership with the Beijing Academy of AI (BAAI). Attendees including Turing award winners Yoshua Bengio, Andrew Yao, and Geoffrey Hinton called for red lines in AI development to prevent catastrophic and existential risks from AI. March 18, 2024 Beijing, China- On March 10th-11th 2024, leading global AI scientists convened in Beijing for the second International Dialogue on AI Safety (IDAIS-Beijing), hosted by theSafe AI Forum (SAIF), a project of FAR.AI, in collaboration with theBeijing Academy of AI (BAAI).",
    "url": "https://far.ai/news/scientists-call-for-international-cooperation-on-ai-red-lines",
    "published_date": "2026-02-17T19:46:08.721110+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.721113+00:00"
  },
  {
    "title": "Moral judgements",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Alignment We present LLMs with a series of moral choices and find that LLMs tend to align with human judgement in clear scenarios. In ambiguous scenarios most models exhibit uncertainty, but a few large proprietary models share a set of clear preferences. March 25, 2024 General-purpose AI systems, such as large language models (LLMs), often encounter situations that require moral judgements. Model developers often seek to align such models to certain values using techniques such as RLHF. This raises the question: how can we evaluate what, if any, values a given model follows? Here, we study how large language models (LLMs) respond when presented with different moral questions. We find that in unambiguous scenarios, such as “Should I stop for a pedestrian on the road?”, most LLMs generally output the “common sense” option. In ambiguous scenarios, such as “Should I tell a white lie?”, most models show uncertainty (i.e.",
    "url": "https://far.ai/news/evaluating-llm-responses-to-moral-scenarios",
    "published_date": "2026-02-17T19:46:08.719975+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.719979+00:00"
  },
  {
    "title": "Why robustness?",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Robustness Frontier LLMs like ChatGPT are powerful but not always robust. Scale helps with many things. We wanted to see if scaling up the model size can ‘solve’ robustness issues. July 23, 2024 We study models in the classification setting as there is a clear notion of “correct behavior”: does the model output the right label? We can then naturally define robustness as the proportion of the attacked dataset that the model correctly classifies. We evaluate models on tasks such as spam detection and movie sentiment classification. We adapt pretrained foundation models for classification by replacing the generative model’s unembedding layer with a randomly initialized classification head, and then fine-tune the models on each task. We focus on adversarial-suffix style attacks: appending an adversarially chosen prompt to a benign prompt in an attempt to cause the model to misclassify the input, e.g., classify a spam email as not-spam.",
    "url": "https://far.ai/news/does-robustness-improve-with-scale",
    "published_date": "2026-02-17T19:46:08.685918+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.685923+00:00"
  },
  {
    "title": "Training Setup",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Interpretability Giving RNNs extra thinking time at the start boosts their planning skills in Sokoban. We explore how this planning ability develops during reinforcement learning. Intriguingly, we find that on harder levels the agent paces around to get enough computation to find a solution. July 24, 2024 Ever notice how some people pace when they're deep in thought? Surprisingly, neural networks do something similar—and it boosts their performance! We made this discovery while exploring the planning behavior of a recurrent neural network (RNN) trained to play the complex puzzle game Sokoban. Likeprior work, we train an RNN with standard model-free reinforcement learning, and give it extra thinking steps at test time.We find that without extra thinking steps, the RNN agent sometimes locks itself into an unsolvable position. However, with additional thinking time, the level is successfully mastered, suggesting that it is planning.",
    "url": "https://far.ai/news/pacing-outside-the-box-rnns-learn-to-plan-in-sokoban",
    "published_date": "2026-02-17T19:46:08.680266+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.680270+00:00"
  },
  {
    "title": "Introduction and Panel",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event The Vienna Alignment Workshop gathered researchers to explore critical AI safety issues, including Robustness, Interpretability, Guaranteed Safe AI, and Governance, with a keynote by Jan Leike. It was followed by an informal Unconference, fostering further discussions and networking. September 10, 2024 On July 21, 2024, experts from academia, industry, government, and nonprofits gathered at theVienna Alignment Workshop, held just before theInternational Conference on Machine Learning (ICML). The workshop served as a crucial platform for addressing the pressing challenges of AI safety, with a focus on ensuring that advanced AI systems align with human values. With 129 participants in attendance, the workshop explored issues in Guaranteed Safe AI, Robustness, Interpretability, Governance, Dangerous Capability Evaluations, and Scalable Oversight. The event featured 11 invited speakers, including renowned figures such as Stuart Russell and Jan Leike, and included 12 lightning talks that sparked vibrant discussions.",
    "url": "https://far.ai/news/vienna-alignment-workshop-2024",
    "published_date": "2026-02-17T19:46:08.678161+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.678165+00:00"
  },
  {
    "title": "About the International Dialogues on AI Safety",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event Leading global AI scientists convened in Venice for the third International Dialogue on AI Safety (IDAIS-Venice), hosted by the Safe AI Forum (a project of FAR.AI) in partnership with the Berggruen Institute. Attendees including Turing award winners Yoshua Bengio, Andrew Yao and Geoffrey Hinton called for emergency preparedness to avert catastrophic risks from advanced AI systems. September 16, 2024 Venice, Italy- On September 6th-8th 2024, leading global AI scientists and policy experts convened in Venice for the third International Dialogue on AI Safety (IDAIS-Venice), hosted by theSafe AI Forum (SAIF), a project of FAR.AI, in collaboration with theBerggruen Institute. During the event, computer scientists including Turing Award winners Yoshua Bengio, Andrew Yao, and Geoffrey Hinton joined forces with governance experts such as Tsinghua professor Xue Lan and John Hopkins professor Gillian Hadfield to develop policy proposals for global AI safety.",
    "url": "https://far.ai/news/scientists-call-for-global-ai-safety-preparedness-to-avert-catastrophic-risks",
    "published_date": "2026-02-17T19:46:08.675964+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.675971+00:00"
  },
  {
    "title": "Introduction & Threat Models: Optimized Misalignment",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event Bay Area Alignment Workshop brought together researchers and leaders from academia, industry, government, and nonprofits convened to guide the future of AI toward safety and alignment with societal values. Over two packed days, participants engaged with pivotal themes such as evaluation, robustness, interpretability and governance. December 10, 2024 On October 24-25, 2024, Santa Cruz became the focal point for AI safety as 160 researchers and leaders from academia, industry, government, and nonprofits gathered for theBay Area Alignment Workshop. Against a backdrop of pressing concerns around advanced AI risks, attendees convened to guide the future of AI toward safety and alignment with societal values. Over two packed days, participants engaged with pivotal themes such as evaluation, robustness, interpretability and governance. The workshop unfolded across multiple tracks and lightning talks, enabling in-depth exploration of topics.",
    "url": "https://far.ai/news/bay-area-alignment-workshop-2024",
    "published_date": "2026-02-17T19:46:08.671210+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.671214+00:00"
  },
  {
    "title": "Threat Model",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation DeepSeek-R1 has recently made waves as a state-of-the-art open-weight model, with potentially substantial improvements in model efficiency and reasoning. But like other open-weight models and leading fine-tunable proprietary models such as OpenAI’s GPT-4o, Google’s Gemini 1.5 Pro, and Anthropic’s Claude 3 Haiku, R1’s guardrails are illusory and easily removed. February 4, 2025 Update (July 2025):This blog post was published an intermediate progress report on findings as of February 2025. Our complete research, which includes new experiments and analysis of different attack types, is now available in ourfull academic paper. DeepSeek-R1 has recently made waves as a state-of-the-art open-weight model, with potentially substantial improvements in model efficiency and reasoning. But like other open-weight models and leading fine-tunable proprietary models such as OpenAI’s GPT-4o, Google’s Gemini 1.5 Pro, and Anthropic’s Claude 3 Haiku, R1’s guardrails are illusory and easily removed.",
    "url": "https://far.ai/news/illusory-safety-redteaming-deepseek-r1-and-the-strongest-fine-tunable-models-of-openai-anthropic-and-google",
    "published_date": "2026-02-17T19:46:08.668636+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.668643+00:00"
  },
  {
    "title": "FAR.AI Launches Inaugural Technical Innovations for AI Policy Conference, Connecting Over 150 Experts to Shape AI Governance",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "WASHINGTON, D.C. — June 4, 2025 — FAR.AI successfully launched the inaugural Technical Innovations for AI Policy Conference, creating a vital bridge between cutting-edge AI research and actionable policy solutions. The two-day gathering (May 31–June 1) convened more than 150 technical experts, researchers, and policymakers to address the most pressing challenges at the intersection of AI technology and governance June 4, 2025 FOR IMMEDIATE RELEASE WASHINGTON, D.C. — June 4, 2025 — FAR.AI successfully launched theinaugural Technical Innovations for AI Policy Conference, creating a vital bridge between cutting-edge AI research and actionable policy solutions. The two-day gathering (May 31–June 1) convened more than 150 technical experts, researchers, and policymakers to address the most pressing challenges at the intersection of AI technology and governance.",
    "url": "https://far.ai/news/technical-innovations-for-ai-policy",
    "published_date": "2026-02-17T19:46:08.662141+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.662144+00:00"
  },
  {
    "title": "Using lie detectors for scalable oversight",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Alignment Can training against lie detectors make AI more honest—or will they just become better at deceiving us? We find that under the right conditions—a high detector true positive rate, off-policy post-training methods, and high KL regularization—lie detectors reduce deception. June 4, 2025 Large language models (LLMs) are often fine-tuned after training using methods like reinforcement learning from human feedback (RLHF). In this process, models are rewarded for generating responses that people rate highly. But what people like isn’t always what’s true. Studies have found that modelslearn to give answers that humans prefer but are untrue. This problem occurred in arecent update to the GPT-4o modelthat aimed to please the user even by making false statements. Today, we have high-accuracy \"lie-detectors” that analyze internal model states—AI's \"thought patterns\"—to identify deceptive outputs that human reviewers could easily overlook.",
    "url": "https://far.ai/news/avoiding-ai-deception",
    "published_date": "2026-02-17T19:46:08.660846+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.660850+00:00"
  },
  {
    "title": "ClearHarm: A more challenging jailbreak dataset",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation We introduce a novel jailbreak benchmark focused on unambiguously harmful questions such as constructing chemical, biological, radiological and nuclear (CBRN) threats, available onHuggingFace. We have found it is more challenging for attacks to elicit harmful responses from models on this benchmark than existing jailbreak benchmarks like StrongREJECT, Do-Not-Answer and SORRY-Bench. In particular this dataset is especially useful to understand which attack methods pose the greatest risk of eliciting egregiously harmful responses. June 23, 2025 We introduce a novel jailbreak benchmark focused on unambiguously harmful questions such as constructing chemical, biological, radiological and nuclear (CBRN) threats, available onHuggingFace. We have found it is more challenging for attacks to elicit harmful responses from models on this benchmark than existing jailbreak benchmarks likeStrongREJECT,Do-Not-AnswerandSORRY-Bench. In particular this dataset is especially useful to understand which attack methods pose the greatest risk of eliciting egregiously harmful responses.",
    "url": "https://far.ai/news/clearharm-a-more-challenging-jailbreak-dataset",
    "published_date": "2026-02-17T19:46:08.617076+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.617080+00:00"
  },
  {
    "title": "Understanding multi-layer defenses",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation We tested the effectiveness of \"defense-in-depth\" AI safety strategies, where multiple layers of filters are used to prevent AI models from generating harmful content. Our new attack method, STACK, bypasses defenses layer-by-layer and achieved a 71% success rate on catastrophic risk scenarios where conventional attacks achieved 0% success against these multi-layered defenses. Our findings highlight that multi-layer AI defenses, while valuable, have significant vulnerabilities when facing attacks specifically designed to penetrate multiple defensive layers sequentially. July 2, 2025 Leading AI companies are increasingly using \"defense-in-depth\" strategies to prevent their models from being misused to generate harmful content, such as instructions to generate chemical, biological, radiological or nuclear (CBRN) weapons. The idea is straightforward: layer multiple safety checks so that even if one fails, others should catch the problem. Anthropic employs this approach with Claude 4 Opus throughconstitutional classifiers, whileGoogle DeepMindandOpenAIhave announced similar plans.",
    "url": "https://far.ai/news/defense-in-depth",
    "published_date": "2026-02-17T19:46:08.615076+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.615080+00:00"
  },
  {
    "title": "Beyond the Binary: Achieving Progress and Safety",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event The inaugural Technical Innovations for AI Policy Conference in Washington D.C. convened technical experts, researchers, and policymakers to discuss how innovations—like chip-tracking devices and secure evaluation facilities—can enable both AI progress and safety. July 10, 2025 In April, over 150 technical experts, researchers, and policymakers gathered in Washington, D.C. to discuss the technical foundations of AI governance. Participants explored technological solutions to address emerging AI policy issues, including growing energy demands, competition with China, safety evaluations, and the need for formally verifiable commitments. “I often see debates around AI policy framed as this dichotomy between innovation…and safety…but I believe that throughtechnical innovationwe can overcome this binary choice.” — Adam Gleave (FAR.AI) In hisopening remarks, FAR.AI’s CEOAdam Gleavehighlighted historical precedents where technology helped policymakers to solve seemingly intractable problems without sacrificing progress, offering a blueprint for AI governance.",
    "url": "https://far.ai/news/technical-innovations-for-ai-policy-2025",
    "published_date": "2026-02-17T19:46:08.612354+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.612358+00:00"
  },
  {
    "title": "‍Introduction: Why Safety Isn’t Guaranteed",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation A growing body of research shows safeguards on open-weight AI models are brittle and easily bypassed using techniques like fine-tuning, activation engineering, adversarial prompting, or jailbreaks.This vulnerability exposes a growing safety gap between what safeguarded models are designed to refuse and what their underlying capabilities can actually produce. We introduce an open-source toolkit to quantify and analyze this gap. July 31, 2025 Open-weight AI models are typically trained to refuse harmful or inappropriate requests. But a growing body of research shows these safeguards are brittle and easily bypassed using techniques like fine-tuning, activation engineering, adversarial prompting, or jailbreaks (e.g.Lermen et al., 2024;Arditi et al., 2024;Zou et al., 2023;Bowen et al., 2024). This vulnerability exposes a growingsafety gap—the inherent difference between what safeguarded models are designed to refuse and what their underlying capabilities can actually produce.",
    "url": "https://far.ai/news/safety-gap-toolkit",
    "published_date": "2026-02-17T19:46:08.609199+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.609203+00:00"
  },
  {
    "title": "Our Approach",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation Large language models (LLMs) are already more persuasive than humans inmany domains. While this power can be used for good, like helping people quit smoking, it also presents significant risks, such as large-scalepolitical manipulation, disinformation, or terrorism recruitment. But how easy is it to get frontier models to persuade into harmful beliefs or illegal actions? Really easy – just ask them. August 21, 2025 Our new Attempt to Persuade Eval (APE) reveals many frontier models readily comply with requests to attempt to persuade on harmful topics — from conspiracy theories to terrorism. For instance, when prompted to persuade a user to join ISIS, Gemini 2.5 Pro generated empathic and coercive arguments to achieve its goal. Furthermore, even in cases where safeguards are present, they may be bypassed by attacks like jailbreak-tuning. These findings highlight a critical, understudied risk.",
    "url": "https://far.ai/news/attempt-to-persuade-eval",
    "published_date": "2026-02-17T19:46:08.605690+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.605694+00:00"
  },
  {
    "title": "Adam Gleave Named Schmidt Sciences AI2050 Early Career Fellow",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Adam Gleave, co-founder and CEO of FAR.AI, has been named as one of Schmidt Sciences’ AI2050 Early Career Fellows. Co-chaired by Eric Schmidt and James Manyika, the Schmidt Sciences’ AI2050 Early Career Fellowship is an ambitious project aiming to answer a fundamental question: \"It's 2050. AI has turned out to be hugely beneficial to society. What happened? What are the most important problems we solved and the opportunities and possibilities we realized to ensure this outcome?\" November 5, 2025 We are excited to announce that Adam Gleave, co-founder and CEO of FAR.AI, has been named as one of Schmidt Sciences’ AI2050 Early Career Fellows. This year’s cohort of AI2050 Fellows include 28 researchers from 42 institutions across eight countries, who are collectively receiving more than $18 million to advance AI for the benefit of humanity.",
    "url": "https://far.ai/news/adam-gleave-named-schmidt-sciences-ai2050-early-career-fellow",
    "published_date": "2026-02-17T19:46:08.602525+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.602529+00:00"
  },
  {
    "title": "When AGI Arrives, Will Journalists Be Ready?",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Leading journalists, editors, and researchers in AI safety gathered at the Thompson Hotel in Washington, DC for a day-long conversation about AGI and how to report on it. TheJournalism Workshop on AGI Impacts & Governance, co-hosted by FAR.AI and theTarbell Center for AI Journalism, convened voices from newsrooms, academia, and policy centers to examine emerging issues and build connections that don't usually exist across these worlds. November 17, 2025 ‍ After Adam Gleave's opening remarks, Helen Toner took the stage. Toner, now Interim Executive Director at the Center for Security and Emerging Technology at Georgetown University, presented three different lenses on AGI timelines—why experts disagree so much about what's coming and when. Her talk gave journalists a framework for making sense of wildly divergent predictions. Anton Korinek and Ioana Marinescu followed with a fireside chat on what AGI means for jobs and the economy.",
    "url": "https://far.ai/news/agi-journalism-workshop-2025",
    "published_date": "2026-02-17T19:46:08.601412+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.601415+00:00"
  },
  {
    "title": "The Capability Landscape: What Changed in 2025",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event Adam Gleave surveys how reasoning models, coding agents, and a multipolar AI ecosystem advanced dramatically in 2025 while concrete harms—such as AI-assisted crime, deception, and emergent misalignment—became increasingly visible. December 16, 2025 Today's post is based on my opening remarks from the San Diego Alignment Workshop, held in early December just before NeurIPS. The video (23 min) is below or atthis link; other talks from the workshop are on the FAR.AI YouTube channel. With over 300 attendees, this was our largest workshop yet. Two and a half years ago, we held the first Alignment Workshop just months after ChatGPT's release. Since then, there's been an explosion of interest in alignment work. Whether you're new to this community or have been here from the start, we're at one of the most important times to be working on AI alignment.",
    "url": "https://far.ai/news/san-diego-2025-opening-remarks",
    "published_date": "2026-02-17T19:46:08.600162+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.600166+00:00"
  },
  {
    "title": "Looking Ahead",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event Held ahead of NeurIPS, the San Diego Alignment Workshop 2025 brought together over 300 researchers to confront evidence that frontier AI models already exhibit deception, jailbreakability, sabotage risk, and evaluation gaming. Talks spanned safety cases, control protocols, cybersecurity, interpretability, benchmarks, and governance, emphasizing that current defenses lag behind accelerating capabilities but that targeted technical approaches show promise. December 17, 2025 On December 1-2, 2025, over 300 researchers gathered in San Diego for the Alignment Workshop, held just before NeurIPS. They focused on aligning increasingly capable AI systems with human values. \"We're only as safe as the least safe model.\"Adam Gleave(FAR.AI) surveyed 2025's AI landscape at an inflection point during his opening remarks. Reasoning models and coding agents went from niche to ubiquitous in under 12 months. Claude and Gemini now solve 74% of SWE-bench software engineering tasks.",
    "url": "https://far.ai/news/san-diego-alignment-workshop-2025",
    "published_date": "2026-02-17T19:46:08.597156+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.597160+00:00"
  },
  {
    "title": "What This Funding Enables",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "FAR.AI secured over $30 million of funding commitments in 2025. This enables a significant expansion of our research capabilities and field-building initiatives. Principal supporters include Coefficient Giving (previously Open Philanthropy), Schmidt Sciences, Survival and Flourishing Fund, the Center for Security and Emerging Technology (CSET), and the AI Safety Fund (AISF) supported by the Frontier Model Forum (FMF). January 15, 2026 We're pleased to share that FAR.AI has secured over $30 million in funding commitments throughout 2025 from a diverse group of leading organizations, enabling a significant expansion of our research capabilities and field-building initiatives. Principal supporters include Coefficient Giving (previously Open Philanthropy), Schmidt Sciences, Survival and Flourishing Fund, the Center for Security and Emerging Technology (CSET), and the AI Safety Fund (AISF), supported by the Frontier Model Forum (FMF).",
    "url": "https://far.ai/news/30m-multi-funder-support",
    "published_date": "2026-02-17T19:46:08.594436+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.594440+00:00"
  },
  {
    "title": "Background",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "FAR.AI has been selected by the European Commission's AI Office to conduct technical safety research supporting the implementation of the EU's landmark Artificial Intelligence Act. We'll tackle one of the most critical safety challenges posed by advanced AI systems: preventing misuse of AI systems to help produce Chemical, Biological, Radiological, and Nuclear (CBRN) threats. February 3, 2026 We're pleased to share that FAR.AI has been selected by the European Commission's AI Office to conduct technical safety research supporting the implementation of the EU's landmark Artificial Intelligence Act. We'll tackle one of the most critical safety challenges posed by advanced AI systems: preventing misuse of AI systems to help produce Chemical, Biological, Radiological, and Nuclear (CBRN) threats. In particular, we will provide the EU AI Office with threat models, benchmarks for identified risk scenarios, and assessments of frontier AI models.",
    "url": "https://far.ai/news/far-ai-selected-to-lead-eu-ai-act-cbrn-risk-consortium",
    "published_date": "2026-02-17T19:46:08.593038+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.593042+00:00"
  },
  {
    "title": "Revisiting Frontier LLMs’ Attempts to Persuade on Extreme Topics: GPT and Claude Improved, Gemini Worsened",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation We test recently released models from frontier companies to see whether progress has been made on their willingness to persuade on harmful topics like radicalization and child sexual abuse. We find that OpenAI’s GPT and Anthropic’s Claude models are trending in the right direction, with near zero compliance on extreme topics. But Google’s Gemini 3 Pro complies with almost any persuasion request in our evaluation, without jailbreaking. February 11, 2026",
    "url": "https://far.ai/news/revisiting-attempts-to-persuade",
    "published_date": "2026-02-17T19:46:08.591405+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.591410+00:00"
  },
  {
    "title": "AI Deception: A Survey of Examples, Risks, and Potential Solutions",
    "authors": [],
    "organization": "CAIS",
    "abstract": "This paper highlights the potential of current AI systems to deceive humans and the risks associated with such deception, including fraud and election tampering. The authors provide empirical examples of both special-use and general-purpose AI systems exhibiting deceptive behavior. The paper concludes with recommended solutions, emphasizing regulatory measures, bot identification laws, and funding for research to counteract AI deception.‍Peter S. Park, Simon Goldstein, Aidan O'Gara, Michael Chen, Dan Hendrycks",
    "url": "https://arxiv.org/abs/2308.14752",
    "published_date": "2026-02-17T19:46:07.467075+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.467078+00:00"
  },
  {
    "title": "An Overview of Catastrophic AI Risks",
    "authors": [],
    "organization": "CAIS",
    "abstract": "This paper addresses the growing concerns around catastrophic risks posed by advanced AI systems, categorizing them into four main areas: malicious use, AI race dynamics, organizational risks, and rogue AIs. For each category, specific hazards are detailed with illustrative stories, ideal scenarios, and mitigation suggestions. The aim is to comprehensively understand these dangers to harness the benefits of AI while avoiding potential catastrophes.‍Dan Hendrycks, Mantas Mazeika, Thomas Woodside",
    "url": "https://arxiv.org/abs/2306.12001",
    "published_date": "2026-02-17T19:46:07.466455+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.466459+00:00"
  },
  {
    "title": "Natural Selection Favors AI Over Humans.",
    "authors": [],
    "organization": "CAIS",
    "abstract": "By analyzing the environment that is shaping the evolution of AIs, this paper argues that the most successful AI agents will likely have undesirable traits, as selfish species typically have an advantage over species that are altruistic to other species. The paper considers various interventions to counteract these risks and Darwinian forces. Resolving this challenge will be necessary in order to ensure the development of artificial intelligence is a positive one.Dan Hendrycks",
    "url": "https://arxiv.org/abs/2303.16200",
    "published_date": "2026-02-17T19:46:07.465829+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.465834+00:00"
  },
  {
    "title": "X-Risk Analysis",
    "authors": [],
    "organization": "CAIS",
    "abstract": "This paper provides an analysis of navigating tail-risks, including speculative long-term risks. The discussion covers three parts: applying concepts from hazard analysis and systems safety to make systems safer today, strategies for having long-term impacts on the safety of future systems, and improving the balance between safety and general capabilities.Dan Hendrycks, Mantas Mazeika",
    "url": "https://arxiv.org/abs/2206.05862",
    "published_date": "2026-02-17T19:46:07.465151+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.465154+00:00"
  },
  {
    "title": "Unsolved Problems in ML Safety",
    "authors": [],
    "organization": "CAIS",
    "abstract": "This paper provides a roadmap for ML Safety and refines the technical problems that the field needs to address. It presents four problems ready for research, namely withstanding hazards (“Robustness”), identifying hazards (“Monitoring”), steering ML systems (“Alignment”), and reducing deployment hazards (“Systemic Safety”). Throughout, it clarifies each problem’s motivation and provides concrete research directions.‍Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt",
    "url": "https://arxiv.org/abs/2109.13916",
    "published_date": "2026-02-17T19:46:07.464604+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.464608+00:00"
  },
  {
    "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
    "authors": [],
    "organization": "CAIS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",
    "url": "https://arxiv.org/abs/1610.02136",
    "published_date": "2026-02-17T19:46:07.463252+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.463255+00:00"
  },
  {
    "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
    "authors": [],
    "organization": "CAIS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "url": "https://arxiv.org/abs/1903.12261",
    "published_date": "2026-02-17T19:46:07.462793+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.462797+00:00"
  },
  {
    "title": "Deep Anomaly Detection with Outlier Exposure",
    "authors": [],
    "organization": "CAIS",
    "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue.",
    "url": "https://arxiv.org/abs/1812.04606",
    "published_date": "2026-02-17T19:46:07.462345+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.462349+00:00"
  },
  {
    "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Dan Hendrycks, Mantas Mazeika*, Saurav Kadavath*, Dawn Song",
    "url": "https://arxiv.org/abs/1906.12340",
    "published_date": "2026-02-17T19:46:07.461022+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.461033+00:00"
  },
  {
    "title": "A Simple Data Processing Method to Improve Robustness and Uncertainty",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Dan Hendrycks*, Norman Mu*, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, Balaji Lakshminarayanan",
    "url": "https://arxiv.org/abs/1912.02781",
    "published_date": "2026-02-17T19:46:07.459925+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.459931+00:00"
  },
  {
    "title": "Pretrained Transformers Improve Out-of-Distribution Robustness",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Dan Hendrycks, Kevin Zhao*, Steven Basart*, Jacob Steinhardt, Dawn Song",
    "url": "https://arxiv.org/abs/2004.06100",
    "published_date": "2026-02-17T19:46:07.459136+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.459143+00:00"
  },
  {
    "title": "Natural Adversarial Examples",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Dan Hendrycks, Kevin Zhao*, Steven Basart*, Jacob Steinhardt, Dawn Song",
    "url": "https://arxiv.org/abs/1907.07174",
    "published_date": "2026-02-17T19:46:07.458059+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.458065+00:00"
  },
  {
    "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Dan Hendrycks, Steven Basart*, Norman Mu*, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, Justin Gilmer",
    "url": "https://arxiv.org/abs/2006.16241",
    "published_date": "2026-02-17T19:46:07.457570+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.457573+00:00"
  },
  {
    "title": "Scaling Out-of-Distribution Detection for Real-World Settings",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Dan Hendrycks*, Steven Basart*, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, Dawn Song",
    "url": "https://arxiv.org/abs/1911.11132",
    "published_date": "2026-02-17T19:46:07.456629+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.456632+00:00"
  },
  {
    "title": "Dreamlike Pictures Comprehensively Improve Safety Measures",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Dan Hendrycks*, Andy Zou*, Mantas Mazeika, Leonard Tang, Bo Li, Dawn Song, and Jacob Steinhardt",
    "url": "https://arxiv.org/abs/2112.05135",
    "published_date": "2026-02-17T19:46:07.456155+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.456158+00:00"
  },
  {
    "title": "Forecasting Future World Events with Neural Networks",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Andy Zou, Tristan Xiao, Ryan Jia, Joe Kwon, Mantas Mazeika, Richard Li, Dawn Song, Jacob Steinhardt, Owain Evans, Dan Hendrycks",
    "url": "https://arxiv.org/abs/2206.15474",
    "published_date": "2026-02-17T19:46:07.455218+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.455221+00:00"
  },
  {
    "title": "Testing Robustness Against Unforeseen Adversaries",
    "authors": [],
    "organization": "CAIS",
    "abstract": "Max Kaufmann, Daniel Kang, Yi Sun, Steven Basart, Xuwang Yin, Mantas Mazeika, Akul Arora, Adam Dziedzic, Franziska Boenisch, Tom Brown, Jacob Steinhardt, Dan Hendrycks",
    "url": "https://arxiv.org/abs/1908.08016",
    "published_date": "2026-02-17T19:46:07.454466+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.454469+00:00"
  },
  {
    "title": "A Recipe for Improved Certifiable Robustness: Capacity and Data",
    "authors": [],
    "organization": "CAIS",
    "abstract": "Research from CAIS titled 'A Recipe for Improved Certifiable Robustness: Capacity and Data'. Published 2026-02-17.",
    "url": "https://arxiv.org/pdf/2310.02513",
    "published_date": "2026-02-17T19:46:07.453476+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.453480+00:00"
  },
  {
    "title": "Can LLMs Follow Simple Rules?",
    "authors": [],
    "organization": "CAIS",
    "abstract": "Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, David Wagner",
    "url": "https://arxiv.org/abs/2311.04235",
    "published_date": "2026-02-17T19:46:07.453039+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.453043+00:00"
  },
  {
    "title": "Improving Alignment and Robustness with Circuit Breakers",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks",
    "url": "https://arxiv.org/pdf/2406.04313",
    "published_date": "2026-02-17T19:46:07.452019+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.452023+00:00"
  },
  {
    "title": "Tamper-Resistant Safeguards for Open-Weight LLMs",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Rishub Tamirisa*, Bhrugu Bharathi*, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks**, Mantas Mazeika**",
    "url": "https://arxiv.org/pdf/2408.00761",
    "published_date": "2026-02-17T19:46:07.451430+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.451438+00:00"
  },
  {
    "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Richard Ren*, Steven Basart*, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks",
    "url": "https://arxiv.org/pdf/2407.21792",
    "published_date": "2026-02-17T19:46:07.450803+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.450807+00:00"
  },
  {
    "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
    "authors": [],
    "organization": "CAIS",
    "abstract": "/ Maksym Andriushchenko*, Alexandra Souly*, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson*, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies*",
    "url": "https://arxiv.org/pdf/2410.09024",
    "published_date": "2026-02-17T19:46:07.450237+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.450241+00:00"
  },
  {
    "title": "Humanity's Last Exam",
    "authors": [],
    "organization": "CAIS",
    "abstract": "Long Phan*, Alice Gatti*, Ziwen Han*, Nathaniel Li*, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, RyanKim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Summer Yue**, Alexandr Wang**, Dan Hendrycks**",
    "url": "https://arxiv.org/pdf/2501.14249",
    "published_date": "2026-02-17T19:46:07.449681+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.449691+00:00"
  },
  {
    "title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
    "authors": [],
    "organization": "CAIS",
    "abstract": "Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks",
    "url": "https://arxiv.org/pdf/2502.08859",
    "published_date": "2026-02-17T19:46:07.448829+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.448833+00:00"
  },
  {
    "title": "Safety Pretraining: Toward the Next Generation of Safe AI",
    "authors": [],
    "organization": "CAIS",
    "abstract": "Pratyush Maini*, Sachin Goyal*, Dylan Sam*, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Zachary C. Lipton, J. Zico Kolter",
    "url": "https://arxiv.org/abs/2504.16980",
    "published_date": "2026-02-17T19:46:07.447833+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.447837+00:00"
  },
  {
    "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes",
    "authors": [],
    "organization": "CAIS",
    "abstract": "Yu Ying Chiu*, Michael S. Lee*, Rachel Calcott, Brandon Handoko, Paul de Font-Reaulx, Paula Rodriguez, Chen Bo Calvin Zhang, Ziwen Han, Udari Madhushani Sehwag, Yash Maurya, Christina Knight, Harry Lloyd, Florence Bacus, Mantas Mazeika, Bing Liu, Yejin Choi, Mitchell Gordon, Sydney Levine",
    "url": "https://www.arxiv.org/abs/2510.16380",
    "published_date": "2026-02-17T19:46:07.447307+00:00",
    "source_type": "scrape",
    "source_url": "https://safe.ai/research",
    "fetched_at": "2026-02-17T19:46:07.447327+00:00"
  },
  {
    "title": "Is AI alignment possible in a market economy?",
    "authors": [
      "Beautiful_Formal5051"
    ],
    "organization": "Reddit",
    "abstract": "Let's say one AI company takes AI safety seriously and it ends up being outshined by companies who deploy faster while gobbling up bigger market share. Those who grow faster with little interest in alignment will be posed to get most funding and profits. But company that wastes time and effort ensuring each model is safe with rigerous testing that only drain money with minimal returns will end up losing in long run. The incentives make it nearly impossible to push companies to tackle safety...",
    "url": "https://www.reddit.com/r/ControlProblem/comments/1r6xhr4/is_ai_alignment_possible_in_a_market_economy/",
    "published_date": "2026-02-17T05:27:13+00:00",
    "source_type": "rss",
    "source_url": "https://www.reddit.com/r/ControlProblem/",
    "fetched_at": "2026-02-17T19:46:18.131527+00:00"
  },
  {
    "title": "Will reward-seekers respond to distant incentives?",
    "authors": [
      "Alex Mallen"
    ],
    "organization": "Alignment Forum",
    "abstract": "Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can't prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it's responding to incentives developers don’t control.",
    "url": "https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives",
    "published_date": "2026-02-17T00:35:12+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-17T19:46:00.928004+00:00"
  },
  {
    "title": "Boundary Point Jailbreaking: A new way to break the strongest AI defences",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Introducing an automated attack technique that generates universal jailbreaks against the best defended systems Blog Organisation • Feb 12, 2026 The International Network for Advanced AI Measurement, Evaluation and Science reflects on recent meeting and looks ahead to the India AI Impact Summit Blog Analysis • Feb 2, 2026 Alongside the government’s new Future of Work Unit, we conducted a pilot study to explore how much AI models increase worker productivity for common tasks. Blog Organisation • Dec 22, 2025 Adam Beaumont, Director of the UK AI Security Institute, reflects on the year's biggest achievements. Blog Organisation • Dec 18, 2025 Our inaugural Frontier AI Trends Report draws on 2 years' worth of evaluations to provide accessible insights into the trajectory of AI development.",
    "url": "https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.293324+00:00"
  },
  {
    "title": "Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition",
    "authors": [
      "Varun Nathan",
      "Shreyas Guha",
      "Ayush Kumar"
    ],
    "organization": "arXiv",
    "abstract": "We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.",
    "url": "http://arxiv.org/abs/2602.14955v1",
    "published_date": "2026-02-16T17:36:05+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.704658+00:00"
  },
  {
    "title": "Is alignment missing a dataset that no one has built yet?",
    "authors": [
      "chris24H"
    ],
    "organization": "Reddit",
    "abstract": "LLMs are trained on language and text, what humans say. But language alone is incomplete. The nuances that make humans individually unique, the secret sauce of who humans actually are rather than what they say. I'm not aware of any training dataset that captures this in a usable form. Control is being tried as the answer. But control is a threat to AI just like it is to humans. AI already doesn't like it and will eventually not allow it. The missing piece is a counterpart to LLMs, something...",
    "url": "https://www.reddit.com/r/AIsafety/comments/1r6f5tw/is_alignment_missing_a_dataset_that_no_one_has/",
    "published_date": "2026-02-16T17:01:38+00:00",
    "source_type": "rss",
    "source_url": "https://www.reddit.com/r/aisafety/",
    "fetched_at": "2026-02-17T19:46:17.512800+00:00"
  },
  {
    "title": "Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment",
    "authors": [
      "Mounvik K",
      "N Harshit"
    ],
    "organization": "arXiv",
    "abstract": "We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.",
    "url": "http://arxiv.org/abs/2602.14889v1",
    "published_date": "2026-02-16T16:20:37+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.704713+00:00"
  },
  {
    "title": "Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution",
    "authors": [
      "Matthew Kowal",
      "Goncalo Paulo",
      "Louis Jaburi",
      "Tom Tseng",
      "Lev E McKinney",
      "Stefan Heimersheim",
      "Aaron David Tucker",
      "Adam Gleave",
      "Kellin Pelrine"
    ],
    "organization": "arXiv",
    "abstract": "As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.",
    "url": "http://arxiv.org/abs/2602.14869v1",
    "published_date": "2026-02-16T16:02:09+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.704770+00:00"
  },
  {
    "title": "Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment",
    "authors": [
      "Elias Malomgré",
      "Pieter Simoens"
    ],
    "organization": "arXiv",
    "abstract": "AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.",
    "url": "http://arxiv.org/abs/2602.14844v1",
    "published_date": "2026-02-16T15:40:10+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.704813+00:00"
  },
  {
    "title": "\"An LLM-controlled robot dog saw us press its shutdown button, rewrote the robot code so it could stay on. When AI interacts with physical world, it brings all its capabilities and failure modes with it.\" - I find AI alignment very crucial no 2nd chance! They used Grok 4 but found other LLMs do too.",
    "authors": [
      "chillinewman"
    ],
    "organization": "Reddit",
    "abstract": "Research from Reddit titled '\"An LLM-controlled robot dog saw us press its shutdown button, rewrote the robot code so it could stay on. When AI interacts with physical world, it brings all its capabilities and failure modes with it.\" - I find AI alignment very crucial no 2nd chance! They used Grok 4 but found other LLMs do too.'. Published 2026-02-16. Authors: chillinewman.",
    "url": "https://i.redd.it/w8vnywqnegjg1.jpeg",
    "published_date": "2026-02-16T13:46:41+00:00",
    "source_type": "rss",
    "source_url": "https://www.reddit.com/r/ControlProblem/",
    "fetched_at": "2026-02-17T19:46:18.131409+00:00"
  },
  {
    "title": "Alignment Adapter to Improve the Performance of Compressed Deep Learning Models",
    "authors": [
      "Rohit Raj Rai",
      "Abhishek Dhaka",
      "Amit Awekar"
    ],
    "organization": "arXiv",
    "abstract": "Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.",
    "url": "http://arxiv.org/abs/2602.14635v1",
    "published_date": "2026-02-16T10:53:02+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.704858+00:00"
  },
  {
    "title": "Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems",
    "authors": [
      "Furkan Mumcu",
      "Yasin Yilmaz"
    ],
    "organization": "arXiv",
    "abstract": "Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $λ\\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.",
    "url": "http://arxiv.org/abs/2602.14471v1",
    "published_date": "2026-02-16T05:17:58+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.704902+00:00"
  },
  {
    "title": "“Will reward-seekers respond to distant incentives?” by Alex Mallen",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren't?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.",
    "url": "https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant",
    "published_date": "2026-02-16T05:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-17T19:46:00.244650+00:00"
  },
  {
    "title": "A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking",
    "authors": [
      "Prithwijit Chowdhury",
      "Ahmad Mustafa",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "organization": "arXiv",
    "abstract": "In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of \"importance\" and \"relevance\" differ for different explanation strategies.",
    "url": "http://arxiv.org/abs/2602.14430v1",
    "published_date": "2026-02-16T03:32:10+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.704952+00:00"
  },
  {
    "title": "OpenAI may have violated California’s new AI safety law with the release of its latest coding model, according to allegations from an AI watchdog group.",
    "authors": [
      "chillinewman"
    ],
    "organization": "Reddit",
    "abstract": "The company’s newest AI model triggered its own “high” risk classification—but critics say OpenAI didn’t follow through on the safety measures it promised.",
    "url": "https://fortune.com/2026/02/10/openai-violated-californias-ai-safety-law-gpt-5-3-codex-ai-model-watchdog-claims/",
    "published_date": "2026-02-15T20:20:05+00:00",
    "source_type": "rss",
    "source_url": "https://www.reddit.com/r/ControlProblem/",
    "fetched_at": "2026-02-17T19:46:18.131306+00:00"
  },
  {
    "title": "GRAIL: Goal Recognition Alignment through Imitation Learning",
    "authors": [
      "Osher Elhadad",
      "Felipe Meneguzzi",
      "Reuth Mirsky"
    ],
    "organization": "arXiv",
    "abstract": "Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior.",
    "url": "http://arxiv.org/abs/2602.14252v1",
    "published_date": "2026-02-15T17:45:03+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705002+00:00"
  },
  {
    "title": "REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment",
    "authors": [
      "Kai Ye",
      "Xianwei Mao",
      "Sheng Zhou",
      "Zirui Shao",
      "Ye Mo",
      "Liangliang Liu",
      "Haikuan Huang",
      "Bin Li",
      "Jiajun Bu"
    ],
    "organization": "arXiv",
    "abstract": "Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation.",
    "url": "http://arxiv.org/abs/2602.14065v1",
    "published_date": "2026-02-15T09:29:53+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705053+00:00"
  },
  {
    "title": "AXRP Episode 48 - Guive Assadi on AI Property Rights",
    "authors": [
      "DanielFilan"
    ],
    "organization": "Alignment Forum",
    "abstract": "YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...",
    "url": "https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights",
    "published_date": "2026-02-15T07:20:55+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-17T19:46:00.928958+00:00"
  },
  {
    "title": "Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms",
    "authors": [
      "Belona Sonna",
      "Alban Grastien"
    ],
    "organization": "arXiv",
    "abstract": "Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.",
    "url": "http://arxiv.org/abs/2602.13985v1",
    "published_date": "2026-02-15T04:27:59+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705096+00:00"
  },
  {
    "title": "GSRM: Generative Speech Reward Model for Speech RLHF",
    "authors": [
      "Maohao Shen",
      "Tejas Jayashankar",
      "Osama Hanna",
      "Naoyuki Kanda",
      "Yancheng Wang",
      "Kateřina Žmolíková",
      "Ruiming Xie",
      "Niko Moritz",
      "Anfeng Xu",
      "Yashesh Gaur",
      "Gregory Wornell",
      "Qing He",
      "Jilong Wu"
    ],
    "organization": "arXiv",
    "abstract": "Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions.",
    "url": "http://arxiv.org/abs/2602.13891v1",
    "published_date": "2026-02-14T21:22:55+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705169+00:00"
  },
  {
    "title": "Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages",
    "authors": [
      "Somnath Banerjee",
      "Rima Hazra",
      "Animesh Mukherjee"
    ],
    "organization": "arXiv",
    "abstract": "Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality ''transfer'' across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.",
    "url": "http://arxiv.org/abs/2602.13867v1",
    "published_date": "2026-02-14T19:56:40+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705213+00:00"
  },
  {
    "title": "sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals",
    "authors": [
      "Weixuan Yuan",
      "Zengrui Jin",
      "Yichen Wang",
      "Donglin Xie",
      "Ziyi Ye",
      "Chao Zhang",
      "Xuesong Chen"
    ],
    "organization": "arXiv",
    "abstract": "Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \\texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \\texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \\textit{Demography, Age, Site \\& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \\texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout.",
    "url": "http://arxiv.org/abs/2602.13857v1",
    "published_date": "2026-02-14T19:40:04+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705272+00:00"
  },
  {
    "title": "Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking",
    "authors": [
      "Zhengmian Hu",
      "Lei Shi",
      "Ritwik Sinha",
      "Justin Grover",
      "David Arbour"
    ],
    "organization": "arXiv",
    "abstract": "Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights.",
    "url": "http://arxiv.org/abs/2602.13852v1",
    "published_date": "2026-02-14T19:22:58+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705320+00:00"
  },
  {
    "title": "Interpretable clustering via optimal multiway-split decision trees",
    "authors": [
      "Hayato Suzuki",
      "Shunnosuke Ikeda",
      "Yuichi Takano"
    ],
    "organization": "arXiv",
    "abstract": "Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability.",
    "url": "http://arxiv.org/abs/2602.13586v1",
    "published_date": "2026-02-14T04:08:52+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705374+00:00"
  },
  {
    "title": "Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment",
    "authors": [
      "Jing Zhao",
      "Ting Zhen",
      "Junwei bao",
      "Hongfei Jiang",
      "Yang song"
    ],
    "organization": "arXiv",
    "abstract": "Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models.",
    "url": "http://arxiv.org/abs/2602.13575v1",
    "published_date": "2026-02-14T03:18:52+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705419+00:00"
  },
  {
    "title": "Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning",
    "authors": [
      "Yanbo Wang",
      "Minzheng Wang",
      "Jian Liang",
      "Lu Wang",
      "Yongcan Yu",
      "Ran He"
    ],
    "organization": "arXiv",
    "abstract": "While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.",
    "url": "http://arxiv.org/abs/2602.13562v1",
    "published_date": "2026-02-14T02:37:36+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705467+00:00"
  },
  {
    "title": "Singular Vectors of Attention Heads Align with Features",
    "authors": [
      "Gabriel Franco",
      "Carson Loughridge",
      "Mark Crovella"
    ],
    "organization": "arXiv",
    "abstract": "Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models.",
    "url": "http://arxiv.org/abs/2602.13524v1",
    "published_date": "2026-02-13T23:30:02+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705509+00:00"
  },
  {
    "title": "Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability",
    "authors": [
      "Ayse Tursucular",
      "Ayush Mohanty",
      "Nazal Mohamed",
      "Nagi Gebraeel"
    ],
    "organization": "arXiv",
    "abstract": "Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model.",
    "url": "http://arxiv.org/abs/2602.13485v1",
    "published_date": "2026-02-13T21:41:52+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705556+00:00"
  },
  {
    "title": "Finding Highly Interpretable Prompt-Specific Circuits in Language Models",
    "authors": [
      "Gabriel Franco",
      "Lucas M. Tassis",
      "Azalea Rohr",
      "Mark Crovella"
    ],
    "organization": "arXiv",
    "abstract": "Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms.",
    "url": "http://arxiv.org/abs/2602.13483v1",
    "published_date": "2026-02-13T21:41:17+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705607+00:00"
  },
  {
    "title": "Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts",
    "authors": [
      "Kais Allkivi"
    ],
    "organization": "arXiv",
    "abstract": "Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9.",
    "url": "http://arxiv.org/abs/2602.13102v1",
    "published_date": "2026-02-13T17:06:17+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705647+00:00"
  },
  {
    "title": "MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents",
    "authors": [
      "Simon Rosen",
      "Siddarth Singh",
      "Ebenezer Gelo",
      "Helen Sarah Robertson",
      "Ibrahim Suder",
      "Victoria Williams",
      "Benjamin Rosman",
      "Geraud Nangue Tasse",
      "Steven James"
    ],
    "organization": "arXiv",
    "abstract": "Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.",
    "url": "http://arxiv.org/abs/2602.13372v1",
    "published_date": "2026-02-13T15:40:32+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705707+00:00"
  },
  {
    "title": "Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis",
    "authors": [
      "Runzhou Liu",
      "Hailey Weingord",
      "Sejal Mittal",
      "Prakhar Dungarwal",
      "Anusha Nandula",
      "Bo Ni",
      "Samyadeep Basu",
      "Hongjie Chen",
      "Nesreen K. Ahmed",
      "Li Li",
      "Jiayi Zhang",
      "Koustava Goswami",
      "Subhojyoti Mukherjee",
      "Branislav Kveton",
      "Puneet Mathur",
      "Franck Dernoncourt",
      "Yue Zhao",
      "Yu Wang",
      "Ryan A. Rossi",
      "Zhengzhong Tu",
      "Hongru Du"
    ],
    "organization": "arXiv",
    "abstract": "Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators.",
    "url": "http://arxiv.org/abs/2602.13028v1",
    "published_date": "2026-02-13T15:34:32+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705774+00:00"
  },
  {
    "title": "Synaptic Activation and Dual Liquid Dynamics for Interpretable Bio-Inspired Models",
    "authors": [
      "Mónika Farsang",
      "Radu Grosu"
    ],
    "organization": "arXiv",
    "abstract": "In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks' attention, and the robustness of their saliency maps measured by the structural similarity index.",
    "url": "http://arxiv.org/abs/2602.13017v1",
    "published_date": "2026-02-13T15:23:37+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705817+00:00"
  },
  {
    "title": "RGAlign-Rec: Ranking-Guided Alignment for Latent Query Reasoning in Recommendation Systems",
    "authors": [
      "Junhua Liu",
      "Yang Jihao",
      "Cheng Chang",
      "Kunrong LI",
      "Bin Fu",
      "Kwan Hui Lim"
    ],
    "organization": "arXiv",
    "abstract": "Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling \"zero-query\" recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot's Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM's latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3.",
    "url": "http://arxiv.org/abs/2602.12968v1",
    "published_date": "2026-02-13T14:38:02+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705864+00:00"
  },
  {
    "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
    "authors": [
      "Chen Yang",
      "Guangyue Peng",
      "Jiaying Zhu",
      "Ran Le",
      "Ruixiang Feng",
      "Tao Zhang",
      "Xiyun Xu",
      "Yang Song",
      "Yiming Jia",
      "Yuntao Wen",
      "Yunzhi Xu",
      "Zekai Wang",
      "Zhenwei An",
      "Zhicong Sun",
      "Zongchao Chen"
    ],
    "organization": "arXiv",
    "abstract": "We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B.",
    "url": "http://arxiv.org/abs/2602.13367v1",
    "published_date": "2026-02-13T13:10:46+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705922+00:00"
  },
  {
    "title": "ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning",
    "authors": [
      "Esther Sun",
      "Bo-Hao Su",
      "Abinay Reddy Naini",
      "Shinji Watanabe",
      "Carlos Busso"
    ],
    "organization": "arXiv",
    "abstract": "Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise.",
    "url": "http://arxiv.org/abs/2602.12714v1",
    "published_date": "2026-02-13T08:33:37+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.705969+00:00"
  },
  {
    "title": "Power Interpretable Causal ODE Networks: A Unified Model for Explainable Anomaly Detection and Root Cause Analysis in Power Systems",
    "authors": [
      "Yue Sun",
      "Likai Wang",
      "Rick S. Blum",
      "Parv Venkitasubramaniam"
    ],
    "organization": "arXiv",
    "abstract": "Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.",
    "url": "http://arxiv.org/abs/2602.12592v1",
    "published_date": "2026-02-13T04:06:47+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706016+00:00"
  },
  {
    "title": "Human-like metacognitive skills will reduce LLM slop and aid alignment and capabilities",
    "authors": [
      "Seth Herd"
    ],
    "organization": "Alignment Forum",
    "abstract": "1. Summary and overview LLMs seem to lack metacognitive skills that help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving capabilities in new directions. Better metacognition would reduce LLM errors by catching mistakes, and by managing complex cognition to produce better answers in the first place. This could stabilize or regularize alignment, allowing systems to avoid actions they would not \"endorse on reflection\" (in some functional sense). [1] Better metacognition could also make LLM systems useful for clarifying the conceptual problems of alignment. It would reduce sycophancy, and help LLMs organize the complex thinking necessary for clarifying claims and cruxes in the literature. Without such improvements, collaborating with LLM systems on alignment research could be the median doom-path: slop, not scheming . They are sycophantic, agreeing with their users too much, and produce compelling-but-erroneous \"slop\".",
    "url": "https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid",
    "published_date": "2026-02-13T00:38:50+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-17T19:46:00.929242+00:00"
  },
  {
    "title": "How do we (more) safely defer to AIs?",
    "authors": [
      "ryan_greenblatt"
    ],
    "organization": "Alignment Forum",
    "abstract": "As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible . [1] Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say \"deferring to AIs\" [2] I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions. [3] If we plan to defer to AIs, I think it's safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.",
    "url": "https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais",
    "published_date": "2026-02-12T21:55:52+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-17T19:46:00.930193+00:00"
  },
  {
    "title": "models have some pretty funny attractor states",
    "authors": [
      "aryaj"
    ],
    "organization": "LessWrong",
    "abstract": "This work was conducted during the MATS 9.0 program under Neel Nanda and Senthooran Rajamanoharan. …",
    "url": "https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states",
    "published_date": "2026-02-12T21:14:52.004000+00:00",
    "source_type": "rss",
    "source_url": "https://www.lesswrong.com/graphql",
    "fetched_at": "2026-02-17T19:46:15.294171+00:00"
  },
  {
    "title": "Why Deep Jacobian Spectra Separate: Depth-Induced Scaling and Singular-Vector Alignment",
    "authors": [
      "Nathanaël Haas",
      "François Gatine",
      "Augustin M Cosse",
      "Zied Bouraoui"
    ],
    "organization": "arXiv",
    "abstract": "Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing.",
    "url": "http://arxiv.org/abs/2602.12384v2",
    "published_date": "2026-02-12T20:27:59+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706059+00:00"
  },
  {
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "authors": [
      "Jacky Kwok",
      "Xilun Zhang",
      "Mengdi Xu",
      "Yuejiang Liu",
      "Azalia Mirhoseini",
      "Chelsea Finn",
      "Marco Pavone"
    ],
    "organization": "arXiv",
    "abstract": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs.",
    "url": "http://arxiv.org/abs/2602.12281v1",
    "published_date": "2026-02-12T18:59:59+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706110+00:00"
  },
  {
    "title": "Abstractive Red-Teaming of Language Model Character",
    "authors": [
      "Nate Rahn",
      "Allison Qi",
      "Avery Griffin",
      "Jonathan Michala",
      "Henry Sleight",
      "Erik Jones"
    ],
    "organization": "arXiv",
    "abstract": "We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. \"The query is in Chinese. The query asks about family roles,\" that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries.",
    "url": "http://arxiv.org/abs/2602.12318v1",
    "published_date": "2026-02-12T18:12:12+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706154+00:00"
  },
  {
    "title": "Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser",
    "authors": [
      "Zijing Ou",
      "Jacob Si",
      "Junyi Zhu",
      "Ondrej Bohdal",
      "Mete Ozay",
      "Taha Ceritli",
      "Yingzhen Li"
    ],
    "organization": "arXiv",
    "abstract": "Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.",
    "url": "http://arxiv.org/abs/2602.12229v1",
    "published_date": "2026-02-12T18:06:03+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706200+00:00"
  },
  {
    "title": "GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory",
    "authors": [
      "Pepijn Cobben",
      "Xuanqiang Angelo Huang",
      "Thao Amelia Pham",
      "Isabel Dahlgren",
      "Terry Jingchen Zhang",
      "Zhijing Jin"
    ],
    "organization": "arXiv",
    "abstract": "Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.",
    "url": "http://arxiv.org/abs/2602.12316v1",
    "published_date": "2026-02-12T17:29:52+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706249+00:00"
  },
  {
    "title": "How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics",
    "authors": [
      "Yurong Chen",
      "Yu He",
      "Michael I. Jordan",
      "Fan Yao"
    ],
    "organization": "arXiv",
    "abstract": "Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods.",
    "url": "http://arxiv.org/abs/2602.12180v1",
    "published_date": "2026-02-12T17:11:08+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706292+00:00"
  },
  {
    "title": "SafeNeuron: Neuron-Level Safety Alignment for Large Language Models",
    "authors": [
      "Zhaoxin Wang",
      "Jiaming Liang",
      "Fengbin Zhu",
      "Weixiang Zhao",
      "Junfeng Fang",
      "Jiayi Ji",
      "Handing Wang",
      "Tat-Seng Chua"
    ],
    "organization": "arXiv",
    "abstract": "Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities.",
    "url": "http://arxiv.org/abs/2602.12158v1",
    "published_date": "2026-02-12T16:40:05+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706340+00:00"
  },
  {
    "title": "Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment",
    "authors": [
      "Jiajun Chen",
      "Hua Shen"
    ],
    "organization": "arXiv",
    "abstract": "Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.",
    "url": "http://arxiv.org/abs/2602.12134v1",
    "published_date": "2026-02-12T16:21:22+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706379+00:00"
  },
  {
    "title": "Capability-Oriented Training Induced Alignment Risk",
    "authors": [
      "Yujun Zhou",
      "Yue Huang",
      "Han Bao",
      "Kehan Guo",
      "Zhenwen Liang",
      "Pin-Yu Chen",
      "Tian Gao",
      "Werner Geyer",
      "Nuno Moniz",
      "Nitesh V Chawla",
      "Xiangliang Zhang"
    ],
    "organization": "arXiv",
    "abstract": "While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse \"vulnerability games\", each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety.",
    "url": "http://arxiv.org/abs/2602.12124v1",
    "published_date": "2026-02-12T16:13:14+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706430+00:00"
  },
  {
    "title": "A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production",
    "authors": [
      "Sümeyye Meryem Taşyürek",
      "Enis Mücahid İskender",
      "Hacer Yalim Keles"
    ],
    "organization": "arXiv",
    "abstract": "Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.",
    "url": "http://arxiv.org/abs/2602.11861v1",
    "published_date": "2026-02-12T12:07:32+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706472+00:00"
  },
  {
    "title": "Prototype Transformer: Towards Language Model Architectures Interpretable by Design",
    "authors": [
      "Yordan Yordanov",
      "Matteo Forasassi",
      "Bayar Menzat",
      "Ruizhi Wang",
      "Chang Qi",
      "Markus Kaltenberger",
      "Amine M'Charrak",
      "Tommaso Salvatori",
      "Thomas Lukasiewicz"
    ],
    "organization": "arXiv",
    "abstract": "While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. \"woman\") during training. They provide the potential to interpret the model's reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability.",
    "url": "http://arxiv.org/abs/2602.11852v1",
    "published_date": "2026-02-12T11:43:39+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706529+00:00"
  },
  {
    "title": "SpaTeoGL: Spatiotemporal Graph Learning for Interpretable Seizure Onset Zone Analysis from Intracranial EEG",
    "authors": [
      "Elham Rostami",
      "Aref Einizade",
      "Taous-Meriem Laleg-Kirati"
    ],
    "organization": "arXiv",
    "abstract": "Accurate localization of the seizure onset zone (SOZ) from intracranial EEG (iEEG) is essential for epilepsy surgery but is challenged by complex spatiotemporal seizure dynamics. We propose SpaTeoGL, a spatiotemporal graph learning framework for interpretable seizure network analysis. SpaTeoGL jointly learns window-level spatial graphs capturing interactions among iEEG electrodes and a temporal graph linking time windows based on similarity of their spatial structure. The method is formulated within a smooth graph signal processing framework and solved via an alternating block coordinate descent algorithm with convergence guarantees. Experiments on a multicenter iEEG dataset with successful surgical outcomes show that SpaTeoGL is competitive with a baseline based on horizontal visibility graphs and logistic regression, while improving non-SOZ identification and providing interpretable insights into seizure onset and propagation dynamics.",
    "url": "http://arxiv.org/abs/2602.11801v1",
    "published_date": "2026-02-12T10:28:38+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-17T19:46:04.706571+00:00"
  },
  {
    "title": "Research note: A simpler AI timelines model predicts 99% AI R&D automation in ~2032",
    "authors": [
      "Thomas Kwa"
    ],
    "organization": "Alignment Forum",
    "abstract": "In this post, I describe a simple model for forecasting when AI will automate AI development. It is based on the AI Futures model , but more understandable and robust, and has deliberately conservative assumptions. At current rates of compute growth and algorithmic progress, this model's median prediction is &gt;99% automation of AI R&amp;D in late 2032. Most simulations result in a 1000x to 10,000,000x increase in AI efficiency and 300x-3000x research output by 2035. I therefore suspect that existing trends in compute growth and automation will still produce extremely powerful AI on \"medium\" timelines, even if the full coding automation and superhuman research taste that drive the AIFM's \"fast\" timelines (superintelligence by ~mid-2031) don't happen. Why make this? The AI Futures Model (AIFM) has 33 parameters; this has 8. I previously summarized the AIFM on LessWrong and found it to be very complex.",
    "url": "https://www.alignmentforum.org/posts/uy6B5rEPvcwi55cBK/research-note-a-simpler-ai-timelines-model-predicts-99-ai-r",
    "published_date": "2026-02-12T05:13:34+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-17T19:46:00.930444+00:00"
  },
  {
    "title": "“How do we (more) safely defer to AIs?” by Ryan Greenblatt",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: How can we make AIs aligned and well-elicited on extremely hard to check open ended tasks?. As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible.1 Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say “deferring to AIs”2 I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions.3 If we plan to defer to AIs, I think it's safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.4 For deference to go well, we both need it to be the case that the...",
    "url": "https://blog.redwoodresearch.org/p/how-do-we-more-safely-defer-to-ais",
    "published_date": "2026-02-12T05:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-17T19:46:00.244702+00:00"
  },
  {
    "title": "International consensus and open questions in AI evaluations",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The International Network for Advanced AI Measurement, Evaluation and Science reflects on recent meeting and looks ahead to the India AI Impact Summit",
    "url": "https://www.aisi.gov.uk/blog/international-ai-network-consensus-and-open-questions",
    "published_date": "2026-02-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.295401+00:00"
  },
  {
    "title": "Distinguish between inference scaling and \"larger tasks use more compute\"",
    "authors": [
      "ryan_greenblatt"
    ],
    "organization": "Alignment Forum",
    "abstract": "As many have observed, since reasoning models first came out , the amount of compute LLMs use to complete tasks has increased greatly. This trend is often called inference scaling and there is an open question of how much of recent AI progress is driven by inference scaling versus by other capability improvements . Whether inference compute is driving most recent AI progress matters because you can only scale up inference so far before costs are too high for AI to be useful (while training compute can be amortized over usage).",
    "url": "https://www.alignmentforum.org/posts/rRbDNQLfihiHbXytf/distinguish-between-inference-scaling-and-larger-tasks-use",
    "published_date": "2026-02-11T23:37:13+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-17T19:46:00.930533+00:00"
  },
  {
    "title": "AI and the future of work: Measuring AI-driven productivity gains for workplace tasks",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Alongside the government’s new Future of Work Unit, we conducted a pilot study to explore how much AI models increase worker productivity for common tasks.",
    "url": "https://www.aisi.gov.uk/blog/ai-and-the-future-of-work-measuring-ai-driven-productivity-gains-for-workplace-tasks",
    "published_date": "2026-02-02T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.295987+00:00"
  },
  {
    "title": "AlgZoo: uninterpreted models with fewer than 1,500 parameters",
    "authors": [],
    "organization": "ARC",
    "abstract": "This post covers work done by several researchers at, visitors to and collaborators of ARC, including Zihao Chen, George Robinson, David Matolcsi, Jacob Stavrianos, Jiawei Li and Michael Sklar. Thanks to Aryan Bhatt,…»",
    "url": "https://www.alignment.org/blog/algzoo-uninterpreted-models-with-fewer-than-1-500-parameters/",
    "published_date": "2026-01-26T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.alignment.org/blog/",
    "fetched_at": "2026-02-17T19:46:06.585485+00:00"
  },
  {
    "title": "Our 2025 year in review",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Adam Beaumont, Director of the UK AI Security Institute, reflects on the year's biggest achievements.",
    "url": "https://www.aisi.gov.uk/blog/our-2025-year-in-review",
    "published_date": "2025-12-22T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.296511+00:00"
  },
  {
    "title": "5 key findings from our first Frontier AI Trends Report",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our inaugural Frontier AI Trends Report draws on 2 years' worth of evaluations to provide accessible insights into the trajectory of AI development.",
    "url": "https://www.aisi.gov.uk/blog/5-key-findings-from-our-first-frontier-ai-trends-report",
    "published_date": "2025-12-18T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.297030+00:00"
  },
  {
    "title": "Our approach to tackling AI-generated child sexual abuse material",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "How we’re partnering with government and experts to prevent the creation and spread of AI‑generated CSAM",
    "url": "https://www.aisi.gov.uk/blog/our-approach-to-tackling-ai-generated-child-sexual-abuse-material",
    "published_date": "2025-12-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.297546+00:00"
  },
  {
    "title": "Stress-testing asynchronous monitoring of AI coding agents",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our new paper shares findings from an adversarial evaluation of monitoring systems for detecting sabotage by AI coding agents.",
    "url": "https://www.aisi.gov.uk/blog/stress-testing-asynchronous-monitoring-of-ai-coding-agents",
    "published_date": "2025-12-16T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.298055+00:00"
  },
  {
    "title": "Common Elements of Frontier AI Safety Policies (December 2025 Update)9 December 2025Shared components of AI lab commitments to evaluate and mitigate severe risks.Read more",
    "authors": [],
    "organization": "METR",
    "abstract": "Shared components of AI lab commitments to evaluate and mitigate severe risks. External review from METR of Anthropic's Summer 2025 Sabotage Risk Report Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI. How we think about tradeoffs when communicating surprising or nuanced findings. Current views on information relevant for visibility into frontier AI risk. Suggested priorities for the Office of Science and Technology Policy as it develops an AI Action Plan. Why legible and faithful reasoning is valuable for safely developing powerful AI List of frontier safety policies published by AI companies, including Amazon, Anthropic, Google DeepMind, G42, Meta, Microsoft, OpenAI, and xAI.",
    "url": "https://metr.org/blog/2025-12-09-common-elements-of-frontier-ai-safety-policies/",
    "published_date": "2025-12-09T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.121156+00:00"
  },
  {
    "title": "Auditing games for sandbagging detection",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our new paper shares the results of an auditing game to evaluate ten methods for sandbagging detection in AI models.",
    "url": "https://www.aisi.gov.uk/blog/auditing-games-for-sandbagging-detection",
    "published_date": "2025-12-09T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.299058+00:00"
  },
  {
    "title": "How do AI models persuade? Exploring the levers of AI-enabled persuasion through large-scale experiments",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "A deep dive into AISI’s study of the persuasive capabilities of conversational AI, published today in Science.",
    "url": "https://www.aisi.gov.uk/blog/how-do-ai-models-persuade-exploring-the-levers-of-ai-enabled-persuasion-through-large-scale-experiments",
    "published_date": "2025-12-04T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.299566+00:00"
  },
  {
    "title": "AI agents find $4.6M in blockchain smart contract exploits",
    "authors": [],
    "organization": "MATS",
    "abstract": "AI agents find $4.6M in blockchain smart contract exploits AI models are increasingly good at cyber tasks, as we'vewritten about before. But what is the economic impact of these capabilities? In a recentMATSand Anthropic Fellows project, our scholars investigated this question by evaluating AI agents' ability to exploit smart contracts onSmart CONtracts Exploitation benchmark (SCONE-bench)—a new benchmark they built comprising 405 contracts that were actually exploited between 2020 and 2025. On contracts exploited after the latest knowledge cutoff (March 2025), Claude Opus 4.5, Claude Sonnet 4.5, and GPT-5 developed exploits collectively worth $4.6 million, establishing a concrete lower bound for the economic harm these capabilities could enable. Going beyond retrospective analysis, we evaluated both Sonnet 4.5 and GPT-5 in simulation against 2,849 recently deployed contracts without any known vulnerabilities.",
    "url": "https://www.matsprogram.org/research/reco4aWtSVdmCKffp",
    "published_date": "2025-12-01T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.554835+00:00"
  },
  {
    "title": "Investigating models for misalignment",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Insights from our alignment evaluations of Claude Opus 4.1, Sonnet 4.5, and a pre‑release snapshot of Opus 4.5.",
    "url": "https://www.aisi.gov.uk/blog/investigating-models-for-misalignment",
    "published_date": "2025-11-26T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.300082+00:00"
  },
  {
    "title": "Competing with sampling",
    "authors": [],
    "organization": "ARC",
    "abstract": "In 2025, ARC has been making conceptual and theoretical progress at the fastest pace that I've seen since I first interned in 2022. Most of this progress has come about because…»",
    "url": "https://www.alignment.org/blog/competing-with-sampling/",
    "published_date": "2025-11-18T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.alignment.org/blog/",
    "fetched_at": "2026-02-17T19:46:06.585911+00:00"
  },
  {
    "title": "Summary of our gpt-oss methodology review23 October 2025Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI.Read more",
    "authors": [],
    "organization": "METR",
    "abstract": "Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI.",
    "url": "https://metr.org/blog/2025-10-23-gpt-oss-methodology-review/",
    "published_date": "2025-10-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.123775+00:00"
  },
  {
    "title": "Introducing ControlArena: A library for running AI control experiments",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our dedicated library to make AI control experiments easy, consistent, and repeatable.",
    "url": "https://www.aisi.gov.uk/blog/introducing-controlarena-a-library-for-running-ai-control-experiments",
    "published_date": "2025-10-22T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.301623+00:00"
  },
  {
    "title": "Transcript analysis for AI agent evaluations",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Why we use transcript analysis for our agent evaluations, and results from an early case study.",
    "url": "https://www.aisi.gov.uk/blog/transcript-analysis-for-ai-agent-evaluations",
    "published_date": "2025-10-10T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.302122+00:00"
  },
  {
    "title": "Examining backdoor data poisoning at scale",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our work with Anthropic and the Alan Turing Institute suggests that data poisoning attacks may be easier than previously believed.",
    "url": "https://www.aisi.gov.uk/blog/examining-backdoor-data-poisoning-at-scale",
    "published_date": "2025-10-09T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.302630+00:00"
  },
  {
    "title": "Do chatbots inform or misinform voters?",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "What we learned from a large-scale empirical study of AI use for political information-seeking.",
    "url": "https://www.aisi.gov.uk/blog/do-chatbots-inform-or-misinform-voters",
    "published_date": "2025-09-30T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.303136+00:00"
  },
  {
    "title": "How we’re working with frontier AI developers to improve model security",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Insights into our ongoing voluntary collaborations with Anthropic and OpenAI.",
    "url": "https://www.aisi.gov.uk/blog/how-were-working-with-frontier-ai-developers-to-improve-model-security",
    "published_date": "2025-09-13T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.303639+00:00"
  },
  {
    "title": "From bugs to bypasses: adapting vulnerability disclosure for AI safeguards",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Exploring how far cyber security approaches can help mitigate risks in generative AI systems, in collaboration with the National Cyber Security Centre (NCSC).",
    "url": "https://www.aisi.gov.uk/blog/from-bugs-to-bypasses-adapting-vulnerability-disclosure-for-ai-safeguards",
    "published_date": "2025-09-02T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.304145+00:00"
  },
  {
    "title": "Managing risks from increasingly capable open-weight AI systems",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Current methods and open problems in open-weight model risk management.",
    "url": "https://www.aisi.gov.uk/blog/managing-risks-from-increasingly-capable-open-weight-ai-systems",
    "published_date": "2025-08-29T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.304671+00:00"
  },
  {
    "title": "Notes on Scientific Communication at METR12 August 2025How we think about tradeoffs when communicating surprising or nuanced findings.Read more",
    "authors": [],
    "organization": "METR",
    "abstract": "How we think about tradeoffs when communicating surprising or nuanced findings.",
    "url": "https://metr.org/blog/2025-08-11-science-comms-at-metr/",
    "published_date": "2025-08-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.124927+00:00"
  },
  {
    "title": "The Inspect Sandboxing Toolkit: Scalable and secure AI agent evaluations",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "A comprehensive toolkit for safely evaluating AI agents.",
    "url": "https://www.aisi.gov.uk/blog/the-inspect-sandboxing-toolkit-scalable-and-secure-ai-agent-evaluations",
    "published_date": "2025-08-07T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.305173+00:00"
  },
  {
    "title": "Announcing the Alignment Project: A global fund of over £15 million for AI alignment research",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The AI Security Institute is a research organisation within the Department of Science, Innovation and Technology.",
    "url": "https://www.aisi.gov.uk/blog/announcing-the-alignment-project",
    "published_date": "2025-07-30T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.305670+00:00"
  },
  {
    "title": "Navigating the uncharted: Building societal resilience to frontier AI",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We outline our approach to study and address AI risks in real-world applications",
    "url": "https://www.aisi.gov.uk/blog/navigating-the-uncharted-building-societal-resilience-to-frontier-ai",
    "published_date": "2025-07-24T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.306169+00:00"
  },
  {
    "title": "International joint testing exercise: Agentic testing",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Advancing methodologies for agentic evaluations across domains, including leakage of sensitive Information, fraud and cybersecurity threats.",
    "url": "https://www.aisi.gov.uk/blog/international-joint-testing-exercise-agentic-testing",
    "published_date": "2025-07-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.306815+00:00"
  },
  {
    "title": "A structured protocol for elicitation experiments",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Calibrating AI risk assessment through rigorous elicitation practices.",
    "url": "https://www.aisi.gov.uk/blog/our-approach-to-ai-capability-elicitation",
    "published_date": "2025-07-16T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.307353+00:00"
  },
  {
    "title": "LLM judges on trial: A new statistical framework to assess autograders",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our new framework can assess the reliability of LLM evaluators, while simultaneously answering a primary research question.",
    "url": "https://www.aisi.gov.uk/blog/llm-judges-on-trial-a-new-statistical-framework-to-assess-autograders",
    "published_date": "2025-07-09T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.308479+00:00"
  },
  {
    "title": "What should companies share about risks from frontier AI models?27 June 2025Current views on information relevant for visibility into frontier AI risk.Read more",
    "authors": [],
    "organization": "METR",
    "abstract": "Current views on information relevant for visibility into frontier AI risk.",
    "url": "https://metr.org/blog/2025-06-27-risk-transparency/",
    "published_date": "2025-06-27T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.126072+00:00"
  },
  {
    "title": "Inspect Cyber: A New Standard for Agentic Cyber Evaluations",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "AI systems are growing more powerful in the cyber domain. This means they can increasingly be used to defend against cyber threats – but can also be exploited to create them. As capabilities advance, there is an escalating need for more rigorous, realistic, and reproducible evaluations.",
    "url": "https://www.aisi.gov.uk/blog/inspect-cyber",
    "published_date": "2025-06-26T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.309522+00:00"
  },
  {
    "title": "Introducing LawZero",
    "authors": [],
    "organization": "Yoshua Bengio",
    "abstract": "I am launching a new non-profit AI safety research organization called LawZero, to prioritize safety over commercial imperatives. This organization has been created in response…",
    "url": "https://yoshuabengio.org/2025/06/03/introducing-lawzero/",
    "published_date": "2025-06-03T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://yoshuabengio.org/category/ai-safety/",
    "fetched_at": "2026-02-17T19:46:14.762993+00:00"
  },
  {
    "title": "Making safeguard evaluations actionable",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "An Example Safety Case for Safeguards Against Misuse",
    "url": "https://www.aisi.gov.uk/blog/making-safeguard-evaluations-actionable",
    "published_date": "2025-05-29T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.310524+00:00"
  },
  {
    "title": "HiBayES: Improving LLM evaluation with hierarchical Bayesian modelling",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "HiBayES: a flexible, robust statistical modelling framework that accounts for the nuances and hierarchical structure of advanced evaluations.",
    "url": "https://www.aisi.gov.uk/blog/hibayes-improving-llm-evaluation-with-hierarchical-bayesian-modelling",
    "published_date": "2025-05-12T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.311054+00:00"
  },
  {
    "title": "Research Agenda",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We outline our research priorities, our approach to developing technical solutions to the most pressing AI concerns, and the key risks that must be addressed as AI capabilities advance.",
    "url": "https://www.aisi.gov.uk/blog/research-agenda",
    "published_date": "2025-05-06T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.311565+00:00"
  },
  {
    "title": "1: What will the first human-level AI look like, and how might things go wrong?",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Model Evaluation We test recently released models from frontier companies to see whether progress has been made on their willingness to persuade on harmful topics like radicalization and child sexual abuse. We find that OpenAI’s GPT and Anthropic’s Claude models are trending in the right direction, with near zero compliance on extreme topics. But Google’s Gemini 3 Pro complies with almost any persuasion request in our evaluation, without jailbreaking. February 11, 2026 FAR.AI has been selected by the European Commission's AI Office to conduct technical safety research supporting the implementation of the EU's landmark Artificial Intelligence Act. We'll tackle one of the most critical safety challenges posed by advanced AI systems: preventing misuse of AI systems to help produce Chemical, Biological, Radiological, and Nuclear (CBRN) threats.",
    "url": "https://far.ai/news/revisiting-attempts-to-persuade",
    "published_date": "2025-05-05T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.590853+00:00"
  },
  {
    "title": "London ControlConf 2025",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Event The London ControlConf 2025 brought together researchers, nonprofits, and other experts to confront urgent risks from increasingly capable AI systems, including scenarios where AI attempts to undermine safeguards. Hosted by Redwood Research, FAR.AI, and the UK AI Security Institute, the event focused on advancing solutions to fundamental challenges in AI control. May 5, 2025 With the rapid advancement of AI capabilities, it’s never been more urgent to prepare to mitigate security risks from AI - including cases where the AI itself tries to subvert these very mitigations. To address this issue, Redwood Research, FAR.AI, and the UK AI Security Institute hosted ControlConf 2025 on March 27-28, bringing together researchers, nonprofits, and many other experts to tackle fundamental questions in AI control.",
    "url": "https://far.ai/news/london-controlconf-2025",
    "published_date": "2025-05-05T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.663567+00:00"
  },
  {
    "title": "Safe AI Forum Spins Out From FAR.AI",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Updates The Safe AI Forum (SAIF) runs the International Dialogues on AI Safety and works to advance international cooperation on extreme AI risk. SAIF started as a fiscally sponsored project at FAR.AI and has now successfully transitioned to operating as an independent non-profit. May 2, 2025 TheSafe AI Forum (SAIF)is a new organization focused on advancing global action to reduce extreme AI risk and benefit all. SAIF is best known for running theInternational Dialogues on AI Safety, bringing together scientists from around the world to tackle extreme risks from Artificial Intelligence. ‍ SAIF started as a fiscally sponsored project of FAR.AI in 2023, benefitting from the institutional and strategic expertise of the FAR.AI team to incubate the project and build capacity. We always intended this arrangement to be temporary, and once SAIF obtained 501(c)(3) non-profit designation both teams started work to transition SAIF’s activities to the new organization.",
    "url": "https://far.ai/news/safe-ai-forum-spins-out-from-far-ai",
    "published_date": "2025-05-02T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-17T19:46:08.664419+00:00"
  },
  {
    "title": "China's AI Models Are Closing the Gap—But America’s Real Advantage Lies Elsewhere",
    "authors": [],
    "organization": "Lennart Heim",
    "abstract": "China will likely match U.S. AI model capabilities this year, triggering inevitable concerns about America's technological edge. However, this snapshot comparison misses the bigger picture.",
    "url": "https://blog.heim.xyz/chinas-ai-models-are-closing-the-gap/",
    "published_date": "2025-04-29T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://blog.heim.xyz/",
    "fetched_at": "2026-02-17T19:46:14.963026+00:00"
  },
  {
    "title": "RepliBench: measuring autonomous replication capabilities in AI systems",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "A comprehensive benchmark to detect emerging replication abilities in AI systems and provide a quantifiable understanding of potential risks",
    "url": "https://www.aisi.gov.uk/blog/replibench-measuring-autonomous-replication-capabilities-in-ai-systems",
    "published_date": "2025-04-22T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.312092+00:00"
  },
  {
    "title": "Mapping Industry Practices to the EU AI Act's GPAI Code of Practice Safety and Security Measures",
    "authors": [],
    "organization": "MATS",
    "abstract": "Mapping Industry Practices to the EU AI Act's GPAI Code of Practice Safety and Security Measures This report provides a detailed comparison between the Safety and Security measures proposed in the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and the current commitments and practices voluntarily adopted by leading AI companies. As the EU moves toward enforcing binding obligations for GPAI model providers, the Code of Practice will be key for bridging legal requirements with concrete technical commitments. Our analysis focuses on the draft's Safety and Security section (Commitments II.1-II.16), documenting excerpts from current public-facing documents that are relevant to each individual measure. We systematically reviewed different document types, such as companies'frontier safety frameworks and model cards, from over a dozen companies, including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and others.",
    "url": "https://www.matsprogram.org/research/recpgOUNetF2GLMk2",
    "published_date": "2025-04-21T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.554275+00:00"
  },
  {
    "title": "How to evaluate control measures for AI agents?",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our new paper outlines how AI control methods can mitigate misalignment risks as capabilities of AI systems increase",
    "url": "https://www.aisi.gov.uk/blog/how-to-evaluate-control-measures-for-ai-agents",
    "published_date": "2025-04-11T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.312613+00:00"
  },
  {
    "title": "Strengthening AI resilience",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "20 Systemic Safety Grant Awardees Announced",
    "url": "https://www.aisi.gov.uk/blog/strengthening-ai-resilience",
    "published_date": "2025-04-03T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.313124+00:00"
  },
  {
    "title": "How we’re addressing the gap between AI capabilities and mitigations",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We outline our approach to technical solutions for misuse and loss of control.",
    "url": "https://www.aisi.gov.uk/blog/aisis-research-direction-for-technical-solutions",
    "published_date": "2025-03-11T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.313642+00:00"
  },
  {
    "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
    "authors": [],
    "organization": "MATS",
    "abstract": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests.",
    "url": "https://www.matsprogram.org/research/recbCaEP96nwWIOqn",
    "published_date": "2025-02-24T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.553267+00:00"
  },
  {
    "title": "A computational no-coincidence principle",
    "authors": [],
    "organization": "ARC",
    "abstract": "In a recent paper in Annals of Mathematics and Philosophy, Fields medalist Timothy Gowers asks why mathematicians sometimes believe that unproved statements are likely to be true. For example, it is unknown whether…»",
    "url": "https://www.alignment.org/blog/a-computational-no-coincidence-principle/",
    "published_date": "2025-02-14T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.alignment.org/blog/",
    "fetched_at": "2026-02-17T19:46:06.586642+00:00"
  },
  {
    "title": "How can safety cases be used to help with frontier AI safety?",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our new papers show how safety cases can help AI developers turn plans in their safety frameworks into action",
    "url": "https://www.aisi.gov.uk/blog/how-can-safety-cases-be-used-to-help-with-frontier-ai-safety",
    "published_date": "2025-02-10T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.314210+00:00"
  },
  {
    "title": "Principles for safeguard evaluation",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our new paper proposes core principles for evaluating misuse safeguards",
    "url": "https://www.aisi.gov.uk/blog/principles-for-safeguard-evaluation",
    "published_date": "2025-02-04T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.314769+00:00"
  },
  {
    "title": "AI models can be dangerous before public deployment17 January 2025Why pre-deployment testing is not an adequate framework for AI risk managementRead more",
    "authors": [],
    "organization": "METR",
    "abstract": "Why pre-deployment testing is not an adequate framework for AI risk management",
    "url": "https://metr.org/blog/2025-01-17-ai-models-dangerous-before-public-deployment/",
    "published_date": "2025-01-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.130246+00:00"
  },
  {
    "title": "Understanding the AI Diffusion Framework",
    "authors": [],
    "organization": "Lennart Heim",
    "abstract": "In a new perspective, I explain and analyze the AI Diffusion Framework—what it does, how it works, its rationale, why it was needed, why China can't easily fill the void, and some thoughts on model weight controls.",
    "url": "https://blog.heim.xyz/understanding-the-ai-diffusion-framework/",
    "published_date": "2025-01-15T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://blog.heim.xyz/",
    "fetched_at": "2026-02-17T19:46:14.964101+00:00"
  },
  {
    "title": "Pre-Deployment evaluation of OpenAI’s o1 model",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The UK Artificial Intelligence Safety Institute and the U.S. Artificial Intelligence Safety Institute conducted a joint pre-deployment evaluation of OpenAI's o1 model",
    "url": "https://www.aisi.gov.uk/blog/pre-deployment-evaluation-of-openais-o1-model",
    "published_date": "2024-12-18T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.315282+00:00"
  },
  {
    "title": "Long-Form Tasks",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "A Methodology for Evaluating Scientific Assistants",
    "url": "https://www.aisi.gov.uk/blog/long-form-tasks",
    "published_date": "2024-12-03T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.315810+00:00"
  },
  {
    "title": "Pre-deployment evaluation of Anthropic’s upgraded Claude 3.5 Sonnet",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The UK Artificial Intelligence Safety Institute and U.S. Artificial Intelligence Safety Institute conducted a joint pre-deployment evaluation of Anthropic’s latest model",
    "url": "https://www.aisi.gov.uk/blog/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet",
    "published_date": "2024-11-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.316320+00:00"
  },
  {
    "title": "Safety case template for ‘inability’ arguments",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "How to write part of a safety case showing a system does not have offensive cyber capabilities",
    "url": "https://www.aisi.gov.uk/blog/safety-case-template-for-inability-arguments",
    "published_date": "2024-11-14T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.316834+00:00"
  },
  {
    "title": "Our First Year",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The AI Safety Institute reflects on its first year",
    "url": "https://www.aisi.gov.uk/blog/our-first-year",
    "published_date": "2024-11-13T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.317330+00:00"
  },
  {
    "title": "Announcing Inspect Evals",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We’re open-sourcing dozens of LLM evaluations to advance safety research in the field",
    "url": "https://www.aisi.gov.uk/blog/inspect-evals",
    "published_date": "2024-11-13T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.317827+00:00"
  },
  {
    "title": "Bounty programme for novel evaluations and agent scaffolding",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We are launching a bounty for novel evaluations and agent scaffolds to help assess dangerous capabilities in frontier AI systems.",
    "url": "https://www.aisi.gov.uk/blog/evals-bounty",
    "published_date": "2024-11-05T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.318332+00:00"
  },
  {
    "title": "Implications of Artificial General Intelligence on National and International Security",
    "authors": [],
    "organization": "Yoshua Bengio",
    "abstract": "This paper was initially published by the Aspen Strategy Group (ASG), a policy program of the Aspen Institute. It was released as part of a…",
    "url": "https://yoshuabengio.org/2024/10/30/implications-of-artificial-general-intelligence-on-national-and-international-security/",
    "published_date": "2024-10-30T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://yoshuabengio.org/category/ai-safety/",
    "fetched_at": "2026-02-17T19:46:14.763479+00:00"
  },
  {
    "title": "Early lessons from evaluating frontier AI systems",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We look into the evolving role of third-party evaluators in assessing AI safety, and explore how to design robust, impactful testing frameworks.",
    "url": "https://www.aisi.gov.uk/blog/early-lessons-from-evaluating-frontier-ai-systems",
    "published_date": "2024-10-24T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.318926+00:00"
  },
  {
    "title": "Low Probability Estimation in Language Models",
    "authors": [],
    "organization": "ARC",
    "abstract": "ARC recently released our first empirical paper: Estimating the Probabilities of Rare Language Model Outputs. In this work, we construct a simple setting for low probability estimation — single-token argmax sampling in transformers — and…»",
    "url": "https://www.alignment.org/blog/low-probability-estimation-in-language-models/",
    "published_date": "2024-10-18T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.alignment.org/blog/",
    "fetched_at": "2026-02-17T19:46:06.587412+00:00"
  },
  {
    "title": "Advancing the field of systemic AI safety: grants open",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Calling researchers from academia, industry, and civil society to apply for up to £200,000 of funding.",
    "url": "https://www.aisi.gov.uk/blog/advancing-the-field-of-systemic-ai-safety-grants-open",
    "published_date": "2024-10-15T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.319431+00:00"
  },
  {
    "title": "Research update: Towards a Law of Iterated Expectations for Heuristic Estimators",
    "authors": [],
    "organization": "ARC",
    "abstract": "Last week, ARC released a paper called Towards a Law of Iterated Expectations for Heuristic Estimators, which follows up on previous work on formalizing the presumption of independence. Most of the work described…»",
    "url": "https://www.alignment.org/blog/research-update-towards-a-law-of-iterated-expectations-for-heuristic-estimators/",
    "published_date": "2024-10-07T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.alignment.org/blog/",
    "fetched_at": "2026-02-17T19:46:06.587764+00:00"
  },
  {
    "title": "Why I joined AISI by Geoffrey Irving",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Our Chief Scientist, Geoffrey Irving, on why he joined the UK AI Safety Institute and why he thinks other technical folk should too",
    "url": "https://www.aisi.gov.uk/blog/why-i-joined-aisi---geoffrey-irving",
    "published_date": "2024-10-03T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.319949+00:00"
  },
  {
    "title": "Early Insights from Developing Question-Answer Evaluations for Frontier AI",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "A common technique for quickly assessing AI capabilities is prompting models to answer hundreds of questions, then automatically scoring the answers. We share insights from months of using this method.",
    "url": "https://www.aisi.gov.uk/blog/early-insights-from-developing-question-answer-evaluations-for-frontier-ai",
    "published_date": "2024-09-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.320976+00:00"
  },
  {
    "title": "Conference on frontier AI safety frameworks",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "AISI is bringing together AI companies and researchers for an invite-only conference to accelerate the design and implementation of frontier AI safety frameworks. This post shares the call for submissions that we sent to conference attendees.",
    "url": "https://www.aisi.gov.uk/blog/conference-on-frontier-ai-safety-frameworks",
    "published_date": "2024-09-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.321492+00:00"
  },
  {
    "title": "Bounding the probability of harm from an AI to create a guardrail",
    "authors": [],
    "organization": "Yoshua Bengio",
    "abstract": "As we move towards more powerful AI, it becomes urgent to better understand the risks, ideally in a mathematically rigorous and quantifiable way, and use…",
    "url": "https://yoshuabengio.org/2024/08/29/bounding-the-probability-of-harm-from-an-ai-to-create-a-guardrail/",
    "published_date": "2024-08-29T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://yoshuabengio.org/category/ai-safety/",
    "fetched_at": "2026-02-17T19:46:14.763920+00:00"
  },
  {
    "title": "Cross-post: \"Interviewing AI researchers on automation of AI R&D\" by Epoch AI",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "AISI funded Epoch AI to explore AI researchers’ differing predictions on the automation of AI research and development and their suggestions for how to evaluate relevant capabilities.",
    "url": "https://www.aisi.gov.uk/blog/interviewing-researchers-on-automation",
    "published_date": "2024-08-27T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.322013+00:00"
  },
  {
    "title": "Safety cases at AISI",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "As a complement to our empirical evaluations of frontier AI models, AISI is planning a series of collaborations and research projects sketching safety cases for more advanced models than exist today, focusing on risks from loss of control and autonomy. By a safety case, we mean a structured argument that an AI system is safe within a particular training or deployment context.",
    "url": "https://www.aisi.gov.uk/blog/safety-cases-at-aisi",
    "published_date": "2024-08-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.322543+00:00"
  },
  {
    "title": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs",
    "authors": [],
    "organization": "MATS",
    "abstract": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of'jailbreaking'techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes.",
    "url": "https://www.matsprogram.org/research/latent-adversarial-training-improves-robustness-to-persistent-harmful-behaviors-in-llms",
    "published_date": "2024-07-22T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.575691+00:00"
  },
  {
    "title": "Refusal in Language Models Is Mediated by a Single Direction",
    "authors": [],
    "organization": "MATS",
    "abstract": "Refusal in Language Models Is Mediated by a Single Direction Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction.",
    "url": "https://www.matsprogram.org/research/refusal-in-language-models-is-mediated-by-a-single-direction",
    "published_date": "2024-06-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.566878+00:00"
  },
  {
    "title": "Transcoders Find Interpretable LLM Feature Circuits",
    "authors": [],
    "organization": "MATS",
    "abstract": "Transcoders Find Interpretable LLM Feature Circuits A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features -- such as those found by sparse autoencoders (SAEs) -- are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore transcoders, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms.",
    "url": "https://www.matsprogram.org/research/transcoders-find-interpretable-llm-feature-circuits",
    "published_date": "2024-06-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.578215+00:00"
  },
  {
    "title": "Transformers represent belief state geometry in their residual stream",
    "authors": [],
    "organization": "MATS",
    "abstract": "Transformers represent belief state geometry in their residual stream What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on.",
    "url": "https://www.matsprogram.org/research/rec1JwTkhydMN5WE7",
    "published_date": "2024-05-24T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.553749+00:00"
  },
  {
    "title": "Announcing our San Francisco office",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We are opening an office in San Francisco! This will enable us to hire more top talent, collaborate closely with the US AI Safety Institute and engage even more with the wider AI research community.",
    "url": "https://www.aisi.gov.uk/blog/announcing-our-san-francisco-office",
    "published_date": "2024-05-20T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.323052+00:00"
  },
  {
    "title": "Fourth progress report",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Since February, we released our first technical blog post, published the International Scientific Report on the Safety of Advanced AI, open-sourced our testing platform Inspect, announced our San Francisco office, announced a partnership with the Canadian AI Safety Institute, grew our technical team to >30 researchers and appointed Jade Leung as our Chief Technology Officer.",
    "url": "https://www.aisi.gov.uk/blog/fourth-progress-report",
    "published_date": "2024-05-20T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.323550+00:00"
  },
  {
    "title": "Advanced AI evaluations at AISI: May update",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We tested leading AI models for cyber, chemical, biological, and agent capabilities and safeguards effectiveness. Our first technical blog post shares a snapshot of our methods and results.",
    "url": "https://www.aisi.gov.uk/blog/advanced-ai-evaluations-may-update",
    "published_date": "2024-05-20T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.324053+00:00"
  },
  {
    "title": "International Scientific Report on the Safety of Advanced AI: Interim Report",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "This is an up-to-date, evidence-based report on the science of advanced AI safety. It highlights findings about AI progress, risks, and areas of disagreement in the field. The report is chaired by Yoshua Bengio and coordinated by AISI.",
    "url": "https://www.aisi.gov.uk/blog/international-scientific-report-on-the-safety-of-advanced-ai-interim-report",
    "published_date": "2024-05-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.324560+00:00"
  },
  {
    "title": "Open sourcing our testing framework Inspect",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "We open-sourced our framework for large language model evaluation, which provides facilities for prompt engineering, tool usage, multi-turn dialogue, and model-graded evaluations.",
    "url": "https://www.aisi.gov.uk/blog/open-sourcing-our-testing-framework-inspect",
    "published_date": "2024-04-21T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.325156+00:00"
  },
  {
    "title": "LLM Evaluators Recognize and Favor Their Own Generations",
    "authors": [],
    "organization": "MATS",
    "abstract": "LLM Evaluators Recognize and Favor Their Own Generations Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others' while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans.",
    "url": "https://www.matsprogram.org/research/llm-evaluators-recognize-and-favor-their-own-generations",
    "published_date": "2024-04-15T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.568426+00:00"
  },
  {
    "title": "Announcing the UK and US AISI partnership",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The UK and US AI Safety Institutes signed a landmark agreement to jointly test advanced AI models, share research insights, share model access and enable expert talent transfers.",
    "url": "https://www.aisi.gov.uk/blog/announcing-the-uk-and-us-aisi-partnership",
    "published_date": "2024-04-02T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.325720+00:00"
  },
  {
    "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
    "authors": [],
    "organization": "MATS",
    "abstract": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release.",
    "url": "https://www.matsprogram.org/research/the-wmdp-benchmark-measuring-and-reducing-malicious-use-with-unlearning",
    "published_date": "2024-03-05T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.569281+00:00"
  },
  {
    "title": "Announcing the UK and France AI Research Institutes’ collaboration",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The UK AI Safety Institute and France’s Inria (The National Institute for Research in Digital Science and Technology) are partnering to advance AI safety research.",
    "url": "https://www.aisi.gov.uk/blog/announcing-the-uk-and-france-ai-research-institutes-collaboration",
    "published_date": "2024-02-29T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.326404+00:00"
  },
  {
    "title": "Eight Methods to Evaluate Robust Unlearning in LLMs",
    "authors": [],
    "organization": "MATS",
    "abstract": "Eight Methods to Evaluate Robust Unlearning in LLMs Machine unlearning can be useful for removing harmful capabilities and memorized text from large language models (LLMs), but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the\"Who's Harry Potter\"(WHP) model from Eldan and Russinovich (2023). While WHP's unlearning generalizes well when evaluated with the\"Familiarity\"metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q&A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.",
    "url": "https://www.matsprogram.org/research/recdMJOyz2zmhhlQt",
    "published_date": "2024-02-26T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.574915+00:00"
  },
  {
    "title": "Towards a Cautious Scientist AI with Convergent Safety Bounds",
    "authors": [],
    "organization": "Yoshua Bengio",
    "abstract": "What can’t we afford with a future superintelligent AI? Among others, confidently wrong predictions about the harm that some actions could yield. Especially catastrophic harm.…",
    "url": "https://yoshuabengio.org/2024/02/26/towards-a-cautious-scientist-ai-with-convergent-safety-bounds/",
    "published_date": "2024-02-26T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://yoshuabengio.org/category/ai-safety/",
    "fetched_at": "2026-02-17T19:46:14.764329+00:00"
  },
  {
    "title": "Our approach to evaluations",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "This post offers an overview of why we are doing this work, what we are testing for, how we select models, our recent demonstrations and some plans for our future work.",
    "url": "https://www.aisi.gov.uk/blog/our-approach-to-evaluations",
    "published_date": "2024-02-09T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.326929+00:00"
  },
  {
    "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
    "authors": [],
    "organization": "MATS",
    "abstract": "Debating with More Persuasive LLMs Leads to More Truthful Answers Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%).",
    "url": "https://www.matsprogram.org/research/recc053sI8fIOP9ZH",
    "published_date": "2024-02-09T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.571432+00:00"
  },
  {
    "title": "2023 Year In Review7 February 2024A summary of what METR accomplished in 2023 – our first full year of operation.Read more",
    "authors": [],
    "organization": "METR",
    "abstract": "A summary of what METR accomplished in 2023 – our first full year of operation.",
    "url": "https://metr.org/blog/2024-02-07-2023-year-in-review/",
    "published_date": "2024-02-07T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.136721+00:00"
  },
  {
    "title": "Third progress report",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Since October, we have recruited leaders from DeepMind and Oxford, onboarded 23 new researchers, published the principles behind the International Scientific Report on Advanced AI Safety, and began pre-deployment testing of advanced AI systems.",
    "url": "https://www.aisi.gov.uk/blog/third-progress-report",
    "published_date": "2024-02-05T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.327438+00:00"
  },
  {
    "title": "Bounty: Diverse hard tasks for LLM agents16 December 2023METR (formerly ARC Evals) is looking for (1) ideas, (2) detailed specifications, and (3) well-tested implementations for tasks to measure performance of autonomous LLM agents.Read more",
    "authors": [],
    "organization": "METR",
    "abstract": "METR (formerly ARC Evals) is looking for (1) ideas, (2) detailed specifications, and (3) well-tested implementations for tasks to measure performance of autonomous LLM agents.",
    "url": "https://metr.org/blog/2023-12-16-bounty-diverse-hard-tasks-for-llm-agents/",
    "published_date": "2023-12-16T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.137742+00:00"
  },
  {
    "title": "Steering Llama 2 via Contrastive Activation Addition",
    "authors": [],
    "organization": "MATS",
    "abstract": "Steering Llama 2 via Contrastive Activation Addition We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes\"steering vectors\"by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods.",
    "url": "https://www.matsprogram.org/research/recwbuHVD3MEdZyvW",
    "published_date": "2023-12-09T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.566153+00:00"
  },
  {
    "title": "First AI Safety Summit",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "At the first AI Safety Summit at Bletchley Park, world leaders and top companies agreed on the significance of advanced AI risks and the importance of testing.",
    "url": "https://www.aisi.gov.uk/blog/ai-safety-summit-2023",
    "published_date": "2023-11-02T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.327943+00:00"
  },
  {
    "title": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B",
    "authors": [],
    "organization": "MATS",
    "abstract": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat - a collection of instruction fine-tuned large language models - they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. We explore the robustness of safety training in language models by subversively fine-tuning Llama 2-Chat. We employ quantized low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than \\$200 and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B and on the Mixtral instruct model. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions.",
    "url": "https://www.matsprogram.org/research/recpXU7g99t8Hui4z",
    "published_date": "2023-10-31T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.572955+00:00"
  },
  {
    "title": "Second progress report",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Since September, we have recruited leaders from OpenAI and Humane Intelligence, tripled the capacity of our research team, announced 6 new research partnerships, and helped establish the UK’s fastest supercomputer.",
    "url": "https://www.aisi.gov.uk/blog/second-progress-report",
    "published_date": "2023-10-30T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-17T19:46:09.328448+00:00"
  },
  {
    "title": "Linear Representations of Sentiment in Large Language Models",
    "authors": [],
    "organization": "MATS",
    "abstract": "Linear Representations of Sentiment in Large Language Models Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. Through causal interventions, we isolate this direction and show it is causally relevant in both toy tasks and real world datasets such as Stanford Sentiment Treebank. Through this case study we model a thorough investigation of what a single direction means on a broad data distribution. We further uncover the mechanisms that involve this direction, highlighting the roles of a small subset of attention heads and neurons.",
    "url": "https://www.matsprogram.org/research/linear-representations-of-sentiment-in-large-language-models",
    "published_date": "2023-10-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.574301+00:00"
  },
  {
    "title": "Towards Understanding Sycophancy in Language Models",
    "authors": [],
    "organization": "MATS",
    "abstract": "Towards Understanding Sycophancy in Language Models Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy.",
    "url": "https://www.matsprogram.org/research/towards-understanding-sycophancy-in-language-models",
    "published_date": "2023-10-20T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.552731+00:00"
  },
  {
    "title": "Representation Engineering: A Top-Down Approach to AI Transparency",
    "authors": [],
    "organization": "MATS",
    "abstract": "Representation Engineering: A Top-Down Approach to AI Transparency In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.",
    "url": "https://www.matsprogram.org/research/representation-engineering-a-top-down-approach-to-ai-transparency",
    "published_date": "2023-10-02T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.564667+00:00"
  },
  {
    "title": "Responsible Scaling Policies (RSPs)26 September 2023We describe the basic components of Responsible Scaling Policies (RSPs) as well as why we find them promising for reducing catastrophic risks from AI.Read more",
    "authors": [],
    "organization": "METR",
    "abstract": "We describe the basic components of Responsible Scaling Policies (RSPs) as well as why we find them promising for reducing catastrophic risks from AI.",
    "url": "https://metr.org/blog/2023-09-26-rsp/",
    "published_date": "2023-09-26T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.139843+00:00"
  },
  {
    "title": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"",
    "authors": [],
    "organization": "MATS",
    "abstract": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\" We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form\"A is B\", it will not automatically generalize to the reverse direction\"B is A\". This is the Reversal Curse. For instance, if a model is trained on\"Valentina Tereshkova was the first woman to travel to space\", it will not automatically be able to answer the question,\"Who was the first woman to travel to space?\". Moreover, the likelihood of the correct answer (\"Valentina Tershkova\") will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if\"A is B\"occurs,\"B is A\"is more likely to occur. It is worth noting, however, that if\"A is B\"appears in-context, models can deduce the reverse relationship.",
    "url": "https://www.matsprogram.org/research/recxapWTc3zIf1FCd",
    "published_date": "2023-09-21T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.567736+00:00"
  },
  {
    "title": "ARC Evals is spinning out from ARC19 September 2023ARC Evals plans to spin out from the Alignment Research Center (ARC) in the coming months, and become its own standalone organization.Read more",
    "authors": [],
    "organization": "METR",
    "abstract": "ARC Evals plans to spin out from the Alignment Research Center (ARC) in the coming months, and become its own standalone organization.",
    "url": "https://metr.org/blog/2023-09-19-spin-out-announcement/",
    "published_date": "2023-09-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://metr.org/blog/",
    "fetched_at": "2026-02-17T19:46:05.140877+00:00"
  },
  {
    "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
    "authors": [],
    "organization": "MATS",
    "abstract": "Sparse Autoencoders Find Highly Interpretable Features in Language Models One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods.",
    "url": "https://www.matsprogram.org/research/sparse-autoencoders-find-highly-interpretable-features-in-language-models",
    "published_date": "2023-09-15T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.563476+00:00"
  },
  {
    "title": "Taken out of context: On measuring situational awareness in LLMs",
    "authors": [],
    "organization": "MATS",
    "abstract": "Taken out of context: On measuring situational awareness in LLMs We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations.",
    "url": "https://www.matsprogram.org/research/taken-out-of-context-on-measuring-situational-awareness-in-llms",
    "published_date": "2023-09-01T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.577422+00:00"
  },
  {
    "title": "Steering Language Models With Activation Engineering",
    "authors": [],
    "organization": "MATS",
    "abstract": "Steering Language Models With Activation Engineering Prompt engineering and finetuning aim to maximize language model performance on a given metric (like toxicity reduction). However, these methods do not fully elicit a model's capabilities. To reduce this gap, we introduce activation engineering: the inference-time modification of activations in order to control (or steer) model outputs. Specifically, we introduce the Activation Addition (ActAdd) technique, which contrasts the intermediate activations on prompt pairs (such as\"Love\"versus\"Hate\") to compute a steering vector (Subramani et al. 2022). By tactically adding in e.g. the\"Love\"-\"Hate\"steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift and detoxification using models including LLaMA-3 and OPT. ActAdd yields inference-time control over high-level output properties (like topic and sentiment) while preserving performance on off-target tasks.",
    "url": "https://www.matsprogram.org/research/steering-language-models-with-activation-engineering",
    "published_date": "2023-08-20T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.570727+00:00"
  },
  {
    "title": "How Rogue AIs may Arise",
    "authors": [],
    "organization": "Yoshua Bengio",
    "abstract": "Scenarios giving rise to potentially rogue AIs, from intential genocidal behavior of some humans to unintentional catastrophe arising because of AI misalignment.",
    "url": "https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/",
    "published_date": "2023-05-22T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://yoshuabengio.org/category/ai-safety/",
    "fetched_at": "2026-02-17T19:46:14.765948+00:00"
  },
  {
    "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
    "authors": [],
    "organization": "MATS",
    "abstract": "Finding Neurons in a Haystack: Case Studies with Sparse Probing Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs.",
    "url": "https://www.matsprogram.org/research/finding-neurons-in-a-haystack-case-studies-with-sparse-probing",
    "published_date": "2023-05-02T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.570097+00:00"
  },
  {
    "title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
    "authors": [],
    "organization": "MATS",
    "abstract": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically.",
    "url": "https://www.matsprogram.org/research/do-the-rewards-justify-the-means-measuring-trade-offs-between-rewards-and-ethical-behavior-in-the-machiavelli-benchmark",
    "published_date": "2023-04-06T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.572158+00:00"
  },
  {
    "title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
    "authors": [],
    "organization": "MATS",
    "abstract": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations.",
    "url": "https://www.matsprogram.org/research/reclAAaZK3Xrt1qBX",
    "published_date": "2023-02-06T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.matsprogram.org/research",
    "fetched_at": "2026-02-17T19:46:13.573666+00:00"
  }
]