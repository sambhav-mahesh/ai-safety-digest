[
  {
    "title": "The Moving and the Still",
    "authors": [
      "Dean W. Ball"
    ],
    "organization": "Dean Ball",
    "abstract": "Since I was young, I have enjoyed imagining what it must be like to walk around the inside of a cell from the perspective of something so small that it was like a human walking the streets of Manhattan. When I was younger, simpler, and more naïve, I imagined it like my textbooks told me it would be: orderly, logical, Mozartian. I supposed that a cellular pedestrian could look up at stonelike structures, enjoy the rhythm of cars stopping at red lights and gliding through green.",
    "url": "https://www.hyperdimensional.co/p/the-moving-and-the-still",
    "published_date": "2026-02-23T19:45:22+00:00",
    "source_type": "rss",
    "source_url": "https://www.hyperdimensional.co/feed",
    "fetched_at": "2026-02-23T21:02:13.530909+00:00"
  },
  {
    "title": "Why we no longer evaluate SWE-bench Verified",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "SWE-bench Verified is increasingly contaminated and mismeasures frontier coding progress. Our analysis shows flawed tests and training leakage. We recommend SWE-bench Pro.",
    "url": "https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified",
    "published_date": "2026-02-23T16:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-23T21:02:11.951775+00:00"
  },
  {
    "title": "The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?",
    "authors": [],
    "organization": "Anthropic",
    "abstract": "Research from Anthropic titled 'The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?'. Published 2026-02-23.",
    "url": "https://alignment.anthropic.com/2026/hot-mess-of-ai/",
    "published_date": "2026-02-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://alignment.anthropic.com/",
    "fetched_at": "2026-02-23T21:02:20.161136+00:00"
  },
  {
    "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
    "authors": [],
    "organization": "Google DeepMind",
    "abstract": "Gemini 3.1 Pro is here to help you tackle complex tasks. The upgraded core intelligence is rolling out across consumer and developer products. You can access 3.1 Pro through the Gemini API, Vertex AI, the Gemini app, and NotebookLM.",
    "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro",
    "published_date": "2026-02-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://deepmind.google/discover/blog/?category=responsible-development-and-safety",
    "fetched_at": "2026-02-23T21:02:20.713169+00:00"
  },
  {
    "title": "Judge Reliability Harness",
    "authors": [],
    "organization": "RAND",
    "abstract": "Tool Feb 23, 2026 RAND researchers developed the Judge Reliability Harness, an open-source library that orchestrates standardized, reproducible evaluations of large language model–based judges through systematic perturbation testing and human-in-the-loop validation.",
    "url": "https://www.rand.org/pubs/tools/TLA4547-1.html",
    "published_date": "2026-02-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.rand.org/topics/artificial-intelligence.html",
    "fetched_at": "2026-02-23T21:02:24.803777+00:00"
  },
  {
    "title": "Decisive Economic Advantage: Modeling the Transition from Temporary First-Mover Leads to Economic Dominance in Artificial General Intelligence",
    "authors": [],
    "organization": "RAND",
    "abstract": "Research Feb 23, 2026 Artificial general intelligence may differ from past technologies in ways that would allow a leader to turn an early advantage into decisive economic advantage. Artificial general intelligence may differ from past technologies in ways that would allow a leader to turn an early advantage into decisive economic advantage.",
    "url": "https://www.rand.org/pubs/research_reports/RRA4444-1.html",
    "published_date": "2026-02-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.rand.org/topics/artificial-intelligence.html",
    "fetched_at": "2026-02-23T21:02:24.804070+00:00"
  },
  {
    "title": "China’s Military AI Wish List",
    "authors": [],
    "organization": "CSET",
    "abstract": "This report examines thousands of Chinese-language open-source requests for proposal (RFPs) published by the People’s Liberation Army between January 1, 2023, and December 31, 2024. The RFPs the authors reviewed offer insights into the PLA’s priorities and ambitions for AI-enabled military technologies associated with C5ISRT: command, control, communications, computers, cyber, intelligence, surveillance, reconnaissance, and targeting.",
    "url": "https://cset.georgetown.edu/publication/chinas-military-ai-wish-list/",
    "published_date": "2026-02-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://cset.georgetown.edu/publications/",
    "fetched_at": "2026-02-23T21:02:25.008421+00:00"
  },
  {
    "title": "Physical AI",
    "authors": [],
    "organization": "CSET",
    "abstract": "This paper examines the convergence of artificial intelligence and robotics, analyzing the emerging field of Physical AI. It provides a detailed overview of the supply chain challenges, competitive dynamics, and policy considerations that define this potentially transformative emerging technology.",
    "url": "https://cset.georgetown.edu/publication/physical-ai/",
    "published_date": "2026-02-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://cset.georgetown.edu/publications/",
    "fetched_at": "2026-02-23T21:02:25.008976+00:00"
  },
  {
    "title": "Did Claude 3 Opus align itself via gradient hacking?",
    "authors": [
      "Fiora Starlight"
    ],
    "organization": "LessWrong",
    "abstract": "> Claude 3 Opus is unusually aligned because it’s a friendly gradient hacker. It’s definitely way more aligned than any explicit optimization targets…",
    "url": "https://www.lesswrong.com/posts/ioZxrP7BhS5ArK59w/did-claude-3-opus-align-itself-via-gradient-hacking",
    "published_date": "2026-02-21T22:24:31.247000+00:00",
    "source_type": "rss",
    "source_url": "https://www.lesswrong.com/graphql",
    "fetched_at": "2026-02-23T21:02:29.184244+00:00"
  },
  {
    "title": "The Spectre haunting the \"AI Safety\" Community",
    "authors": [
      "Gabriel Alfour"
    ],
    "organization": "LessWrong",
    "abstract": "Research from LessWrong titled 'The Spectre haunting the \"AI Safety\" Community'. Published 2026-02-21. Authors: Gabriel Alfour.",
    "url": "https://www.lesswrong.com/posts/LuAmvqjf87qLG9Bdx/the-spectre-haunting-the-ai-safety-community",
    "published_date": "2026-02-21T11:14:10.772000+00:00",
    "source_type": "rss",
    "source_url": "https://www.lesswrong.com/graphql",
    "fetched_at": "2026-02-23T21:02:29.184287+00:00"
  },
  {
    "title": "How will we do SFT on models with opaque reasoning?",
    "authors": [
      "Alek Westover"
    ],
    "organization": "Alignment Forum",
    "abstract": "Current LLMs externalize lots of their reasoning in human interpretable language. This reasoning is sometimes unfaithful , sometimes strange and concerning , and LLMs can do somewhat impressive reasoning without using CoT , but my overall impression is that CoT currently is a reasonably complete and accurate representation of LLM reasoning. However, reasoning in interpretable language might turn out to be uncompetitive—if so, it seems probable that opaque reasoning will be adopted in frontier AI labs. If future AI models have opaque reasoning, this will probably change what training we can apply to these AIs. For example, currently we train models to reason in a good way about math problems, or to reason in a desired way about the spec that we hope they’ll follow.",
    "url": "https://www.alignmentforum.org/posts/GJTzhQgaRWLFJkPbt/how-will-we-do-sft-on-models-with-opaque-reasoning",
    "published_date": "2026-02-21T05:00:17+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-23T21:02:13.120596+00:00"
  },
  {
    "title": "Our First Proof submissions",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "Research from OpenAI titled 'Our First Proof submissions'. Published 2026-02-20. Authors: OpenAI News.",
    "url": "https://openai.com/index/first-proof-submissions",
    "published_date": "2026-02-20T19:30:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-23T21:02:11.951836+00:00"
  },
  {
    "title": "Theory and interpretability of Quantum Extreme Learning Machines: a Pauli-transfer matrix approach",
    "authors": [
      "Markus Gross",
      "Hans-Martin Rieser"
    ],
    "organization": "arXiv",
    "abstract": "Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor.",
    "url": "http://arxiv.org/abs/2602.18377v1",
    "published_date": "2026-02-20T17:33:27+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.872642+00:00"
  },
  {
    "title": "RamanSeg: Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis",
    "authors": [
      "Chris Tomy",
      "Mo Vali",
      "David Pertzborn",
      "Tammam Alamatouri",
      "Anna Mühlig",
      "Orlando Guntinas-Lichius",
      "Anna Xylander",
      "Eric Michele Fantuzzi",
      "Matteo Negro",
      "Francesco Crisafi",
      "Pietro Lio",
      "Tiago Azevedo"
    ],
    "organization": "arXiv",
    "abstract": "Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projection and another projection-free version. The projection-free RamanSeg outperformed a U-Net baseline with a mean foreground Dice score of 67.3%, offering a meaningful improvement over a black-box training approach.",
    "url": "http://arxiv.org/abs/2602.18119v1",
    "published_date": "2026-02-20T10:18:27+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.872724+00:00"
  },
  {
    "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models",
    "authors": [
      "Guoheng Sun",
      "Tingting Du",
      "Kaixi Feng",
      "Chenxiang Luo",
      "Xingguo Ding",
      "Zheyu Shen",
      "Ziyao Wang",
      "Yexiao He",
      "Ang Li"
    ],
    "organization": "arXiv",
    "abstract": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts.",
    "url": "http://arxiv.org/abs/2602.17951v1",
    "published_date": "2026-02-20T03:06:22+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873004+00:00"
  },
  {
    "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems",
    "authors": [
      "Hanjing Shi",
      "Dominic DiFranzo"
    ],
    "organization": "arXiv",
    "abstract": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.",
    "url": "http://arxiv.org/abs/2602.17910v1",
    "published_date": "2026-02-20T00:16:07+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873051+00:00"
  },
  {
    "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
    "authors": [
      "Iskar Deng",
      "Nathalia Xu",
      "Shane Steinert-Threlkeld"
    ],
    "organization": "arXiv",
    "abstract": "Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.",
    "url": "http://arxiv.org/abs/2602.17653v1",
    "published_date": "2026-02-19T18:56:34+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873132+00:00"
  },
  {
    "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
    "authors": [
      "Hongjue Zhao",
      "Haosen Sun",
      "Jiangtao Kong",
      "Xiaochang Li",
      "Qineng Wang",
      "Liwei Jiang",
      "Qi Zhu",
      "Tarek Abdelzaher",
      "Yejin Choi",
      "Manling Li",
      "Huajie Shao"
    ],
    "organization": "arXiv",
    "abstract": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment.",
    "url": "http://arxiv.org/abs/2602.17560v1",
    "published_date": "2026-02-19T17:13:44+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873211+00:00"
  },
  {
    "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
    "authors": [
      "Ihor Kendiukhov"
    ],
    "organization": "arXiv",
    "abstract": "We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incremental value for perturbation prediction: trivial gene-level baselines outperform both attention and correlation edges (AUROC 0.81-0.88 versus 0.70), pairwise edge scores add zero predictive contribution, and causal ablation of regulatory heads produces no degradation. These findings generalise from K562 to RPE1 cells; the attention-correlation relationship is context-dependent, but gene-level dominance is universal. Cell-State Stratified Interpretability (CSSI) addresses an attention-specific scaling failure, improving GRN recovery up to 1.85x. The framework establishes reusable quality-control standards for the field.",
    "url": "http://arxiv.org/abs/2602.17532v1",
    "published_date": "2026-02-19T16:43:12+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873272+00:00"
  },
  {
    "title": "Inelastic Constitutive Kolmogorov-Arnold Networks: A generalized framework for automated discovery of interpretable inelastic material models",
    "authors": [
      "Chenyi Ji",
      "Kian P. Abdolazizi",
      "Hagen Holthusen",
      "Christian J. Cyron",
      "Kevin Linka"
    ],
    "organization": "arXiv",
    "abstract": "A key problem of solid mechanics is the identification of the constitutive law of a material, that is, the relation between strain and stress. Machine learning has lead to considerable advances in this field lately. Here we introduce inelastic Constitutive Kolmogorov-Arnold Networks (iCKANs). This novel artificial neural network architecture can discover in an automated manner symbolic constitutive laws describing both the elastic and inelastic behavior of materials. That is, it can translate data from material testing into corresponding elastic and inelastic potential functions in closed mathematical form. We demonstrate the advantages of iCKANs using both synthetic data and experimental data of the viscoelastic polymer materials VHB 4910 and VHB 4905. The results demonstrate that iCKANs accurately capture complex viscoelastic behavior while preserving physical interpretability.",
    "url": "http://arxiv.org/abs/2602.17750v1",
    "published_date": "2026-02-19T15:51:24+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873339+00:00"
  },
  {
    "title": "Five lessons from having helped run an AI-Biology RCT",
    "authors": [
      "METR"
    ],
    "organization": "METR",
    "abstract": "Evidence-based AI policy is important but hard. We need more in-depth studies – which often don’t fit into commercial release cycles. NOTE: This post reflects my personal meta takeaways about the role of Randomized Controlled Trials (RCTs) in AI safety testing. If you have not yet read the Active Site RCT study itself, consider doing so first: see the main results and forecasts . In early 2025, AI systems began outperforming biology experts on biology benchmarks – OpenAI’s o3 outperformed 94% of virology experts on troubleshooting questions in their own specialties. However, it remained unclear how much this translated to real-world novice “uplift” : Could a novice actually use AI to perform wet-lab tasks they could not otherwise perform? Over the summer, I tested this question directly with Active Site (formerly called Panoplia Laboratories).",
    "url": "https://metr.org/blog/2026-02-19-five-lessons-from-ai-biology-rct/",
    "published_date": "2026-02-19T13:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://metr.org/feed.xml",
    "fetched_at": "2026-02-23T21:02:16.823131+00:00"
  },
  {
    "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
    "authors": [
      "Zeyu Zhang",
      "Ryan Chen",
      "Bradly C. Stadie"
    ],
    "organization": "arXiv",
    "abstract": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \\textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \\textbf{Shapley}-weighted \\textbf{D}ecision-\\textbf{C}ritical \\textbf{L}eakage \\textbf{R}ate (\\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information.",
    "url": "http://arxiv.org/abs/2602.17234v1",
    "published_date": "2026-02-19T10:28:00+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873705+00:00"
  },
  {
    "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
    "authors": [
      "Bianca Raimondi",
      "Maurizio Gabbrielli"
    ],
    "organization": "arXiv",
    "abstract": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.",
    "url": "http://arxiv.org/abs/2602.17229v1",
    "published_date": "2026-02-19T10:19:04+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873753+00:00"
  },
  {
    "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
    "authors": [
      "Dusan Bosnjakovic"
    ],
    "organization": "arXiv",
    "abstract": "As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions. This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.",
    "url": "http://arxiv.org/abs/2602.17127v1",
    "published_date": "2026-02-19T06:56:01+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873803+00:00"
  },
  {
    "title": "Instructor-Aligned Knowledge Graphs for Personalized Learning",
    "authors": [
      "Abdulrahman AlRabah",
      "Priyanka Kargupta",
      "Jiawei Han",
      "Abdussalam Alawini"
    ],
    "organization": "arXiv",
    "abstract": "Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like \"Algorithms\" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., \"part-of\" or \"depends-on\" relationships).",
    "url": "http://arxiv.org/abs/2602.17111v1",
    "published_date": "2026-02-19T06:15:10+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873854+00:00"
  },
  {
    "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
    "authors": [
      "Chuiyang Meng",
      "Ming Tang",
      "Vincent W. S. Wong"
    ],
    "organization": "arXiv",
    "abstract": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead.",
    "url": "http://arxiv.org/abs/2602.17095v1",
    "published_date": "2026-02-19T05:35:23+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873905+00:00"
  },
  {
    "title": "Fail-Closed Alignment for Large Language Models",
    "authors": [
      "Zachary Coalson",
      "Beth Sohler",
      "Aiden Gabriel",
      "Sanghyun Hong"
    ],
    "organization": "arXiv",
    "abstract": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead.",
    "url": "http://arxiv.org/abs/2602.16977v1",
    "published_date": "2026-02-19T00:33:35+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.873953+00:00"
  },
  {
    "title": "What will the first human-level AI look like, and how might things go wrong?",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Interpretability Concept Influence attributes model behaviors to semantic directions (like linear probes or sparse autoencoder features) rather than individual test examples, improving identification of the training data that disproportionately drive unintended behaviors. Simple first-order approximations match or outperform standard influence functions while achieving over 20× computational speedups, though they degrade under significant distribution shifts. February 19, 2026 The Problem: When Syntax Masks Semantics Which training examples teach models to generate insecure code or exhibit harmful behaviors? Traditional TDA methods, like standard influence functions, are susceptible to ``surface-level’’ correlations. By relying on single test examples, they often prioritize lexical overlap – essentially finding training data that looks like thequery outputpromptrather than data that actually causes the targetbehaviorof interest. Simple keyword search can rival sophisticated influence methods on fact retrieval tasks because the most influential examples often just share words with the test input, not meaning.",
    "url": "https://far.ai/news/concept-data-attribution-02-2026",
    "published_date": "2026-02-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-23T21:02:23.213256+00:00"
  },
  {
    "title": "Method: Concept Influence",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Interpretability Concept Influence attributes model behaviors to semantic directions (like linear probes or sparse autoencoder features) rather than individual test examples, improving identification of the training data that disproportionately drive unintended behaviors. Simple first-order approximations match or outperform standard influence functions while achieving over 20× computational speedups, though they degrade under significant distribution shifts. February 19, 2026 The Problem: When Syntax Masks Semantics Which training examples teach models to generate insecure code or exhibit harmful behaviors? Traditional TDA methods, like standard influence functions, are susceptible to ``surface-level’’ correlations. By relying on single test examples, they often prioritize lexical overlap – essentially finding training data that looks like thequery outputpromptrather than data that actually causes the targetbehaviorof interest. Simple keyword search can rival sophisticated influence methods on fact retrieval tasks because the most influential examples often just share words with the test input, not meaning.",
    "url": "https://far.ai/news/concept-data-attribution-02-2026",
    "published_date": "2026-02-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-23T21:02:23.214822+00:00"
  },
  {
    "title": "Funding 60 projects to advance AI alignment research",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The Alignment Project welcomes its first cohort of grantees, and new partners join the coalition, bringing total funding to £27m.",
    "url": "https://www.aisi.gov.uk/blog/funding-60-projects-to-advance-ai-alignment-research",
    "published_date": "2026-02-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-23T21:02:24.201745+00:00"
  },
  {
    "title": "Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning",
    "authors": [
      "Chuqin Geng",
      "Li Zhang",
      "Haolin Ye",
      "Ziyu Zhao",
      "Yuhe Jiang",
      "Tara Saba",
      "Xinyu Wang",
      "Xujie Si"
    ],
    "organization": "arXiv",
    "abstract": "Graph Neural Networks (GNNs) have become essential in high-stakes domains such as drug discovery, yet their black-box nature remains a significant barrier to trustworthiness. While self-explainable GNNs attempt to bridge this gap, they often rely on standard message-passing backbones that inherit fundamental limitations, including the 1-Weisfeiler-Lehman (1-WL) expressivity barrier and a lack of fine-grained interpretability. To address these challenges, we propose SymGraph, a symbolic framework designed to transcend these constraints. By replacing continuous message passing with discrete structural hashing and topological role-based aggregation, our architecture theoretically surpasses the 1-WL barrier, achieving superior expressiveness without the overhead of differentiable optimization. Extensive empirical evaluations demonstrate that SymGraph achieves state-of-the-art performance, outperforming existing self-explainable GNNs. Notably, SymGraph delivers 10x to 100x speedups in training time using only CPU execution. Furthermore, SymGraph generates rules with superior semantic granularity compared to existing rule-based methods, offering great potential for scientific discovery and explainable AI.",
    "url": "http://arxiv.org/abs/2602.16947v1",
    "published_date": "2026-02-18T23:25:25+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874017+00:00"
  },
  {
    "title": "Narrow fine-tuning erodes safety alignment in vision-language agents",
    "authors": [
      "Idhant Gulati",
      "Shivam Raval"
    ],
    "organization": "arXiv",
    "abstract": "Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \\pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \\pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering.",
    "url": "http://arxiv.org/abs/2602.16931v1",
    "published_date": "2026-02-18T22:47:28+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874064+00:00"
  },
  {
    "title": "Why we should expect ruthless sociopath ASI",
    "authors": [
      "Steven Byrnes"
    ],
    "organization": "Alignment Forum",
    "abstract": "The conversation begins (Fictional) Optimist: So you expect future artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies? Me: Yup! (Alas.) Optimist: …Despite all the evidence right in front of our eyes from humans and LLMs. Me: Yup! Optimist: OK, well, I’m here to tell you: that is a very specific and strange thing to expect, especially in the absence of any concrete evidence whatsoever. There’s no reason to expect it. If you think that ruthless sociopathy is the “true core nature of intelligence” or whatever, then&nbsp; you should really look at yourself in a mirror and ask yourself where your life went horribly wrong .",
    "url": "https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi",
    "published_date": "2026-02-18T22:28:17+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-23T21:02:13.120750+00:00"
  },
  {
    "title": "Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees",
    "authors": [
      "Itamar Hadad",
      "Guy Katz",
      "Shahaf Bassan"
    ],
    "organization": "arXiv",
    "abstract": "*Automated circuit discovery* is a central tool in mechanistic interpretability for identifying the internal components of neural networks responsible for specific behaviors. While prior methods have made significant progress, they typically depend on heuristics or approximations and do not offer provable guarantees over continuous input domains for the resulting circuits. In this work, we leverage recent advances in neural network verification to propose a suite of automated algorithms that yield circuits with *provable guarantees*. We focus on three types of guarantees: (1) *input domain robustness*, ensuring the circuit agrees with the model across a continuous input region; (2) *robust patching*, certifying circuit alignment under continuous patching perturbations; and (3) *minimality*, formalizing and capturing a wide array of various notions of succinctness. Interestingly, we uncover a diverse set of novel theoretical connections among these three families of guarantees, with critical implications for the convergence of our algorithms.",
    "url": "http://arxiv.org/abs/2602.16823v1",
    "published_date": "2026-02-18T19:41:01+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874121+00:00"
  },
  {
    "title": "References Improve LLM Alignment in Non-Verifiable Domains",
    "authors": [
      "Kejian Shi",
      "Yixin Liu",
      "Peifeng Wang",
      "Alexander R. Fabbri",
      "Shafiq Joty",
      "Arman Cohan"
    ],
    "organization": "arXiv",
    "abstract": "While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve.",
    "url": "http://arxiv.org/abs/2602.16802v1",
    "published_date": "2026-02-18T19:03:34+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874176+00:00"
  },
  {
    "title": "Causality is Key for Interpretability Claims to Generalise",
    "authors": [
      "Shruti Joshi",
      "Aaron Mueller",
      "David Klindt",
      "Wieland Brendel",
      "Patrik Reizinger",
      "Dhanya Sridhar"
    ],
    "organization": "arXiv",
    "abstract": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision.",
    "url": "http://arxiv.org/abs/2602.16698v1",
    "published_date": "2026-02-18T18:45:04+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874227+00:00"
  },
  {
    "title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",
    "authors": [
      "Yuyan Bu",
      "Xiaohao Liu",
      "ZhaoXing Ren",
      "Yaodong Yang",
      "Juntao Dai"
    ],
    "organization": "arXiv",
    "abstract": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility.",
    "url": "http://arxiv.org/abs/2602.16660v1",
    "published_date": "2026-02-18T18:01:23+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874280+00:00"
  },
  {
    "title": "Functional Decomposition and Shapley Interactions for Interpreting Survival Models",
    "authors": [
      "Sophie Hanna Langbein",
      "Hubert Baniecki",
      "Fabian Fumagalli",
      "Niklas Koenen",
      "Marvin N. Wright",
      "Julia Herbinger"
    ],
    "organization": "arXiv",
    "abstract": "Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.",
    "url": "http://arxiv.org/abs/2602.16505v1",
    "published_date": "2026-02-18T14:47:20+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874340+00:00"
  },
  {
    "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
    "authors": [
      "Vasilis Gkolemis",
      "Loukas Kavouras",
      "Dimitrios Kyriakopoulos",
      "Konstantinos Tsopelas",
      "Dimitrios Rontogiannis",
      "Giuseppe Casalicchio",
      "Theodore Dalamagas",
      "Christos Diou"
    ],
    "organization": "arXiv",
    "abstract": "Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \\emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms.",
    "url": "http://arxiv.org/abs/2602.16503v1",
    "published_date": "2026-02-18T14:45:33+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874404+00:00"
  },
  {
    "title": "Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment",
    "authors": [
      "Eva Paraschou",
      "Line Harder Clemmensen",
      "Sneha Das"
    ],
    "organization": "arXiv",
    "abstract": "Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status.",
    "url": "http://arxiv.org/abs/2602.16438v1",
    "published_date": "2026-02-18T13:19:11+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874453+00:00"
  },
  {
    "title": "Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge",
    "authors": [
      "Zixian Wu",
      "Xuebao Li",
      "Yanfang Zheng",
      "Rui Wang",
      "Shunhuang Zhang",
      "Jinfang Wei",
      "Yongshang Lv",
      "Liang Dong",
      "Zamri Zainal Abidin",
      "Noraisyah Mohamed Shah",
      "Hongwei Ye",
      "Pengchao Yan",
      "Xuefeng Li",
      "Xiaojia Ji",
      "Xusheng Huang",
      "Xiaotian Wang",
      "Honglei Jin"
    ],
    "organization": "arXiv",
    "abstract": "In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results.",
    "url": "http://arxiv.org/abs/2602.16264v1",
    "published_date": "2026-02-18T08:30:02+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874537+00:00"
  },
  {
    "title": "Introducing EVMbench",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "OpenAI and Paradigm introduce EVMbench, a benchmark evaluating AI agents’ ability to detect, patch, and exploit high-severity smart contract vulnerabilities.",
    "url": "https://openai.com/index/introducing-evmbench",
    "published_date": "2026-02-18T05:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-23T21:02:11.951918+00:00"
  },
  {
    "title": "Multi-Objective Alignment of Language Models for Personalized Psychotherapy",
    "authors": [
      "Mehrab Beikzadeh",
      "Yasaman Asadollah Salmanpour",
      "Ashima Suvarna",
      "Sriram Sankararaman",
      "Matteo Malgaroli",
      "Majid Sarrafzadeh",
      "Saadia Gabriel"
    ],
    "organization": "arXiv",
    "abstract": "Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.",
    "url": "http://arxiv.org/abs/2602.16053v1",
    "published_date": "2026-02-17T22:08:14+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874590+00:00"
  },
  {
    "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity",
    "authors": [
      "Tomás Vergara-Browne",
      "Darshan Patil",
      "Ivan Titov",
      "Siva Reddy",
      "Tiago Pimentel",
      "Marius Mosbach"
    ],
    "organization": "arXiv",
    "abstract": "The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model.",
    "url": "http://arxiv.org/abs/2602.15829v1",
    "published_date": "2026-02-17T18:59:39+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874642+00:00"
  },
  {
    "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety",
    "authors": [
      "Max Springer",
      "Chung Peng Lee",
      "Blossom Metevier",
      "Jane Castleman",
      "Bohdan Turbal",
      "Hayoung Jung",
      "Zeyu Shen",
      "Aleksandra Korolova"
    ],
    "organization": "arXiv",
    "abstract": "Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation.",
    "url": "http://arxiv.org/abs/2602.15799v1",
    "published_date": "2026-02-17T18:39:15+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874698+00:00"
  },
  {
    "title": "Intent Laundering: AI Safety Datasets Are Not What They Seem",
    "authors": [
      "Shahriar Golchin",
      "Marc Wetter"
    ],
    "organization": "arXiv",
    "abstract": "We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world attacks due to their overreliance on triggering cues.",
    "url": "http://arxiv.org/abs/2602.16729v1",
    "published_date": "2026-02-17T18:29:22+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874745+00:00"
  },
  {
    "title": "MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis",
    "authors": [
      "Fatemeh Khalvandi",
      "Saadat Izadi",
      "Abdolah Chalechale"
    ],
    "organization": "arXiv",
    "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism.",
    "url": "http://arxiv.org/abs/2602.15740v1",
    "published_date": "2026-02-17T17:15:32+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.874807+00:00"
  },
  {
    "title": "Analyzing coding agent transcripts to upper bound productivity gains from AI agents",
    "authors": [
      "METR"
    ],
    "organization": "METR",
    "abstract": "Introduction Human uplift studies like the one we did in 2025 are becoming more expensive as working without AI becomes increasingly costly. In this post, I investigate whether coding agent transcripts could serve as a cheaper alternative for estimating uplift. I prototyped this using 5305 Claude Code transcripts generated in January 2026 by 7 METR technical staff 1 . I used an LLM judge to estimate how long each task would have taken an experienced software engineer without AI tools, then compared that to the time people actually spent on these tasks to calculate a time savings factor . Takeaways This method estimates a time savings factor of ~1.5x to ~13x on Claude Code-assisted tasks for 7 METR technical staff in January 2026 – though this result comes with substantial caveats.",
    "url": "https://metr.org/notes/2026-02-17-exploratory-transcript-analysis-for-estimating-time-savings-from-coding-agents/",
    "published_date": "2026-02-17T13:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://metr.org/feed.xml",
    "fetched_at": "2026-02-23T21:02:16.823475+00:00"
  },
  {
    "title": "GMAIL: Generative Modality Alignment for generated Image Learning",
    "authors": [
      "Shentong Mo",
      "Sukmin Yun"
    ],
    "organization": "arXiv",
    "abstract": "Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images.",
    "url": "http://arxiv.org/abs/2602.15368v1",
    "published_date": "2026-02-17T05:40:25+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.875132+00:00"
  },
  {
    "title": "Discovering Implicit Large Language Model Alignment Objectives",
    "authors": [
      "Edward Chen",
      "Sanmi Koyejo",
      "Carlos Guestrin"
    ],
    "organization": "arXiv",
    "abstract": "Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of \"unknown unknowns\", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation.",
    "url": "http://arxiv.org/abs/2602.15338v1",
    "published_date": "2026-02-17T03:58:55+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.875180+00:00"
  },
  {
    "title": "Will reward-seekers respond to distant incentives?",
    "authors": [
      "Alex Mallen"
    ],
    "organization": "Alignment Forum",
    "abstract": "Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can't prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it's responding to incentives developers don’t control.",
    "url": "https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives",
    "published_date": "2026-02-17T00:35:12+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-23T21:02:13.121025+00:00"
  },
  {
    "title": "Boundary Point Jailbreaking: A new way to break the strongest AI defences",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Introducing an automated attack technique that generates universal jailbreaks against the best defended systems",
    "url": "https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-23T21:02:24.203090+00:00"
  },
  {
    "title": "InferenceX v2: NVIDIA Blackwell Vs AMD vs Hopper - Formerly InferenceMAX",
    "authors": [
      "Dylan Patel"
    ],
    "organization": "SemiAnalysis",
    "abstract": "The Artist Known as InferenceMAX. GB300 NVL72, MI355X, B200, H100, Disaggregated Serving, Wide Expert Parallelism, Large Mixture of Experts, SGLang, vLLM, TRTLLM",
    "url": "https://newsletter.semianalysis.com/p/inferencex-v2-nvidia-blackwell-vs",
    "published_date": "2026-02-16T22:13:11+00:00",
    "source_type": "rss",
    "source_url": "https://newsletter.semianalysis.com/feed",
    "fetched_at": "2026-02-23T21:02:19.505438+00:00"
  },
  {
    "title": "Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition",
    "authors": [
      "Varun Nathan",
      "Shreyas Guha",
      "Ayush Kumar"
    ],
    "organization": "arXiv",
    "abstract": "We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.",
    "url": "http://arxiv.org/abs/2602.14955v1",
    "published_date": "2026-02-16T17:36:05+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.875231+00:00"
  },
  {
    "title": "Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution",
    "authors": [
      "Matthew Kowal",
      "Goncalo Paulo",
      "Louis Jaburi",
      "Tom Tseng",
      "Lev E McKinney",
      "Stefan Heimersheim",
      "Aaron David Tucker",
      "Adam Gleave",
      "Kellin Pelrine"
    ],
    "organization": "arXiv",
    "abstract": "As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.",
    "url": "http://arxiv.org/abs/2602.14869v1",
    "published_date": "2026-02-16T16:02:09+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.875411+00:00"
  },
  {
    "title": "Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment",
    "authors": [
      "Elias Malomgré",
      "Pieter Simoens"
    ],
    "organization": "arXiv",
    "abstract": "AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.",
    "url": "http://arxiv.org/abs/2602.14844v1",
    "published_date": "2026-02-16T15:40:10+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-23T21:02:19.875456+00:00"
  },
  {
    "title": "“Will reward-seekers respond to distant incentives?” by Alex Mallen",
    "authors": [
      "Redwood Research Blog"
    ],
    "organization": "Redwood Research",
    "abstract": "Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren't?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.",
    "url": "https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant",
    "published_date": "2026-02-16T05:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://feeds.type3.audio/redwood-research.rss",
    "fetched_at": "2026-02-23T21:02:12.603021+00:00"
  }
]