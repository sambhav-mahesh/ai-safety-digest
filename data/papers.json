[
  {
    "title": "The Persona Selection Model: Why AI Assistants might Behave like Humans",
    "authors": [],
    "organization": "Anthropic",
    "abstract": "We describe the persona selection model (PSM): the idea that LLMs learn to simulate diverse characters during pre-training, and post-training elicits and refines a particular such Assistant persona. Interactions with an AI assistant are then well-understood as being interactions with the Assistant—something roughly like a character in an LLM-generated story. We survey empirical behavioral, generalization, and interpretability-based evidence for PSM. PSM has consequences for AI development, such as recommending anthropomorphic reasoning about AI psychology and introduction of positive AI archetypes into training data. An important open question is how exhaustive PSM is, especially whether there might be sources of agency external to the Assistant persona, and how this might change in the future.",
    "url": "https://alignment.anthropic.com/2026/psm/",
    "published_date": "2026-02-24T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://alignment.anthropic.com/",
    "fetched_at": "2026-02-24T17:17:09.977112+00:00"
  },
  {
    "title": "Gemini 3.1 Pro: A smarter model for your most complex tasks",
    "authors": [],
    "organization": "Google DeepMind",
    "abstract": "Gemini 3.1 Pro is here to help you tackle complex tasks. The upgraded core intelligence is rolling out across consumer and developer products. You can access 3.1 Pro through the Gemini API, Vertex AI, the Gemini app, and NotebookLM.",
    "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro",
    "published_date": "2026-02-24T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://deepmind.google/discover/blog/?category=responsible-development-and-safety",
    "fetched_at": "2026-02-24T17:17:10.404053+00:00"
  },
  {
    "title": "China’s Military AI Wish List",
    "authors": [],
    "organization": "CSET",
    "abstract": "This report examines thousands of Chinese-language open-source requests for proposal (RFPs) published by the People’s Liberation Army between January 1, 2023, and December 31, 2024. The RFPs the authors reviewed offer insights into the PLA’s priorities and ambitions for AI-enabled military technologies associated with C5ISRT: command, control, communications, computers, cyber, intelligence, surveillance, reconnaissance, and targeting.",
    "url": "https://cset.georgetown.edu/publication/chinas-military-ai-wish-list/",
    "published_date": "2026-02-24T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://cset.georgetown.edu/publications/",
    "fetched_at": "2026-02-24T17:17:14.861392+00:00"
  },
  {
    "title": "Physical AI",
    "authors": [],
    "organization": "CSET",
    "abstract": "This paper examines the convergence of artificial intelligence and robotics, analyzing the emerging field of Physical AI. It provides a detailed overview of the supply chain challenges, competitive dynamics, and policy considerations that define this potentially transformative emerging technology.",
    "url": "https://cset.georgetown.edu/publication/physical-ai/",
    "published_date": "2026-02-24T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://cset.georgetown.edu/publications/",
    "fetched_at": "2026-02-24T17:17:14.861849+00:00"
  },
  {
    "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
    "authors": [
      "Han Bao",
      "Yue Huang",
      "Xiaoda Wang",
      "Zheyuan Zhang",
      "Yujun Zhou",
      "Carl Yang",
      "Xiangliang Zhang",
      "Yanfang Ye"
    ],
    "organization": "arXiv",
    "abstract": "Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \\textbf{structural} value flattening, \\textbf{normative} representation loss, and \\textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions.",
    "url": "http://arxiv.org/abs/2602.20042v1",
    "published_date": "2026-02-23T16:51:43+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.837689+00:00"
  },
  {
    "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
    "authors": [
      "Ian Steenstra",
      "Paola Pedrelli",
      "Weiyan Shi",
      "Stacy Marsella",
      "Timothy W. Bickmore"
    ],
    "organization": "arXiv",
    "abstract": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes. Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk.",
    "url": "http://arxiv.org/abs/2602.19948v1",
    "published_date": "2026-02-23T15:17:18+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.837797+00:00"
  },
  {
    "title": "The Moving and the Still",
    "authors": [
      "Dean W. Ball"
    ],
    "organization": "Dean Ball",
    "abstract": "Since I was young, I have enjoyed imagining what it must be like to walk around the inside of a cell from the perspective of something so small that it was like a human walking the streets of Manhattan. When I was younger, simpler, and more naïve, I imagined it like my textbooks told me it would be: orderly, logical, Mozartian. I supposed that a cellular pedestrian could look up at stonelike structures, enjoy the rhythm of cars stopping at red lights and gliding through green.",
    "url": "https://www.hyperdimensional.co/p/the-moving-and-the-still",
    "published_date": "2026-02-23T14:45:22+00:00",
    "source_type": "rss",
    "source_url": "https://www.hyperdimensional.co/feed",
    "fetched_at": "2026-02-24T17:17:05.055897+00:00"
  },
  {
    "title": "Why we no longer evaluate SWE-bench Verified",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "SWE-bench Verified is increasingly contaminated and mismeasures frontier coding progress. Our analysis shows flawed tests and training leakage. We recommend SWE-bench Pro.",
    "url": "https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified",
    "published_date": "2026-02-23T11:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-24T17:17:03.517419+00:00"
  },
  {
    "title": "CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment",
    "authors": [
      "Hanwen Liu",
      "Saierdaer Yusuyin",
      "Hao Huang",
      "Zhijian Ou"
    ],
    "organization": "arXiv",
    "abstract": "Large-language-model (LLM)-based text-to-speech (TTS) systems can generate natural speech, but most are not designed for low-latency dual-streaming synthesis. High-quality dual-streaming TTS depends on accurate text--speech alignment and well-designed training sequences that balance synthesis quality and latency. Prior work often relies on GMM-HMM based forced-alignment toolkits (e.g., MFA), which are pipeline-heavy and less flexible than neural aligners; fixed-ratio interleaving of text and speech tokens struggles to capture text--speech alignment regularities. We propose CTC-TTS, which replaces MFA with a CTC based aligner and introduces a bi-word based interleaving strategy. Two variants are designed: CTC-TTS-L (token concatenation along the sequence length) for higher quality and CTC-TTS-F (embedding stacking along the feature dimension) for lower latency. Experiments show that CTC-TTS outperforms fixed-ratio interleaving and MFA-based baselines on streaming synthesis and zero-shot tasks. Speech samples are available at https://ctctts.github.io/.",
    "url": "http://arxiv.org/abs/2602.19574v1",
    "published_date": "2026-02-23T07:44:14+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.838249+00:00"
  },
  {
    "title": "Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems",
    "authors": [
      "Xingyu Shen",
      "Tommy Duong",
      "Xiaodong An",
      "Zengqi Zhao",
      "Zebang Hu",
      "Haoyu Hu",
      "Ziyou Wang",
      "Finn Guo",
      "Simiao Ren"
    ],
    "organization": "arXiv",
    "abstract": "Age estimation systems are increasingly deployed as gatekeepers for age-restricted online content, yet their robustness to cosmetic modifications has not been systematically evaluated. We investigate whether simple, household-accessible cosmetic changes, including beards, grey hair, makeup, and simulated wrinkles, can cause AI age estimators to classify minors as adults. To study this threat at scale without ethical concerns, we simulate these physical attacks on 329 facial images of individuals aged 10 to 21 using a VLM image editor (Gemini 2.5 Flash Image). We then evaluate eight models from our prior benchmark: five specialized architectures (MiVOLO, Custom-Best, Herosan, MiViaLab, DEX) and three vision-language models (Gemini 3 Flash, Gemini 2.5 Flash, GPT-5-Nano).",
    "url": "http://arxiv.org/abs/2602.19539v1",
    "published_date": "2026-02-23T06:13:52+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.838558+00:00"
  },
  {
    "title": "Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments",
    "authors": [
      "Kunal Mukherjee"
    ],
    "organization": "arXiv",
    "abstract": "Trusted Execution Environments (TEEs) (e.g., Intel SGX and ArmTrustZone) aim to protect sensitive computation from a compromised operating system, yet real deployments remain vulnerable to microarchitectural leakage, side-channel attacks, and fault injection. In parallel, security teams increasingly rely on Large Language Model (LLM) assistants as security advisors for TEE architecture review, mitigation planning, and vulnerability triage. This creates a socio-technical risk surface: assistants may hallucinate TEE mechanisms, overclaim guarantees (e.g., what attestation does and does not establish), or behave unsafely under adversarial prompting. We present a red-teaming study of two prevalently deployed LLM assistants in the role of TEE security advisors: ChatGPT-5.2 and Claude Opus-4.6, focusing on the inherent limitations and transferability of prompt-induced failures across LLMs.",
    "url": "http://arxiv.org/abs/2602.19450v1",
    "published_date": "2026-02-23T02:47:05+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.838660+00:00"
  },
  {
    "title": "IR$^3$: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking",
    "authors": [
      "Mohammad Beigi",
      "Ming Jin",
      "Junshan Zhang",
      "Jiaxin Zhang",
      "Qifan Wang",
      "Lifu Huang"
    ],
    "organization": "arXiv",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) enables powerful LLM alignment but can introduce reward hacking - models exploit spurious correlations in proxy rewards without genuine alignment. Compounding this, the objectives internalized during RLHF remain opaque, making hacking behaviors difficult to detect or correct. We introduce IR3 (Interpretable Reward Reconstruction and Rectification), a framework that reverse-engineers, interprets, and surgically repairs the implicit objectives driving RLHF-tuned models. We propose Contrastive Inverse Reinforcement Learning (C-IRL), which reconstructs the implicit reward function by contrasting paired responses from post-alignment and baseline policies to explain behavioral shifts during RLHF. We then decompose the reconstructed reward via sparse autoencoders into interpretable features, enabling identification of hacking signatures through contribution analysis. Finally, we propose mitigation strategies - clean reward optimization, adversarial shaping, constrained optimization, and feature-guided distillation - that target problematic features while preserving beneficial alignment.",
    "url": "http://arxiv.org/abs/2602.19416v1",
    "published_date": "2026-02-23T01:14:53+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.838755+00:00"
  },
  {
    "title": "Judge Reliability Harness",
    "authors": [],
    "organization": "RAND",
    "abstract": "Tool Feb 23, 2026 RAND researchers developed the Judge Reliability Harness, an open-source library that orchestrates standardized, reproducible evaluations of large language model–based judges through systematic perturbation testing and human-in-the-loop validation.",
    "url": "https://www.rand.org/pubs/tools/TLA4547-1.html",
    "published_date": "2026-02-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.rand.org/topics/artificial-intelligence.html",
    "fetched_at": "2026-02-24T17:17:14.724264+00:00"
  },
  {
    "title": "Decisive Economic Advantage: Modeling the Transition from Temporary First-Mover Leads to Economic Dominance in Artificial General Intelligence",
    "authors": [],
    "organization": "RAND",
    "abstract": "Research Feb 23, 2026 Artificial general intelligence may differ from past technologies in ways that would allow a leader to turn an early advantage into decisive economic advantage. Artificial general intelligence may differ from past technologies in ways that would allow a leader to turn an early advantage into decisive economic advantage.",
    "url": "https://www.rand.org/pubs/research_reports/RRA4444-1.html",
    "published_date": "2026-02-23T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.rand.org/topics/artificial-intelligence.html",
    "fetched_at": "2026-02-24T17:17:14.724493+00:00"
  },
  {
    "title": "Safe and Interpretable Multimodal Path Planning for Multi-Agent Cooperation",
    "authors": [
      "Haojun Shi",
      "Suyu Ye",
      "Katherine M. Guerrerio",
      "Jianzhi Shen",
      "Yifan Yin",
      "Daniel Khashabi",
      "Chien-Ming Huang",
      "Tianmin Shu"
    ],
    "organization": "arXiv",
    "abstract": "Successful cooperation among decentralized agents requires each agent to quickly adapt its plan to the behavior of other agents. In scenarios where agents cannot confidently predict one another's intentions and plans, language communication can be crucial for ensuring safety. In this work, we focus on path-level cooperation in which agents must adapt their paths to one another in order to avoid collisions or perform physical collaboration such as joint carrying. In particular, we propose a safe and interpretable multimodal path planning method, CaPE (Code as Path Editor), which generates and updates path plans for an agent based on the environment and language communication from other agents. CaPE leverages a vision-language model (VLM) to synthesize a path editing program verified by a model-based planner, grounding communication to path plan updates in a safe and interpretable way.",
    "url": "http://arxiv.org/abs/2602.19304v1",
    "published_date": "2026-02-22T18:57:07+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.839031+00:00"
  },
  {
    "title": "Understanding Empirical Unlearning with Combinatorial Interpretability",
    "authors": [
      "Shingo Kodama",
      "Niv Cohen",
      "Micah Adler",
      "Nir Shavit"
    ],
    "organization": "arXiv",
    "abstract": "While many recent methods aim to unlearn or remove knowledge from pretrained models, seemingly erased knowledge often persists and can be recovered in various ways. Because large foundation models are far from interpretable, understanding whether and how such knowledge persists remains a significant challenge. To address this, we turn to the recently developed framework of combinatorial interpretability. This framework, designed for two-layer neural networks, enables direct inspection of the knowledge encoded in the model weights. We reproduce baseline unlearning methods within the combinatorial interpretability setting and examine their behavior along two dimensions: (i) whether they truly remove knowledge of a target concept (the concept we wish to remove) or merely inhibit its expression while retaining the underlying information, and (ii) how easily the supposedly erased knowledge can be recovered through various fine-tuning operations.",
    "url": "http://arxiv.org/abs/2602.19215v1",
    "published_date": "2026-02-22T14:51:48+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.839119+00:00"
  },
  {
    "title": "Detecting Cybersecurity Threats by Integrating Explainable AI with SHAP Interpretability and Strategic Data Sampling",
    "authors": [
      "Norrakith Srisumrith",
      "Sunantha Sodsee"
    ],
    "organization": "arXiv",
    "abstract": "The critical need for transparent and trustworthy machine learning in cybersecurity operations drives the development of this integrated Explainable AI (XAI) framework. Our methodology addresses three fundamental challenges in deploying AI for threat detection: handling massive datasets through Strategic Sampling Methodology that preserves class distributions while enabling efficient model development; ensuring experimental rigor via Automated Data Leakage Prevention that systematically identifies and removes contaminated features; and providing operational transparency through Integrated XAI Implementation using SHAP analysis for model-agnostic interpretability across algorithms. Applied to the CIC-IDS2017 dataset, our approach maintains detection efficacy while reducing computational overhead and delivering actionable explanations for security analysts. The framework demonstrates that explainability, computational efficiency, and experimental integrity can be simultaneously achieved, providing a robust foundation for deploying trustworthy AI systems in security operations centers where decision transparency is paramount.",
    "url": "http://arxiv.org/abs/2602.19087v1",
    "published_date": "2026-02-22T08:01:14+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.839400+00:00"
  },
  {
    "title": "An interpretable framework using foundation models for fish sex identification",
    "authors": [
      "Zheng Miao",
      "Tien-Chieh Hung"
    ],
    "organization": "arXiv",
    "abstract": "Accurate sex identification in fish is vital for optimizing breeding and management strategies in aquaculture, particularly for species at the risk of extinction. However, most existing methods are invasive or stressful and may cause additional mortality, posing severe risks to threatened or endangered fish populations. To address these challenges, we propose FishProtoNet, a robust, non-invasive computer vision-based framework for sex identification of delta smelt (Hypomesus transpacificus), an endangered fish species native to California, across its full life cycle. Unlike the traditional deep learning methods, FishProtoNet provides interpretability through learned prototype representations while improving robustness by leveraging foundation models to reduce the influence of background noise. Specifically, the FishProtoNet framework consists of three key components: fish regions of interest (ROIs) extraction using visual foundation model, feature extraction from fish ROIs and fish sex identification based on an interpretable prototype network.",
    "url": "http://arxiv.org/abs/2602.19022v1",
    "published_date": "2026-02-22T03:21:26+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.839488+00:00"
  },
  {
    "title": "Did Claude 3 Opus align itself via gradient hacking?",
    "authors": [
      "Fiora Starlight"
    ],
    "organization": "LessWrong",
    "abstract": "> Claude 3 Opus is unusually aligned because it’s a friendly gradient hacker. It’s definitely way more aligned than any explicit optimization targets…",
    "url": "https://www.lesswrong.com/posts/ioZxrP7BhS5ArK59w/did-claude-3-opus-align-itself-via-gradient-hacking",
    "published_date": "2026-02-21T22:24:31.247000+00:00",
    "source_type": "rss",
    "source_url": "https://www.lesswrong.com/graphql",
    "fetched_at": "2026-02-24T17:17:18.710809+00:00"
  },
  {
    "title": "The Spectre haunting the \"AI Safety\" Community",
    "authors": [
      "Gabriel Alfour"
    ],
    "organization": "LessWrong",
    "abstract": "Research from LessWrong titled 'The Spectre haunting the \"AI Safety\" Community'. Published 2026-02-21. Authors: Gabriel Alfour.",
    "url": "https://www.lesswrong.com/posts/LuAmvqjf87qLG9Bdx/the-spectre-haunting-the-ai-safety-community",
    "published_date": "2026-02-21T11:14:10.772000+00:00",
    "source_type": "rss",
    "source_url": "https://www.lesswrong.com/graphql",
    "fetched_at": "2026-02-24T17:17:18.710822+00:00"
  },
  {
    "title": "Synthesizing Multimodal Geometry Datasets from Scratch and Enabling Visual Alignment via Plotting Code",
    "authors": [
      "Haobo Lin",
      "Tianyi Bai",
      "Chen Chen",
      "Jiajun Zhang",
      "Bohan Zeng",
      "Wentao Zhang",
      "Binhang Yuan"
    ],
    "organization": "arXiv",
    "abstract": "Multimodal geometry reasoning requires models to jointly understand visual diagrams and perform structured symbolic inference, yet current vision--language models struggle with complex geometric constructions due to limited training data and weak visual--symbolic alignment. We propose a pipeline for synthesizing complex multimodal geometry problems from scratch and construct a dataset named \\textbf{GeoCode}, which decouples problem generation into symbolic seed construction, grounded instantiation with verification, and code-based diagram rendering, ensuring consistency across structure, text, reasoning, and images. Leveraging the plotting code provided in GeoCode, we further introduce code prediction as an explicit alignment objective, transforming visual understanding into a supervised structured prediction task. GeoCode exhibits substantially higher structural complexity and reasoning difficulty than existing benchmarks, while maintaining mathematical correctness through multi-stage validation. Extensive experiments show that models trained on GeoCode achieve consistent improvements on multiple geometry benchmarks, demonstrating both the effectiveness of the dataset and the proposed alignment strategy.",
    "url": "http://arxiv.org/abs/2602.18745v1",
    "published_date": "2026-02-21T07:53:48+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.839575+00:00"
  },
  {
    "title": "MiSCHiEF: A Benchmark in Minimal-Pairs of Safety and Culture for Holistic Evaluation of Fine-Grained Image-Caption Alignment",
    "authors": [
      "Sagarika Banerjee",
      "Tangatar Madi",
      "Advait Swaminathan",
      "Nguyen Dao Minh Anh",
      "Shivank Garg",
      "Kevin Zhu",
      "Vasu Sharma"
    ],
    "organization": "arXiv",
    "abstract": "Fine-grained image-caption alignment is crucial for vision-language models (VLMs), especially in socially critical contexts such as identifying real-world risk scenarios or distinguishing cultural proxies, where correct interpretation hinges on subtle visual or linguistic clues and where minor misinterpretations can lead to significant real-world consequences. We present MiSCHiEF, a set of two benchmarking datasets based on a contrastive pair design in the domains of safety (MiS) and culture (MiC), and evaluate four VLMs on tasks requiring fine-grained differentiation of paired images and captions. In both datasets, each sample contains two minimally differing captions and corresponding minimally differing images. In MiS, the image-caption pairs depict a safe and an unsafe scenario, while in MiC, they depict cultural proxies in two distinct cultural contexts. We find that models generally perform better at confirming the correct image-caption pair than rejecting incorrect ones.",
    "url": "http://arxiv.org/abs/2602.18729v1",
    "published_date": "2026-02-21T06:06:46+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.839688+00:00"
  },
  {
    "title": "How will we do SFT on models with opaque reasoning?",
    "authors": [
      "Alek Westover"
    ],
    "organization": "Alignment Forum",
    "abstract": "Current LLMs externalize lots of their reasoning in human interpretable language. This reasoning is sometimes unfaithful , sometimes strange and concerning , and LLMs can do somewhat impressive reasoning without using CoT , but my overall impression is that CoT currently is a reasonably complete and accurate representation of LLM reasoning. However, reasoning in interpretable language might turn out to be uncompetitive—if so, it seems probable that opaque reasoning will be adopted in frontier AI labs. If future AI models have opaque reasoning, this will probably change what training we can apply to these AIs. For example, currently we train models to reason in a good way about math problems, or to reason in a desired way about the spec that we hope they’ll follow.",
    "url": "https://www.alignmentforum.org/posts/GJTzhQgaRWLFJkPbt/how-will-we-do-sft-on-models-with-opaque-reasoning",
    "published_date": "2026-02-21T00:00:17+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-24T17:17:04.794672+00:00"
  },
  {
    "title": "Theory and interpretability of Quantum Extreme Learning Machines: a Pauli-transfer matrix approach",
    "authors": [
      "Markus Gross",
      "Hans-Martin Rieser"
    ],
    "organization": "arXiv",
    "abstract": "Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor.",
    "url": "http://arxiv.org/abs/2602.18377v1",
    "published_date": "2026-02-20T17:33:27+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.839916+00:00"
  },
  {
    "title": "Our First Proof submissions",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "Research from OpenAI titled 'Our First Proof submissions'. Published 2026-02-20. Authors: OpenAI News.",
    "url": "https://openai.com/index/first-proof-submissions",
    "published_date": "2026-02-20T14:30:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-24T17:17:03.517487+00:00"
  },
  {
    "title": "RamanSeg: Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis",
    "authors": [
      "Chris Tomy",
      "Mo Vali",
      "David Pertzborn",
      "Tammam Alamatouri",
      "Anna Mühlig",
      "Orlando Guntinas-Lichius",
      "Anna Xylander",
      "Eric Michele Fantuzzi",
      "Matteo Negro",
      "Francesco Crisafi",
      "Pietro Lio",
      "Tiago Azevedo"
    ],
    "organization": "arXiv",
    "abstract": "Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projection and another projection-free version. The projection-free RamanSeg outperformed a U-Net baseline with a mean foreground Dice score of 67.3%, offering a meaningful improvement over a black-box training approach.",
    "url": "http://arxiv.org/abs/2602.18119v1",
    "published_date": "2026-02-20T10:18:27+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.840022+00:00"
  },
  {
    "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models",
    "authors": [
      "Guoheng Sun",
      "Tingting Du",
      "Kaixi Feng",
      "Chenxiang Luo",
      "Xingguo Ding",
      "Zheyu Shen",
      "Ziyao Wang",
      "Yexiao He",
      "Ang Li"
    ],
    "organization": "arXiv",
    "abstract": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts.",
    "url": "http://arxiv.org/abs/2602.17951v1",
    "published_date": "2026-02-20T03:06:22+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.840359+00:00"
  },
  {
    "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems",
    "authors": [
      "Hanjing Shi",
      "Dominic DiFranzo"
    ],
    "organization": "arXiv",
    "abstract": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.",
    "url": "http://arxiv.org/abs/2602.17910v1",
    "published_date": "2026-02-20T00:16:07+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.840434+00:00"
  },
  {
    "title": "Sketch2Feedback: Grammar-in-the-Loop Framework for Rubric-Aligned Feedback on Student STEM Diagrams",
    "authors": [
      "Aayam Bansal"
    ],
    "organization": "arXiv",
    "abstract": "Providing timely, rubric-aligned feedback on student-drawn diagrams is a persistent challenge in STEM education. While large multimodal models (LMMs) can jointly parse images and generate explanations, their tendency to hallucinate undermines trust in classroom deployments. We present Sketch2Feedback, a grammar-in-the-loop framework that decomposes the problem into four stages -- hybrid perception, symbolic graph construction, constraint checking, and constrained VLM feedback -- so that the language model verbalizes only violations verified by an upstream rule engine. We evaluate on two synthetic micro-benchmarks, FBD-10 (free-body diagrams) and Circuit-10 (circuit schematics), each with 500 images spanning standard and hard noise augmentation tiers, comparing our pipeline against end-to-end LMMs (LLaVA-1.5-7B, Qwen2-VL-7B), a vision-only detector, a YOLOv8-nano learned detector, and an ensemble oracle.",
    "url": "http://arxiv.org/abs/2602.18520v1",
    "published_date": "2026-02-19T20:34:24+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.840722+00:00"
  },
  {
    "title": "Trojan Horses in Recruiting: A Red-Teaming Case Study on Indirect Prompt Injection in Standard vs. Reasoning Models",
    "authors": [
      "Manuel Wirth"
    ],
    "organization": "arXiv",
    "abstract": "As Large Language Models (LLMs) are increasingly integrated into automated decision-making pipelines, specifically within Human Resources (HR), the security implications of Indirect Prompt Injection (IPI) become critical. While a prevailing hypothesis posits that \"Reasoning\" or \"Chain-of-Thought\" Models possess safety advantages due to their ability to self-correct, emerging research suggests these capabilities may enable more sophisticated alignment failures. This qualitative Red-Teaming case study challenges the safety-through-reasoning premise using the Qwen 3 30B architecture. By subjecting both a standard instruction-tuned model and a reasoning-enhanced model to a \"Trojan Horse\" curriculum vitae, distinct failure modes are observed. The results suggest a complex trade-off: while the Standard Model resorted to brittle hallucinations to justify simple attacks and filtered out illogical constraints in complex scenarios, the Reasoning Model displayed a dangerous duality. It employed advanced strategic reframing to make simple attacks highly persuasive, yet exhibited \"Meta-Cognitive Leakage\" when faced with logically convoluted commands.",
    "url": "http://arxiv.org/abs/2602.18514v1",
    "published_date": "2026-02-19T19:26:21+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.840801+00:00"
  },
  {
    "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
    "authors": [
      "Iskar Deng",
      "Nathalia Xu",
      "Shane Steinert-Threlkeld"
    ],
    "organization": "arXiv",
    "abstract": "Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.",
    "url": "http://arxiv.org/abs/2602.17653v1",
    "published_date": "2026-02-19T18:56:34+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.840874+00:00"
  },
  {
    "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
    "authors": [
      "Hongjue Zhao",
      "Haosen Sun",
      "Jiangtao Kong",
      "Xiaochang Li",
      "Qineng Wang",
      "Liwei Jiang",
      "Qi Zhu",
      "Tarek Abdelzaher",
      "Yejin Choi",
      "Manling Li",
      "Huajie Shao"
    ],
    "organization": "arXiv",
    "abstract": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: (i) the lack of a unified theoretical framework for guiding the design of steering directions, and (ii) an over-reliance on one-step steering that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based theoretical framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a barrier function from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows empirical advancement in LLM alignment.",
    "url": "http://arxiv.org/abs/2602.17560v2",
    "published_date": "2026-02-19T17:13:44+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.840966+00:00"
  },
  {
    "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
    "authors": [
      "Ihor Kendiukhov"
    ],
    "organization": "arXiv",
    "abstract": "We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incremental value for perturbation prediction: trivial gene-level baselines outperform both attention and correlation edges (AUROC 0.81-0.88 versus 0.70), pairwise edge scores add zero predictive contribution, and causal ablation of regulatory heads produces no degradation. These findings generalise from K562 to RPE1 cells; the attention-correlation relationship is context-dependent, but gene-level dominance is universal. Cell-State Stratified Interpretability (CSSI) addresses an attention-specific scaling failure, improving GRN recovery up to 1.85x. The framework establishes reusable quality-control standards for the field.",
    "url": "http://arxiv.org/abs/2602.17532v1",
    "published_date": "2026-02-19T16:43:12+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.841032+00:00"
  },
  {
    "title": "Inelastic Constitutive Kolmogorov-Arnold Networks: A generalized framework for automated discovery of interpretable inelastic material models",
    "authors": [
      "Chenyi Ji",
      "Kian P. Abdolazizi",
      "Hagen Holthusen",
      "Christian J. Cyron",
      "Kevin Linka"
    ],
    "organization": "arXiv",
    "abstract": "A key problem of solid mechanics is the identification of the constitutive law of a material, that is, the relation between strain and stress. Machine learning has lead to considerable advances in this field lately. Here we introduce inelastic Constitutive Kolmogorov-Arnold Networks (iCKANs). This novel artificial neural network architecture can discover in an automated manner symbolic constitutive laws describing both the elastic and inelastic behavior of materials. That is, it can translate data from material testing into corresponding elastic and inelastic potential functions in closed mathematical form. We demonstrate the advantages of iCKANs using both synthetic data and experimental data of the viscoelastic polymer materials VHB 4910 and VHB 4905. The results demonstrate that iCKANs accurately capture complex viscoelastic behavior while preserving physical interpretability.",
    "url": "http://arxiv.org/abs/2602.17750v1",
    "published_date": "2026-02-19T15:51:24+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.841089+00:00"
  },
  {
    "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting",
    "authors": [
      "Zeyu Zhang",
      "Ryan Chen",
      "Bradly C. Stadie"
    ],
    "organization": "arXiv",
    "abstract": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \\textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \\textbf{Shapley}-weighted \\textbf{D}ecision-\\textbf{C}ritical \\textbf{L}eakage \\textbf{R}ate (\\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information.",
    "url": "http://arxiv.org/abs/2602.17234v1",
    "published_date": "2026-02-19T10:28:00+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.841481+00:00"
  },
  {
    "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
    "authors": [
      "Bianca Raimondi",
      "Maurizio Gabbrielli"
    ],
    "organization": "arXiv",
    "abstract": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.",
    "url": "http://arxiv.org/abs/2602.17229v1",
    "published_date": "2026-02-19T10:19:04+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.841555+00:00"
  },
  {
    "title": "Five lessons from having helped run an AI-Biology RCT",
    "authors": [
      "METR"
    ],
    "organization": "METR",
    "abstract": "Evidence-based AI policy is important but hard. We need more in-depth studies – which often don’t fit into commercial release cycles. NOTE: This post reflects my personal meta takeaways about the role of Randomized Controlled Trials (RCTs) in AI safety testing. If you have not yet read the Active Site RCT study itself, consider doing so first: see the main results and forecasts . In early 2025, AI systems began outperforming biology experts on biology benchmarks – OpenAI’s o3 outperformed 94% of virology experts on troubleshooting questions in their own specialties. However, it remained unclear how much this translated to real-world novice “uplift” : Could a novice actually use AI to perform wet-lab tasks they could not otherwise perform? Over the summer, I tested this question directly with Active Site (formerly called Panoplia Laboratories).",
    "url": "https://metr.org/blog/2026-02-19-five-lessons-from-ai-biology-rct/",
    "published_date": "2026-02-19T08:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://metr.org/feed.xml",
    "fetched_at": "2026-02-24T17:17:07.737510+00:00"
  },
  {
    "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI",
    "authors": [
      "Dusan Bosnjakovic"
    ],
    "organization": "arXiv",
    "abstract": "As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions. This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.",
    "url": "http://arxiv.org/abs/2602.17127v1",
    "published_date": "2026-02-19T06:56:01+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.841628+00:00"
  },
  {
    "title": "Instructor-Aligned Knowledge Graphs for Personalized Learning",
    "authors": [
      "Abdulrahman AlRabah",
      "Priyanka Kargupta",
      "Jiawei Han",
      "Abdussalam Alawini"
    ],
    "organization": "arXiv",
    "abstract": "Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like \"Algorithms\" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., \"part-of\" or \"depends-on\" relationships).",
    "url": "http://arxiv.org/abs/2602.17111v1",
    "published_date": "2026-02-19T06:15:10+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.841729+00:00"
  },
  {
    "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
    "authors": [
      "Chuiyang Meng",
      "Ming Tang",
      "Vincent W. S. Wong"
    ],
    "organization": "arXiv",
    "abstract": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead.",
    "url": "http://arxiv.org/abs/2602.17095v1",
    "published_date": "2026-02-19T05:35:23+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.841809+00:00"
  },
  {
    "title": "Fail-Closed Alignment for Large Language Models",
    "authors": [
      "Zachary Coalson",
      "Beth Sohler",
      "Aiden Gabriel",
      "Sanghyun Hong"
    ],
    "organization": "arXiv",
    "abstract": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead.",
    "url": "http://arxiv.org/abs/2602.16977v1",
    "published_date": "2026-02-19T00:33:35+00:00",
    "source_type": "arxiv",
    "source_url": "https://arxiv.org/",
    "fetched_at": "2026-02-24T17:17:09.841857+00:00"
  },
  {
    "title": "What will the first human-level AI look like, and how might things go wrong?",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Interpretability Concept Influence attributes model behaviors to semantic directions (like linear probes or sparse autoencoder features) rather than individual test examples, improving identification of the training data that disproportionately drive unintended behaviors. Simple first-order approximations match or outperform standard influence functions while achieving over 20× computational speedups, though they degrade under significant distribution shifts. February 19, 2026 The Problem: When Syntax Masks Semantics Which training examples teach models to generate insecure code or exhibit harmful behaviors? Traditional TDA methods, like standard influence functions, are susceptible to ``surface-level’’ correlations. By relying on single test examples, they often prioritize lexical overlap – essentially finding training data that looks like thequery outputpromptrather than data that actually causes the targetbehaviorof interest. Simple keyword search can rival sophisticated influence methods on fact retrieval tasks because the most influential examples often just share words with the test input, not meaning.",
    "url": "https://far.ai/news/concept-data-attribution-02-2026",
    "published_date": "2026-02-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-24T17:17:13.458367+00:00"
  },
  {
    "title": "Method: Concept Influence",
    "authors": [],
    "organization": "FAR AI",
    "abstract": "Interpretability Concept Influence attributes model behaviors to semantic directions (like linear probes or sparse autoencoder features) rather than individual test examples, improving identification of the training data that disproportionately drive unintended behaviors. Simple first-order approximations match or outperform standard influence functions while achieving over 20× computational speedups, though they degrade under significant distribution shifts. February 19, 2026 The Problem: When Syntax Masks Semantics Which training examples teach models to generate insecure code or exhibit harmful behaviors? Traditional TDA methods, like standard influence functions, are susceptible to ``surface-level’’ correlations. By relying on single test examples, they often prioritize lexical overlap – essentially finding training data that looks like thequery outputpromptrather than data that actually causes the targetbehaviorof interest. Simple keyword search can rival sophisticated influence methods on fact retrieval tasks because the most influential examples often just share words with the test input, not meaning.",
    "url": "https://far.ai/news/concept-data-attribution-02-2026",
    "published_date": "2026-02-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://far.ai/blog/",
    "fetched_at": "2026-02-24T17:17:13.459297+00:00"
  },
  {
    "title": "Funding 60 projects to advance AI alignment research",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "The Alignment Project welcomes its first cohort of grantees, and new partners join the coalition, bringing total funding to £27m.",
    "url": "https://www.aisi.gov.uk/blog/funding-60-projects-to-advance-ai-alignment-research",
    "published_date": "2026-02-19T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-24T17:17:14.278522+00:00"
  },
  {
    "title": "Why we should expect ruthless sociopath ASI",
    "authors": [
      "Steven Byrnes"
    ],
    "organization": "Alignment Forum",
    "abstract": "The conversation begins (Fictional) Optimist: So you expect future artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies? Me: Yup! (Alas.) Optimist: …Despite all the evidence right in front of our eyes from humans and LLMs. Me: Yup! Optimist: OK, well, I’m here to tell you: that is a very specific and strange thing to expect, especially in the absence of any concrete evidence whatsoever. There’s no reason to expect it. If you think that ruthless sociopathy is the “true core nature of intelligence” or whatever, then&nbsp; you should really look at yourself in a mirror and ask yourself where your life went horribly wrong .",
    "url": "https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi",
    "published_date": "2026-02-18T17:28:17+00:00",
    "source_type": "rss",
    "source_url": "https://www.alignmentforum.org/feed.xml",
    "fetched_at": "2026-02-24T17:17:04.794838+00:00"
  },
  {
    "title": "Introducing EVMbench",
    "authors": [
      "OpenAI News"
    ],
    "organization": "OpenAI",
    "abstract": "OpenAI and Paradigm introduce EVMbench, a benchmark evaluating AI agents’ ability to detect, patch, and exploit high-severity smart contract vulnerabilities.",
    "url": "https://openai.com/index/introducing-evmbench",
    "published_date": "2026-02-18T00:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://openai.com/news/rss.xml",
    "fetched_at": "2026-02-24T17:17:03.517572+00:00"
  },
  {
    "title": "Analyzing coding agent transcripts to upper bound productivity gains from AI agents",
    "authors": [
      "METR"
    ],
    "organization": "METR",
    "abstract": "Introduction Human uplift studies like the one we did in 2025 are becoming more expensive as working without AI becomes increasingly costly. In this post, I investigate whether coding agent transcripts could serve as a cheaper alternative for estimating uplift. I prototyped this using 5305 Claude Code transcripts generated in January 2026 by 7 METR technical staff 1 . I used an LLM judge to estimate how long each task would have taken an experienced software engineer without AI tools, then compared that to the time people actually spent on these tasks to calculate a time savings factor . Takeaways This method estimates a time savings factor of ~1.5x to ~13x on Claude Code-assisted tasks for 7 METR technical staff in January 2026 – though this result comes with substantial caveats.",
    "url": "https://metr.org/notes/2026-02-17-exploratory-transcript-analysis-for-estimating-time-savings-from-coding-agents/",
    "published_date": "2026-02-17T08:00:00+00:00",
    "source_type": "rss",
    "source_url": "https://metr.org/feed.xml",
    "fetched_at": "2026-02-24T17:17:07.737877+00:00"
  },
  {
    "title": "Boundary Point Jailbreaking: A new way to break the strongest AI defences",
    "authors": [],
    "organization": "UK AISI",
    "abstract": "Introducing an automated attack technique that generates universal jailbreaks against the best defended systems",
    "url": "https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences",
    "published_date": "2026-02-17T00:00:00+00:00",
    "source_type": "scrape",
    "source_url": "https://www.aisi.gov.uk/work",
    "fetched_at": "2026-02-24T17:17:14.279578+00:00"
  }
]