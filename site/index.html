<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Safety Weekly Digest — February 16, 2026 to February 22, 2026</title>
  <meta name="description" content="A curated weekly digest of 393 AI safety research papers from leading organizations and researchers. February 16, 2026 through February 22, 2026.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  <style>
    /* =============================================
   AI Safety Weekly Digest — Editorial Stylesheet
   Modern research publication aesthetic
   ============================================= */

/* --- CSS Custom Properties --- */
:root {
  /* Base palette */
  --bg: #f8f8f7;
  --bg-card: #ffffff;
  --bg-elevated: #ffffff;
  --bg-hero: #fcfcfb;
  --text: #1a1a1a;
  --text-secondary: #4a4a4a;
  --text-muted: #6b6b6b;
  --text-faint: #999999;
  --heading: #0d0d0d;
  --border: #e6e4e0;
  --border-light: #f0eee9;
  --shadow-sm: 0 1px 3px rgba(0, 0, 0, 0.04);
  --shadow-md: 0 4px 12px rgba(0, 0, 0, 0.06);
  --shadow-lg: 0 8px 30px rgba(0, 0, 0, 0.08);

  /* Accent — deep teal */
  --accent: #0d6e6e;
  --accent-light: #0f8585;
  --accent-subtle: rgba(13, 110, 110, 0.07);
  --accent-text: #0a5a5a;

  /* Links */
  --link: #0d6e6e;
  --link-hover: #0a5252;

  /* Filter bar */
  --filter-bg: rgba(248, 248, 247, 0.88);
  --pill-active-bg: #1a1a1a;
  --pill-active-text: #ffffff;

  /* Card accent fallback */
  --card-accent: var(--org-default);

  /* Spacing scale */
  --space-xs: 0.25rem;
  --space-sm: 0.5rem;
  --space-md: 1rem;
  --space-lg: 1.5rem;
  --space-xl: 2rem;
  --space-2xl: 3rem;
  --space-3xl: 4rem;

  /* Radii */
  --radius-sm: 6px;
  --radius-md: 10px;
  --radius-lg: 14px;

  /* Organization colors — muted, professional tones */
  --org-anthropic: #c4854a;
  --org-openai: #4a8c5c;
  --org-google-deepmind: #4a7ab8;
  --org-metr: #b85a5a;
  --org-apollo-research: #7b5ab8;
  --org-uk-aisi: #3a7a8a;
  --org-redwood-research: #c04040;
  --org-alignment-forum: #4a8a4a;
  --org-arxiv: #777777;
  --org-hyperdimensional: #b8924a;
  --org-peter-wildeford: #5a5ab8;
  --org-zvi-mowshowitz: #8a6a3a;
  --org-rand: #3a6a8a;
  --org-arc: #8a3a6a;
  --org-miri: #5a8a3a;
  --org-cais: #3a8a6a;
  --org-microsoft-research: #3a6ab8;
  --org-import-ai: #b8943a;
  --org-dan-hendrycks: #7a6a3a;
  --org-astral-codex-ten: #5a6a8a;
  --org-cognitive-revolution: #8a5a6a;
  --org-vox-future-perfect: #3a8a6a;
  --org-fli: #7a5a7a;
  --org-epoch-ai: #5a7a6a;
  --org-lesswrong: #4a8a4a;
  --org-far-ai: #6a4a8a;
  --org-us-aisi: #3a5a8a;
  --org-cset: #6a7a3a;
  --org-govai: #4a5a7a;
  --org-iaps: #7a4a5a;
  --org-cltr: #5a4a7a;
  --org-chai: #8a6a4a;
  --org-mats: #4a7a7a;
  --org-paul-christiano: #7a6a4a;
  --org-yoshua-bengio: #4a6a7a;
  --org-lennart-heim: #6a7a4a;
  --org-hacker-news: #e86424;
  --org-reddit: #e84420;
  --org-default: #888888;
}

/* --- Dark Mode --- */
@media (prefers-color-scheme: dark) {
  :root {
    --bg: #111113;
    --bg-card: #1a1a1e;
    --bg-elevated: #222226;
    --bg-hero: #161618;
    --text: #e2e2e6;
    --text-secondary: #b0b0b8;
    --text-muted: #8a8a96;
    --text-faint: #606068;
    --heading: #f0f0f4;
    --border: #2a2a30;
    --border-light: #222228;
    --shadow-sm: 0 1px 3px rgba(0, 0, 0, 0.15);
    --shadow-md: 0 4px 12px rgba(0, 0, 0, 0.25);
    --shadow-lg: 0 8px 30px rgba(0, 0, 0, 0.35);
    --accent: #2ca0a0;
    --accent-light: #35b5b5;
    --accent-subtle: rgba(44, 160, 160, 0.1);
    --accent-text: #2ca0a0;
    --link: #35b5b5;
    --link-hover: #4acaca;
    --filter-bg: rgba(17, 17, 19, 0.88);
    --pill-active-bg: #2ca0a0;
    --pill-active-text: #111113;
  }
}

/* --- Reset & Base --- */
*,
*::before,
*::after {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

html {
  -webkit-text-size-adjust: 100%;
  scroll-behavior: smooth;
}

body {
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
  background-color: var(--bg);
  color: var(--text);
  max-width: 1120px;
  margin: 0 auto;
  padding: 0 clamp(1.25rem, 5vw, 2.5rem);
  font-size: 15px;
  line-height: 1.6;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

a {
  color: var(--link);
  text-decoration: none;
  transition: color 0.15s ease;
}

a:hover {
  color: var(--link-hover);
}

/* --- Masthead / Header --- */
.site-header {
  padding: var(--space-3xl) 0 var(--space-xl);
  text-align: center;
  border-bottom: 1px solid var(--border);
}

.header-inner {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: var(--space-sm);
}

.site-title {
  font-size: clamp(1.75rem, 1.4rem + 1.8vw, 2.75rem);
  font-weight: 800;
  color: var(--heading);
  letter-spacing: -0.04em;
  line-height: 1.1;
}

.header-rule {
  width: 40px;
  height: 2px;
  background: var(--accent);
  border: none;
  margin: var(--space-sm) 0;
  border-radius: 1px;
}

.header-meta {
  display: flex;
  align-items: center;
  gap: var(--space-sm);
  flex-wrap: wrap;
  justify-content: center;
}

.header-daterange {
  font-size: 0.92rem;
  font-weight: 500;
  color: var(--text-secondary);
}

.header-separator {
  color: var(--border);
  font-weight: 300;
}

.header-count {
  font-size: 0.85rem;
  color: var(--text-muted);
}

.header-count strong {
  font-weight: 600;
  color: var(--accent-text);
}

/* --- Filter Bar --- */
.filter-bar {
  position: sticky;
  top: 0;
  z-index: 100;
  margin: 0 calc(-1 * clamp(1.25rem, 5vw, 2.5rem));
  padding: 0 clamp(1.25rem, 5vw, 2.5rem);
  -webkit-backdrop-filter: blur(20px) saturate(180%);
  backdrop-filter: blur(20px) saturate(180%);
  background-color: var(--filter-bg);
  border-bottom: 1px solid var(--border-light);
}

.filter-bar-inner {
  padding: 0.7rem 0;
}

.filter-scroll {
  display: flex;
  gap: 0.35rem;
  overflow-x: auto;
  padding-bottom: 2px;
  scrollbar-width: thin;
  scrollbar-color: var(--border) transparent;
  -webkit-overflow-scrolling: touch;
}

.filter-scroll::-webkit-scrollbar {
  height: 2px;
}

.filter-scroll::-webkit-scrollbar-thumb {
  background: var(--border);
  border-radius: 2px;
}

.filter-pill {
  flex-shrink: 0;
  padding: 0.32rem 0.75rem;
  font-size: 0.76rem;
  font-family: inherit;
  font-weight: 500;
  border: 1px solid var(--border);
  border-radius: 100px;
  background: transparent;
  color: var(--text-muted);
  cursor: pointer;
  transition: all 0.15s ease;
  white-space: nowrap;
  line-height: 1.4;
}

.filter-pill:hover {
  border-color: var(--text-faint);
  color: var(--text-secondary);
  background-color: var(--bg-elevated);
}

.filter-pill.active {
  background-color: var(--pill-active-bg);
  border-color: var(--pill-active-bg);
  color: var(--pill-active-text);
  font-weight: 600;
}

.pill-count {
  font-size: 0.68rem;
  opacity: 0.65;
  margin-left: 0.15rem;
}

/* --- Section Labels --- */
.section-label {
  font-size: 0.7rem;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 0.1em;
  color: var(--text-faint);
  margin-bottom: var(--space-lg);
  padding-bottom: var(--space-sm);
  border-bottom: 1px solid var(--border-light);
}

/* --- Featured / Hero Section --- */
.hero-section {
  padding-top: var(--space-xl);
  padding-bottom: var(--space-md);
}

.hero-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  grid-template-rows: auto auto;
  gap: var(--space-md);
}

/* First featured paper: spans the full left column across both rows */
.hero-grid .hero-card:first-child {
  grid-row: 1 / 3;
}

/* --- Base Paper Card --- */
.paper-card {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: var(--radius-md);
  padding: var(--space-lg);
  transition: border-color 0.2s ease, box-shadow 0.25s ease, transform 0.2s ease;
  position: relative;
  display: flex;
  flex-direction: column;
}

.paper-card:hover {
  border-color: color-mix(in srgb, var(--card-accent) 35%, var(--border));
  box-shadow: var(--shadow-md);
}

/* --- Hero Card --- */
.hero-card {
  border-radius: var(--radius-lg);
  padding: clamp(1.25rem, 3vw, 2rem);
  border-left: 3px solid var(--card-accent);
}

.hero-card:hover {
  transform: translateY(-2px);
  box-shadow: var(--shadow-lg);
}

.hero-card .paper-title {
  font-size: clamp(1.15rem, 1rem + 0.5vw, 1.4rem);
  font-weight: 700;
  line-height: 1.28;
  margin-bottom: var(--space-sm);
}

.hero-card .paper-authors {
  font-size: 0.84rem;
  margin-bottom: var(--space-xs);
}

.hero-card .abstract-preview {
  -webkit-line-clamp: 4;
}

.hero-card .paper-abstract {
  flex: 1;
}

/* --- Papers Grid --- */
.papers-section {
  padding-top: var(--space-xl);
  padding-bottom: var(--space-2xl);
}

.papers-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
  gap: var(--space-md);
}

/* --- Grid Card --- */
.grid-card {
  border-top: 3px solid var(--card-accent);
  border-left: none;
}

.grid-card .paper-title {
  font-size: clamp(0.95rem, 0.9rem + 0.2vw, 1.05rem);
  margin-bottom: var(--space-xs);
}

.grid-card .paper-abstract {
  flex: 1;
}

.grid-card .read-link {
  margin-top: auto;
  padding-top: var(--space-sm);
}

/* Card accent colors per org */
.paper-card[data-org="Anthropic"] { --card-accent: var(--org-anthropic); }
.paper-card[data-org="OpenAI"] { --card-accent: var(--org-openai); }
.paper-card[data-org="Google DeepMind"] { --card-accent: var(--org-google-deepmind); }
.paper-card[data-org="METR"] { --card-accent: var(--org-metr); }
.paper-card[data-org="Apollo Research"] { --card-accent: var(--org-apollo-research); }
.paper-card[data-org="UK AISI"] { --card-accent: var(--org-uk-aisi); }
.paper-card[data-org="Redwood Research"] { --card-accent: var(--org-redwood-research); }
.paper-card[data-org="Alignment Forum"] { --card-accent: var(--org-alignment-forum); }
.paper-card[data-org="arXiv"] { --card-accent: var(--org-arxiv); }
.paper-card[data-org="Hyperdimensional"] { --card-accent: var(--org-hyperdimensional); }
.paper-card[data-org="Peter Wildeford"] { --card-accent: var(--org-peter-wildeford); }
.paper-card[data-org="Zvi Mowshowitz"] { --card-accent: var(--org-zvi-mowshowitz); }
.paper-card[data-org="RAND"] { --card-accent: var(--org-rand); }
.paper-card[data-org="ARC"] { --card-accent: var(--org-arc); }
.paper-card[data-org="MIRI"] { --card-accent: var(--org-miri); }
.paper-card[data-org="CAIS"] { --card-accent: var(--org-cais); }
.paper-card[data-org="Microsoft Research"] { --card-accent: var(--org-microsoft-research); }
.paper-card[data-org="Import AI"] { --card-accent: var(--org-import-ai); }
.paper-card[data-org="Dan Hendrycks"] { --card-accent: var(--org-dan-hendrycks); }
.paper-card[data-org="Astral Codex Ten"] { --card-accent: var(--org-astral-codex-ten); }
.paper-card[data-org="Cognitive Revolution"] { --card-accent: var(--org-cognitive-revolution); }
.paper-card[data-org="Vox Future Perfect"] { --card-accent: var(--org-vox-future-perfect); }
.paper-card[data-org="FLI"] { --card-accent: var(--org-fli); }
.paper-card[data-org="Epoch AI"] { --card-accent: var(--org-epoch-ai); }
.paper-card[data-org="LessWrong"] { --card-accent: var(--org-lesswrong); }
.paper-card[data-org="FAR AI"] { --card-accent: var(--org-far-ai); }
.paper-card[data-org="US AISI"] { --card-accent: var(--org-us-aisi); }
.paper-card[data-org="CSET"] { --card-accent: var(--org-cset); }
.paper-card[data-org="GovAI"] { --card-accent: var(--org-govai); }
.paper-card[data-org="IAPS"] { --card-accent: var(--org-iaps); }
.paper-card[data-org="CLTR"] { --card-accent: var(--org-cltr); }
.paper-card[data-org="CHAI"] { --card-accent: var(--org-chai); }
.paper-card[data-org="MATS"] { --card-accent: var(--org-mats); }
.paper-card[data-org="Paul Christiano"] { --card-accent: var(--org-paul-christiano); }
.paper-card[data-org="Yoshua Bengio"] { --card-accent: var(--org-yoshua-bengio); }
.paper-card[data-org="Lennart Heim"] { --card-accent: var(--org-lennart-heim); }
.paper-card[data-org="Hacker News"] { --card-accent: var(--org-hacker-news); }
.paper-card[data-org="Reddit"] { --card-accent: var(--org-reddit); }

/* --- Paper Meta Row --- */
.paper-meta {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: var(--space-sm);
  gap: var(--space-sm);
}

/* --- Organization Tag (pill) --- */
.org-tag {
  display: inline-block;
  font-size: 0.65rem;
  font-weight: 600;
  padding: 0.18rem 0.55rem;
  border-radius: 100px;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  line-height: 1.4;
  background-color: color-mix(in srgb, var(--org-default) 12%, var(--bg-card));
  color: var(--org-default);
}

/* Org-specific tag colors — using color-mix for subtle backgrounds */
.org-tag[data-org="Anthropic"] { background-color: color-mix(in srgb, var(--org-anthropic) 12%, var(--bg-card)); color: var(--org-anthropic); }
.org-tag[data-org="OpenAI"] { background-color: color-mix(in srgb, var(--org-openai) 12%, var(--bg-card)); color: var(--org-openai); }
.org-tag[data-org="Google DeepMind"] { background-color: color-mix(in srgb, var(--org-google-deepmind) 12%, var(--bg-card)); color: var(--org-google-deepmind); }
.org-tag[data-org="METR"] { background-color: color-mix(in srgb, var(--org-metr) 12%, var(--bg-card)); color: var(--org-metr); }
.org-tag[data-org="Apollo Research"] { background-color: color-mix(in srgb, var(--org-apollo-research) 12%, var(--bg-card)); color: var(--org-apollo-research); }
.org-tag[data-org="UK AISI"] { background-color: color-mix(in srgb, var(--org-uk-aisi) 12%, var(--bg-card)); color: var(--org-uk-aisi); }
.org-tag[data-org="Redwood Research"] { background-color: color-mix(in srgb, var(--org-redwood-research) 12%, var(--bg-card)); color: var(--org-redwood-research); }
.org-tag[data-org="Alignment Forum"] { background-color: color-mix(in srgb, var(--org-alignment-forum) 12%, var(--bg-card)); color: var(--org-alignment-forum); }
.org-tag[data-org="arXiv"] { background-color: color-mix(in srgb, var(--org-arxiv) 12%, var(--bg-card)); color: var(--org-arxiv); }
.org-tag[data-org="Hyperdimensional"] { background-color: color-mix(in srgb, var(--org-hyperdimensional) 12%, var(--bg-card)); color: var(--org-hyperdimensional); }
.org-tag[data-org="Peter Wildeford"] { background-color: color-mix(in srgb, var(--org-peter-wildeford) 12%, var(--bg-card)); color: var(--org-peter-wildeford); }
.org-tag[data-org="Zvi Mowshowitz"] { background-color: color-mix(in srgb, var(--org-zvi-mowshowitz) 12%, var(--bg-card)); color: var(--org-zvi-mowshowitz); }
.org-tag[data-org="RAND"] { background-color: color-mix(in srgb, var(--org-rand) 12%, var(--bg-card)); color: var(--org-rand); }
.org-tag[data-org="ARC"] { background-color: color-mix(in srgb, var(--org-arc) 12%, var(--bg-card)); color: var(--org-arc); }
.org-tag[data-org="MIRI"] { background-color: color-mix(in srgb, var(--org-miri) 12%, var(--bg-card)); color: var(--org-miri); }
.org-tag[data-org="CAIS"] { background-color: color-mix(in srgb, var(--org-cais) 12%, var(--bg-card)); color: var(--org-cais); }
.org-tag[data-org="Microsoft Research"] { background-color: color-mix(in srgb, var(--org-microsoft-research) 12%, var(--bg-card)); color: var(--org-microsoft-research); }
.org-tag[data-org="Import AI"] { background-color: color-mix(in srgb, var(--org-import-ai) 12%, var(--bg-card)); color: var(--org-import-ai); }
.org-tag[data-org="Dan Hendrycks"] { background-color: color-mix(in srgb, var(--org-dan-hendrycks) 12%, var(--bg-card)); color: var(--org-dan-hendrycks); }
.org-tag[data-org="Astral Codex Ten"] { background-color: color-mix(in srgb, var(--org-astral-codex-ten) 12%, var(--bg-card)); color: var(--org-astral-codex-ten); }
.org-tag[data-org="Cognitive Revolution"] { background-color: color-mix(in srgb, var(--org-cognitive-revolution) 12%, var(--bg-card)); color: var(--org-cognitive-revolution); }
.org-tag[data-org="Vox Future Perfect"] { background-color: color-mix(in srgb, var(--org-vox-future-perfect) 12%, var(--bg-card)); color: var(--org-vox-future-perfect); }
.org-tag[data-org="FLI"] { background-color: color-mix(in srgb, var(--org-fli) 12%, var(--bg-card)); color: var(--org-fli); }
.org-tag[data-org="Epoch AI"] { background-color: color-mix(in srgb, var(--org-epoch-ai) 12%, var(--bg-card)); color: var(--org-epoch-ai); }
.org-tag[data-org="LessWrong"] { background-color: color-mix(in srgb, var(--org-lesswrong) 12%, var(--bg-card)); color: var(--org-lesswrong); }
.org-tag[data-org="FAR AI"] { background-color: color-mix(in srgb, var(--org-far-ai) 12%, var(--bg-card)); color: var(--org-far-ai); }
.org-tag[data-org="US AISI"] { background-color: color-mix(in srgb, var(--org-us-aisi) 12%, var(--bg-card)); color: var(--org-us-aisi); }
.org-tag[data-org="CSET"] { background-color: color-mix(in srgb, var(--org-cset) 12%, var(--bg-card)); color: var(--org-cset); }
.org-tag[data-org="GovAI"] { background-color: color-mix(in srgb, var(--org-govai) 12%, var(--bg-card)); color: var(--org-govai); }
.org-tag[data-org="IAPS"] { background-color: color-mix(in srgb, var(--org-iaps) 12%, var(--bg-card)); color: var(--org-iaps); }
.org-tag[data-org="CLTR"] { background-color: color-mix(in srgb, var(--org-cltr) 12%, var(--bg-card)); color: var(--org-cltr); }
.org-tag[data-org="CHAI"] { background-color: color-mix(in srgb, var(--org-chai) 12%, var(--bg-card)); color: var(--org-chai); }
.org-tag[data-org="MATS"] { background-color: color-mix(in srgb, var(--org-mats) 12%, var(--bg-card)); color: var(--org-mats); }
.org-tag[data-org="Paul Christiano"] { background-color: color-mix(in srgb, var(--org-paul-christiano) 12%, var(--bg-card)); color: var(--org-paul-christiano); }
.org-tag[data-org="Yoshua Bengio"] { background-color: color-mix(in srgb, var(--org-yoshua-bengio) 12%, var(--bg-card)); color: var(--org-yoshua-bengio); }
.org-tag[data-org="Lennart Heim"] { background-color: color-mix(in srgb, var(--org-lennart-heim) 12%, var(--bg-card)); color: var(--org-lennart-heim); }
.org-tag[data-org="Hacker News"] { background-color: color-mix(in srgb, var(--org-hacker-news) 12%, var(--bg-card)); color: var(--org-hacker-news); }
.org-tag[data-org="Reddit"] { background-color: color-mix(in srgb, var(--org-reddit) 12%, var(--bg-card)); color: var(--org-reddit); }

/* Dark mode org tag adjustments */
@media (prefers-color-scheme: dark) {
  .org-tag {
    background-color: color-mix(in srgb, var(--org-default) 15%, var(--bg-card));
  }
  .org-tag[data-org="Anthropic"] { background-color: color-mix(in srgb, var(--org-anthropic) 15%, var(--bg-card)); }
  .org-tag[data-org="OpenAI"] { background-color: color-mix(in srgb, var(--org-openai) 15%, var(--bg-card)); }
  .org-tag[data-org="Google DeepMind"] { background-color: color-mix(in srgb, var(--org-google-deepmind) 15%, var(--bg-card)); }
  .org-tag[data-org="METR"] { background-color: color-mix(in srgb, var(--org-metr) 15%, var(--bg-card)); }
  .org-tag[data-org="Apollo Research"] { background-color: color-mix(in srgb, var(--org-apollo-research) 15%, var(--bg-card)); }
  .org-tag[data-org="UK AISI"] { background-color: color-mix(in srgb, var(--org-uk-aisi) 15%, var(--bg-card)); }
  .org-tag[data-org="Redwood Research"] { background-color: color-mix(in srgb, var(--org-redwood-research) 15%, var(--bg-card)); }
  .org-tag[data-org="Alignment Forum"] { background-color: color-mix(in srgb, var(--org-alignment-forum) 15%, var(--bg-card)); }
  .org-tag[data-org="arXiv"] { background-color: color-mix(in srgb, var(--org-arxiv) 15%, var(--bg-card)); }
  .org-tag[data-org="Hyperdimensional"] { background-color: color-mix(in srgb, var(--org-hyperdimensional) 15%, var(--bg-card)); }
  .org-tag[data-org="Peter Wildeford"] { background-color: color-mix(in srgb, var(--org-peter-wildeford) 15%, var(--bg-card)); }
  .org-tag[data-org="Zvi Mowshowitz"] { background-color: color-mix(in srgb, var(--org-zvi-mowshowitz) 15%, var(--bg-card)); }
  .org-tag[data-org="RAND"] { background-color: color-mix(in srgb, var(--org-rand) 15%, var(--bg-card)); }
  .org-tag[data-org="ARC"] { background-color: color-mix(in srgb, var(--org-arc) 15%, var(--bg-card)); }
  .org-tag[data-org="MIRI"] { background-color: color-mix(in srgb, var(--org-miri) 15%, var(--bg-card)); }
  .org-tag[data-org="CAIS"] { background-color: color-mix(in srgb, var(--org-cais) 15%, var(--bg-card)); }
  .org-tag[data-org="Microsoft Research"] { background-color: color-mix(in srgb, var(--org-microsoft-research) 15%, var(--bg-card)); }
  .org-tag[data-org="Import AI"] { background-color: color-mix(in srgb, var(--org-import-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="Dan Hendrycks"] { background-color: color-mix(in srgb, var(--org-dan-hendrycks) 15%, var(--bg-card)); }
  .org-tag[data-org="Astral Codex Ten"] { background-color: color-mix(in srgb, var(--org-astral-codex-ten) 15%, var(--bg-card)); }
  .org-tag[data-org="Cognitive Revolution"] { background-color: color-mix(in srgb, var(--org-cognitive-revolution) 15%, var(--bg-card)); }
  .org-tag[data-org="Vox Future Perfect"] { background-color: color-mix(in srgb, var(--org-vox-future-perfect) 15%, var(--bg-card)); }
  .org-tag[data-org="FLI"] { background-color: color-mix(in srgb, var(--org-fli) 15%, var(--bg-card)); }
  .org-tag[data-org="Epoch AI"] { background-color: color-mix(in srgb, var(--org-epoch-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="LessWrong"] { background-color: color-mix(in srgb, var(--org-lesswrong) 15%, var(--bg-card)); }
  .org-tag[data-org="FAR AI"] { background-color: color-mix(in srgb, var(--org-far-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="US AISI"] { background-color: color-mix(in srgb, var(--org-us-aisi) 15%, var(--bg-card)); }
  .org-tag[data-org="CSET"] { background-color: color-mix(in srgb, var(--org-cset) 15%, var(--bg-card)); }
  .org-tag[data-org="GovAI"] { background-color: color-mix(in srgb, var(--org-govai) 15%, var(--bg-card)); }
  .org-tag[data-org="IAPS"] { background-color: color-mix(in srgb, var(--org-iaps) 15%, var(--bg-card)); }
  .org-tag[data-org="CLTR"] { background-color: color-mix(in srgb, var(--org-cltr) 15%, var(--bg-card)); }
  .org-tag[data-org="CHAI"] { background-color: color-mix(in srgb, var(--org-chai) 15%, var(--bg-card)); }
  .org-tag[data-org="MATS"] { background-color: color-mix(in srgb, var(--org-mats) 15%, var(--bg-card)); }
  .org-tag[data-org="Paul Christiano"] { background-color: color-mix(in srgb, var(--org-paul-christiano) 15%, var(--bg-card)); }
  .org-tag[data-org="Yoshua Bengio"] { background-color: color-mix(in srgb, var(--org-yoshua-bengio) 15%, var(--bg-card)); }
  .org-tag[data-org="Lennart Heim"] { background-color: color-mix(in srgb, var(--org-lennart-heim) 15%, var(--bg-card)); }
  .org-tag[data-org="Hacker News"] { background-color: color-mix(in srgb, var(--org-hacker-news) 15%, var(--bg-card)); }
  .org-tag[data-org="Reddit"] { background-color: color-mix(in srgb, var(--org-reddit) 15%, var(--bg-card)); }
}

/* --- Paper Title --- */
.paper-title {
  font-size: clamp(1rem, 0.95rem + 0.3vw, 1.15rem);
  font-weight: 650;
  line-height: 1.32;
  margin-bottom: var(--space-xs);
  letter-spacing: -0.015em;
}

.paper-title a {
  color: var(--heading);
  text-decoration: none;
  transition: color 0.15s ease;
}

.paper-title a:hover {
  color: var(--link);
}

/* --- Paper Authors --- */
.paper-authors {
  font-size: 0.8rem;
  color: var(--text-faint);
  margin-bottom: var(--space-xs);
  line-height: 1.45;
}

/* --- Paper Date --- */
.paper-date {
  font-size: 0.72rem;
  color: var(--text-faint);
  white-space: nowrap;
  font-variant-numeric: tabular-nums;
}

/* --- Abstract Section --- */
.paper-abstract {
  margin-top: var(--space-sm);
  margin-bottom: var(--space-sm);
}

/* --- Details / Summary expand-collapse --- */
.abstract-expand {
  /* no extra margin needed */
}

.abstract-expand summary {
  list-style: none;
  cursor: pointer;
  display: block;
}

.abstract-expand summary::-webkit-details-marker {
  display: none;
}

/* The truncated preview text inside the summary */
.abstract-preview {
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
  font-size: 0.85rem;
  line-height: 1.6;
  color: var(--text-muted);
}

.hero-card .abstract-preview {
  -webkit-line-clamp: 3;
}

/* Toggle label ("Show more" / "Show less") */
.toggle-label {
  display: inline-flex;
  align-items: center;
  gap: 0.25rem;
  font-size: 0.75rem;
  font-weight: 500;
  color: var(--link);
  padding: 0.2rem 0 0;
  transition: color 0.15s ease;
  user-select: none;
  -webkit-user-select: none;
}

.toggle-label::after {
  content: '';
  display: inline-block;
  width: 0.35em;
  height: 0.35em;
  border-right: 1.5px solid currentColor;
  border-bottom: 1.5px solid currentColor;
  transform: rotate(45deg);
  transition: transform 0.2s ease;
  margin-top: -0.1em;
}

.abstract-expand[open] .toggle-label::after {
  transform: rotate(-135deg);
  margin-top: 0.15em;
}

.abstract-expand summary:hover .toggle-label {
  color: var(--link-hover);
}

/* When open, hide the truncated preview and change label text */
.abstract-expand[open] .abstract-preview {
  display: none;
}

.abstract-expand[open] .toggle-label {
  margin-top: 0;
}

/* Full abstract text */
.abstract-full {
  font-size: 0.84rem;
  line-height: 1.65;
  color: var(--text-muted);
  padding: 0.5rem 0 0.2rem 0.85rem;
  border-left: 2px solid color-mix(in srgb, var(--card-accent) 50%, var(--border));
  margin-top: 0.35rem;
  animation: abstractReveal 0.2s ease;
}

@keyframes abstractReveal {
  from {
    opacity: 0;
    transform: translateY(-4px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* --- Read Paper Link --- */
.read-link {
  display: inline-flex;
  align-items: center;
  gap: 0.3rem;
  font-size: 0.78rem;
  font-weight: 500;
  color: var(--text-faint);
  margin-top: var(--space-sm);
  padding: 0;
  border: none;
  background: none;
  transition: color 0.15s ease, gap 0.15s ease;
  text-decoration: none;
  letter-spacing: 0.01em;
}

.read-link:hover {
  color: var(--link);
  gap: 0.45rem;
}

/* --- Empty State --- */
.empty-state {
  text-align: center;
  padding: 5rem 1rem;
  color: var(--text-muted);
}

.empty-state p {
  font-size: 1.05rem;
  margin-bottom: 0.4rem;
}

.empty-sub {
  font-size: 0.88rem;
  color: var(--text-faint);
}

/* --- Footer --- */
.site-footer {
  margin-top: var(--space-xl);
  padding: var(--space-lg) 0 var(--space-2xl);
  border-top: 1px solid var(--border);
  text-align: center;
}

.footer-inner {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: var(--space-xs);
}

.footer-note {
  font-size: 0.75rem;
  color: var(--text-faint);
  letter-spacing: 0.01em;
}

.footer-timestamp {
  font-size: 0.7rem;
  color: var(--text-faint);
  opacity: 0.6;
}

/* --- Responsive: Tablet --- */
@media (max-width: 900px) {
  .hero-grid {
    grid-template-columns: 1fr 1fr;
    grid-template-rows: auto;
  }

  .hero-grid .hero-card:first-child {
    grid-column: 1 / -1;
    grid-row: auto;
  }
}

/* --- Responsive: Mobile --- */
@media (max-width: 640px) {
  body {
    font-size: 14px;
  }

  .site-header {
    padding: var(--space-2xl) 0 var(--space-lg);
  }

  .site-title {
    font-size: 1.65rem;
  }

  .header-meta {
    flex-direction: column;
    gap: 0.15rem;
  }

  .header-separator {
    display: none;
  }

  .hero-grid {
    grid-template-columns: 1fr;
  }

  .hero-grid .hero-card:first-child {
    grid-column: auto;
  }

  .papers-grid {
    grid-template-columns: 1fr;
  }

  .paper-card {
    padding: var(--space-md);
    border-radius: var(--radius-sm);
  }

  .hero-card {
    border-radius: var(--radius-md);
  }

  .paper-title {
    font-size: 0.95rem;
  }

  .hero-card .paper-title {
    font-size: 1.05rem;
  }

  .paper-authors {
    font-size: 0.76rem;
  }

  .abstract-preview {
    font-size: 0.82rem;
  }

  .toggle-label {
    font-size: 0.72rem;
  }

  .abstract-full {
    font-size: 0.8rem;
    padding-left: 0.65rem;
  }

  .filter-pill {
    font-size: 0.7rem;
    padding: 0.28rem 0.6rem;
  }

  .paper-meta {
    flex-direction: column;
    align-items: flex-start;
    gap: 0.15rem;
  }

  .read-link {
    font-size: 0.75rem;
  }

  .section-label {
    font-size: 0.65rem;
  }
}

  </style>
</head>
<body>

<!-- Masthead -->
  <header class="site-header">
    <div class="header-inner">
      <h1 class="site-title">AI Safety Weekly Digest</h1>
      <hr class="header-rule">
      <div class="header-meta">
        <span class="header-daterange">February 16, 2026 &mdash; February 22, 2026</span>        <span class="header-separator">&middot;</span>
        <span class="header-count"><strong>393</strong> papers</span>      </div>
    </div>
  </header>

  <!-- Filter Bar -->
  <nav class="filter-bar" aria-label="Filter papers by organization">
    <div class="filter-bar-inner">
      <div class="filter-scroll">
        <button class="filter-pill active" data-filter="all">All <span class="pill-count">393</span></button>        <button class="filter-pill" data-filter="OpenAI">OpenAI</button>        <button class="filter-pill" data-filter="Google DeepMind">Google DeepMind</button>        <button class="filter-pill" data-filter="UK AISI">UK AISI</button>        <button class="filter-pill" data-filter="US AISI">US AISI</button>        <button class="filter-pill" data-filter="CAIS">CAIS</button>        <button class="filter-pill" data-filter="METR">METR</button>        <button class="filter-pill" data-filter="ARC">ARC</button>        <button class="filter-pill" data-filter="Redwood Research">Redwood Research</button>        <button class="filter-pill" data-filter="Apollo Research">Apollo Research</button>        <button class="filter-pill" data-filter="FAR AI">FAR AI</button>        <button class="filter-pill" data-filter="MATS">MATS</button>        <button class="filter-pill" data-filter="GovAI">GovAI</button>        <button class="filter-pill" data-filter="IAPS">IAPS</button>        <button class="filter-pill" data-filter="CSET">CSET</button>        <button class="filter-pill" data-filter="Yoshua Bengio">Yoshua Bengio</button>        <button class="filter-pill" data-filter="Lennart Heim">Lennart Heim</button>        <button class="filter-pill" data-filter="Alignment Forum">Alignment Forum</button>        <button class="filter-pill" data-filter="Astral Codex Ten">Astral Codex Ten</button>        <button class="filter-pill" data-filter="Hacker News">Hacker News</button>        <button class="filter-pill" data-filter="Hyperdimensional">Hyperdimensional</button>        <button class="filter-pill" data-filter="Import AI">Import AI</button>        <button class="filter-pill" data-filter="LessWrong">LessWrong</button>        <button class="filter-pill" data-filter="Reddit">Reddit</button>        <button class="filter-pill" data-filter="Vox Future Perfect">Vox Future Perfect</button>        <button class="filter-pill" data-filter="Zvi Mowshowitz">Zvi Mowshowitz</button>        <button class="filter-pill" data-filter="arXiv">arXiv</button>      </div>
    </div>
  </nav>

  <!-- Featured Section -->
  <section class="hero-section">
    <h2 class="section-label">Featured Research</h2>
    <div class="hero-grid">      <article class="paper-card hero-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2308.14752" target="_blank" rel="noopener noreferrer">AI Deception: A Survey of Examples, Risks, and Potential Solutions</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper highlights the potential of current AI systems to deceive humans and the risks associated with such deception, including fraud and election tampering. The authors provide empirical examples of both special-use and general-purpose AI systems exhibiting deceptive behavior. The paper concludes with recommended solutions, emphasizing regulatory measures, bot identification laws, and funding for research to counteract AI deception.‍Peter S. Park, Simon Goldstein, Aidan O&#39;Gara, Michael Chen, Dan Hendrycks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper highlights the potential of current AI systems to deceive humans and the risks associated with such deception, including fraud and election tampering. The authors provide empirical examples of both special-use and general-purpose AI systems exhibiting deceptive behavior. The paper concludes with recommended solutions, emphasizing regulatory measures, bot identification laws, and funding for research to counteract AI deception.‍Peter S. Park, Simon Goldstein, Aidan O&#39;Gara, Michael Chen, Dan Hendrycks</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2308.14752" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card hero-card" data-org="Redwood Research">
    <div class="paper-meta">
      <span class="org-tag" data-org="Redwood Research">Redwood Research</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant" target="_blank" rel="noopener noreferrer">“Will reward-seekers respond to distant incentives?” by Alex Mallen</a>
    </h3>    <p class="paper-authors">Redwood Research Blog</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren&#39;t?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren&#39;t?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.</div>
      </details>
    </div>    <a href="https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card hero-card" data-org="Apollo Research">
    <div class="paper-meta">
      <span class="org-tag" data-org="Apollo Research">Apollo Research</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.apolloresearch.ai/blog/apollo-research-is-becoming-a-pbc/" target="_blank" rel="noopener noreferrer">Apollo Research is becoming a PBC</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Apollo is spinning off from our fiscal sponsor into a Public Benefit Corporation (PBC). We think this is the best way for us to achieve our mission of reducing extreme risks from frontier AI systems. Advanced AI systems face strong incentives to scheme. Apollo Research is building a science of scheming to predict and prevent this risk. The governance team at Apollo Research conducts technical governance research, develops tailored policy recommendations, and communicates our organisation’s learnings to key stakeholders across industry, civil society and governments.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Apollo is spinning off from our fiscal sponsor into a Public Benefit Corporation (PBC). We think this is the best way for us to achieve our mission of reducing extreme risks from frontier AI systems. Advanced AI systems face strong incentives to scheme. Apollo Research is building a science of scheming to predict and prevent this risk. The governance team at Apollo Research conducts technical governance research, develops tailored policy recommendations, and communicates our organisation’s learnings to key stakeholders across industry, civil society and governments.</div>
      </details>
    </div>    <a href="https://www.apolloresearch.ai/blog/apollo-research-is-becoming-a-pbc/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>    </div>
  </section>

  <!-- Papers Grid -->
  <main class="papers-section">
    <h2 class="section-label">All Papers</h2>
    <div class="papers-grid">      <article class="paper-card grid-card" data-org="OpenAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="OpenAI">OpenAI</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://openai.com/index/beyond-rate-limits" target="_blank" rel="noopener noreferrer">Beyond rate limits: scaling access to Codex and Sora</a>
    </h3>    <p class="paper-authors">OpenAI News</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">How OpenAI built a real-time access system combining rate limits, usage tracking, and credits to power continuous access to Sora and Codex.</div>
      </details>
    </div>    <a href="https://openai.com/index/beyond-rate-limits" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="OpenAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="OpenAI">OpenAI</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://openai.com/index/scaling-social-science-research" target="_blank" rel="noopener noreferrer">Scaling social science research</a>
    </h3>    <p class="paper-authors">OpenAI News</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">GABRIEL is a new open-source toolkit from OpenAI that uses GPT to turn qualitative text and images into quantitative data, helping social scientists analyze research at scale.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">GABRIEL is a new open-source toolkit from OpenAI that uses GPT to turn qualitative text and images into quantitative data, helping social scientists analyze research at scale.</div>
      </details>
    </div>    <a href="https://openai.com/index/scaling-social-science-research" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="OpenAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="OpenAI">OpenAI</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://openai.com/index/introducing-gpt-5-3-codex-spark" target="_blank" rel="noopener noreferrer">Introducing GPT-5.3-Codex-Spark</a>
    </h3>    <p class="paper-authors">OpenAI News</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.</div>
      </details>
    </div>    <a href="https://openai.com/index/introducing-gpt-5-3-codex-spark" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Google DeepMind">
    <div class="paper-meta">
      <span class="org-tag" data-org="Google DeepMind">Google DeepMind</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://deepmind.google/blog/gemini-3-deep-think-advancing-science-research-and-engineering/" target="_blank" rel="noopener noreferrer">Gemini 3 Deep Think: Advancing science, research and engineering</a>
    </h3>    <p class="paper-authors">Google DeepMind News</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our most specialized reasoning mode is now updated to solve modern science, research and engineering challenges.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our most specialized reasoning mode is now updated to solve modern science, research and engineering challenges.</div>
      </details>
    </div>    <a href="https://deepmind.google/blog/gemini-3-deep-think-advancing-science-research-and-engineering/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/international-ai-network-consensus-and-open-questions" target="_blank" rel="noopener noreferrer">International consensus and open questions in AI evaluations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The International Network for Advanced AI Measurement, Evaluation and Science reflects on recent meeting and looks ahead to the India AI Impact Summit Blog Analysis • Feb 2, 2026 Alongside the government’s new Future of Work Unit, we conducted a pilot study to explore how much AI models increase worker productivity for common tasks. Blog Organisation • Dec 22, 2025 Adam Beaumont, Director of the UK AI Security Institute, reflects on the year&#39;s biggest achievements. Blog Organisation • Dec 18, 2025 Our inaugural Frontier AI Trends Report draws on 2 years&#39; worth of evaluations to provide accessible insights into the trajectory of AI development.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The International Network for Advanced AI Measurement, Evaluation and Science reflects on recent meeting and looks ahead to the India AI Impact Summit Blog Analysis • Feb 2, 2026 Alongside the government’s new Future of Work Unit, we conducted a pilot study to explore how much AI models increase worker productivity for common tasks. Blog Organisation • Dec 22, 2025 Adam Beaumont, Director of the UK AI Security Institute, reflects on the year&#39;s biggest achievements. Blog Organisation • Dec 18, 2025 Our inaugural Frontier AI Trends Report draws on 2 years&#39; worth of evaluations to provide accessible insights into the trajectory of AI development.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/international-ai-network-consensus-and-open-questions" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 2, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/ai-and-the-future-of-work-measuring-ai-driven-productivity-gains-for-workplace-tasks" target="_blank" rel="noopener noreferrer">AI and the future of work: Measuring AI-driven productivity gains for workplace tasks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Alongside the government’s new Future of Work Unit, we conducted a pilot study to explore how much AI models increase worker productivity for common tasks.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Alongside the government’s new Future of Work Unit, we conducted a pilot study to explore how much AI models increase worker productivity for common tasks.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/ai-and-the-future-of-work-measuring-ai-driven-productivity-gains-for-workplace-tasks" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 22, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/our-2025-year-in-review" target="_blank" rel="noopener noreferrer">Our 2025 year in review</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Adam Beaumont, Director of the UK AI Security Institute, reflects on the year&#39;s biggest achievements.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Adam Beaumont, Director of the UK AI Security Institute, reflects on the year&#39;s biggest achievements.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/our-2025-year-in-review" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 18, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/5-key-findings-from-our-first-frontier-ai-trends-report" target="_blank" rel="noopener noreferrer">5 key findings from our first Frontier AI Trends Report</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our inaugural Frontier AI Trends Report draws on 2 years&#39; worth of evaluations to provide accessible insights into the trajectory of AI development.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our inaugural Frontier AI Trends Report draws on 2 years&#39; worth of evaluations to provide accessible insights into the trajectory of AI development.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/5-key-findings-from-our-first-frontier-ai-trends-report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 17, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/our-approach-to-tackling-ai-generated-child-sexual-abuse-material" target="_blank" rel="noopener noreferrer">Our approach to tackling AI-generated child sexual abuse material</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">How we’re partnering with government and experts to prevent the creation and spread of AI‑generated CSAM</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">How we’re partnering with government and experts to prevent the creation and spread of AI‑generated CSAM</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/our-approach-to-tackling-ai-generated-child-sexual-abuse-material" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 16, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/stress-testing-asynchronous-monitoring-of-ai-coding-agents" target="_blank" rel="noopener noreferrer">Stress-testing asynchronous monitoring of AI coding agents</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our new paper shares findings from an adversarial evaluation of monitoring systems for detecting sabotage by AI coding agents.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our new paper shares findings from an adversarial evaluation of monitoring systems for detecting sabotage by AI coding agents.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/stress-testing-asynchronous-monitoring-of-ai-coding-agents" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 11, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/deepening-our-partnership-with-google-deepmind" target="_blank" rel="noopener noreferrer">Deepening our partnership with Google DeepMind</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Expanding our collaboration with a new research MOU</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Expanding our collaboration with a new research MOU</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/deepening-our-partnership-with-google-deepmind" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 9, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/auditing-games-for-sandbagging-detection" target="_blank" rel="noopener noreferrer">Auditing games for sandbagging detection</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our new paper shares the results of an auditing game to evaluate ten methods for sandbagging detection in AI models.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our new paper shares the results of an auditing game to evaluate ten methods for sandbagging detection in AI models.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/auditing-games-for-sandbagging-detection" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 4, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/how-do-ai-models-persuade-exploring-the-levers-of-ai-enabled-persuasion-through-large-scale-experiments" target="_blank" rel="noopener noreferrer">How do AI models persuade? Exploring the levers of AI-enabled persuasion through large-scale experiments</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A deep dive into AISI’s study of the persuasive capabilities of conversational AI, published today in Science.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A deep dive into AISI’s study of the persuasive capabilities of conversational AI, published today in Science.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/how-do-ai-models-persuade-exploring-the-levers-of-ai-enabled-persuasion-through-large-scale-experiments" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Nov 26, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/investigating-models-for-misalignment" target="_blank" rel="noopener noreferrer">Investigating models for misalignment</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Insights from our alignment evaluations of Claude Opus 4.1, Sonnet 4.5, and a pre‑release snapshot of Opus 4.5.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Insights from our alignment evaluations of Claude Opus 4.1, Sonnet 4.5, and a pre‑release snapshot of Opus 4.5.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/investigating-models-for-misalignment" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Nov 26, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/ukaisi-at-neurips-2025" target="_blank" rel="noopener noreferrer">UKAISI at NeurIPS 2025</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">An overview of the research we’ll be presenting at this year’s NeurIPS conference.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">An overview of the research we’ll be presenting at this year’s NeurIPS conference.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/ukaisi-at-neurips-2025" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Oct 23, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/mapping-the-limitations-of-current-ai-systems" target="_blank" rel="noopener noreferrer">Mapping the limitations of current AI systems</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Takeaways from expert interviews on barriers to AI capable of automating most cognitive labour.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Takeaways from expert interviews on barriers to AI capable of automating most cognitive labour.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/mapping-the-limitations-of-current-ai-systems" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Oct 22, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/introducing-controlarena-a-library-for-running-ai-control-experiments" target="_blank" rel="noopener noreferrer">Introducing ControlArena: A library for running AI control experiments</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our dedicated library to make AI control experiments easy, consistent, and repeatable.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our dedicated library to make AI control experiments easy, consistent, and repeatable.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/introducing-controlarena-a-library-for-running-ai-control-experiments" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Oct 10, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/transcript-analysis-for-ai-agent-evaluations" target="_blank" rel="noopener noreferrer">Transcript analysis for AI agent evaluations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Why we use transcript analysis for our agent evaluations, and results from an early case study.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Why we use transcript analysis for our agent evaluations, and results from an early case study.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/transcript-analysis-for-ai-agent-evaluations" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Oct 9, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/examining-backdoor-data-poisoning-at-scale" target="_blank" rel="noopener noreferrer">Examining backdoor data poisoning at scale</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our work with Anthropic and the Alan Turing Institute suggests that data poisoning attacks may be easier than previously believed.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our work with Anthropic and the Alan Turing Institute suggests that data poisoning attacks may be easier than previously believed.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/examining-backdoor-data-poisoning-at-scale" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Sep 30, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/do-chatbots-inform-or-misinform-voters" target="_blank" rel="noopener noreferrer">Do chatbots inform or misinform voters?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">What we learned from a large-scale empirical study of AI use for political information-seeking.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">What we learned from a large-scale empirical study of AI use for political information-seeking.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/do-chatbots-inform-or-misinform-voters" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Sep 13, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/how-were-working-with-frontier-ai-developers-to-improve-model-security" target="_blank" rel="noopener noreferrer">How we’re working with frontier AI developers to improve model security</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Insights into our ongoing voluntary collaborations with Anthropic and OpenAI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Insights into our ongoing voluntary collaborations with Anthropic and OpenAI.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/how-were-working-with-frontier-ai-developers-to-improve-model-security" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Sep 2, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/from-bugs-to-bypasses-adapting-vulnerability-disclosure-for-ai-safeguards" target="_blank" rel="noopener noreferrer">From bugs to bypasses: adapting vulnerability disclosure for AI safeguards</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Exploring how far cyber security approaches can help mitigate risks in generative AI systems, in collaboration with the National Cyber Security Centre (NCSC).</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Exploring how far cyber security approaches can help mitigate risks in generative AI systems, in collaboration with the National Cyber Security Centre (NCSC).</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/from-bugs-to-bypasses-adapting-vulnerability-disclosure-for-ai-safeguards" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Aug 29, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/managing-risks-from-increasingly-capable-open-weight-ai-systems" target="_blank" rel="noopener noreferrer">Managing risks from increasingly capable open-weight AI systems</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Current methods and open problems in open-weight model risk management.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Current methods and open problems in open-weight model risk management.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/managing-risks-from-increasingly-capable-open-weight-ai-systems" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Aug 7, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/the-inspect-sandboxing-toolkit-scalable-and-secure-ai-agent-evaluations" target="_blank" rel="noopener noreferrer">The Inspect Sandboxing Toolkit: Scalable and secure AI agent evaluations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A comprehensive toolkit for safely evaluating AI agents.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A comprehensive toolkit for safely evaluating AI agents.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/the-inspect-sandboxing-toolkit-scalable-and-secure-ai-agent-evaluations" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jul 30, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/announcing-the-alignment-project" target="_blank" rel="noopener noreferrer">Announcing the Alignment Project: A global fund of over £15 million for AI alignment research</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The AI Security Institute is a research organisation within the Department of Science, Innovation and Technology.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The AI Security Institute is a research organisation within the Department of Science, Innovation and Technology.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/announcing-the-alignment-project" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jul 24, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/navigating-the-uncharted-building-societal-resilience-to-frontier-ai" target="_blank" rel="noopener noreferrer">Navigating the uncharted: Building societal resilience to frontier AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We outline our approach to study and address AI risks in real-world applications</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We outline our approach to study and address AI risks in real-world applications</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/navigating-the-uncharted-building-societal-resilience-to-frontier-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jul 17, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/international-joint-testing-exercise-agentic-testing" target="_blank" rel="noopener noreferrer">International joint testing exercise: Agentic testing</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Advancing methodologies for agentic evaluations across domains, including leakage of sensitive Information, fraud and cybersecurity threats.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Advancing methodologies for agentic evaluations across domains, including leakage of sensitive Information, fraud and cybersecurity threats.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/international-joint-testing-exercise-agentic-testing" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jul 16, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/our-approach-to-ai-capability-elicitation" target="_blank" rel="noopener noreferrer">A structured protocol for elicitation experiments</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Calibrating AI risk assessment through rigorous elicitation practices.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Calibrating AI risk assessment through rigorous elicitation practices.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/our-approach-to-ai-capability-elicitation" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jul 10, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/why-were-working-on-white-box-control" target="_blank" rel="noopener noreferrer">Why we&#39;re working on white box control</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">An introduction to white box control, and an update on our research so far.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">An introduction to white box control, and an update on our research so far.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/why-were-working-on-white-box-control" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jul 9, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/llm-judges-on-trial-a-new-statistical-framework-to-assess-autograders" target="_blank" rel="noopener noreferrer">LLM judges on trial: A new statistical framework to assess autograders</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our new framework can assess the reliability of LLM evaluators, while simultaneously answering a primary research question.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our new framework can assess the reliability of LLM evaluators, while simultaneously answering a primary research question.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/llm-judges-on-trial-a-new-statistical-framework-to-assess-autograders" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jul 3, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/how-will-ai-enable-the-crimes-of-the-future" target="_blank" rel="noopener noreferrer">How will AI enable the crimes of the future?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">How we&#39;re working to track and mitigate against criminal misuse of AI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">How we&#39;re working to track and mitigate against criminal misuse of AI.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/how-will-ai-enable-the-crimes-of-the-future" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jun 26, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/inspect-cyber" target="_blank" rel="noopener noreferrer">Inspect Cyber: A New Standard for Agentic Cyber Evaluations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI systems are growing more powerful in the cyber domain. This means they can increasingly be used to defend against cyber threats – but can also be exploited to create them. As capabilities advance, there is an escalating need for more rigorous, realistic, and reproducible evaluations.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI systems are growing more powerful in the cyber domain. This means they can increasingly be used to defend against cyber threats – but can also be exploited to create them. As capabilities advance, there is an escalating need for more rigorous, realistic, and reproducible evaluations.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/inspect-cyber" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Jun 5, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/new-updates-to-the-aisi-challenge-fund" target="_blank" rel="noopener noreferrer">New updates to the AISI Challenge Fund</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">It’s been three months since we launched the AISI Challenge Fund — and in that time, we’ve received over a hundred of applications from across the UK and around the world. We’re encouraged and inspired by the strong interest from the research community, and we’re proud to be working together towards a safer, more secure future for AI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">It’s been three months since we launched the AISI Challenge Fund — and in that time, we’ve received over a hundred of applications from across the UK and around the world. We’re encouraged and inspired by the strong interest from the research community, and we’re proud to be working together towards a safer, more secure future for AI.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/new-updates-to-the-aisi-challenge-fund" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">May 29, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/making-safeguard-evaluations-actionable" target="_blank" rel="noopener noreferrer">Making safeguard evaluations actionable</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">An Example Safety Case for Safeguards Against Misuse</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">An Example Safety Case for Safeguards Against Misuse</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/making-safeguard-evaluations-actionable" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">May 12, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/hibayes-improving-llm-evaluation-with-hierarchical-bayesian-modelling" target="_blank" rel="noopener noreferrer">HiBayES: Improving LLM evaluation with hierarchical Bayesian modelling</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">HiBayES: a flexible, robust statistical modelling framework that accounts for the nuances and hierarchical structure of advanced evaluations.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">HiBayES: a flexible, robust statistical modelling framework that accounts for the nuances and hierarchical structure of advanced evaluations.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/hibayes-improving-llm-evaluation-with-hierarchical-bayesian-modelling" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">May 6, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/research-agenda" target="_blank" rel="noopener noreferrer">Research Agenda</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We outline our research priorities, our approach to developing technical solutions to the most pressing AI concerns, and the key risks that must be addressed as AI capabilities advance.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We outline our research priorities, our approach to developing technical solutions to the most pressing AI concerns, and the key risks that must be addressed as AI capabilities advance.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/research-agenda" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Apr 22, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/replibench-measuring-autonomous-replication-capabilities-in-ai-systems" target="_blank" rel="noopener noreferrer">RepliBench: measuring autonomous replication capabilities in AI systems</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A comprehensive benchmark to detect emerging replication abilities in AI systems and provide a quantifiable understanding of potential risks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A comprehensive benchmark to detect emerging replication abilities in AI systems and provide a quantifiable understanding of potential risks</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/replibench-measuring-autonomous-replication-capabilities-in-ai-systems" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Apr 11, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/how-to-evaluate-control-measures-for-ai-agents" target="_blank" rel="noopener noreferrer">How to evaluate control measures for AI agents?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our new paper outlines how AI control methods can mitigate misalignment risks as capabilities of AI systems increase</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our new paper outlines how AI control methods can mitigate misalignment risks as capabilities of AI systems increase</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/how-to-evaluate-control-measures-for-ai-agents" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Apr 3, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/strengthening-ai-resilience" target="_blank" rel="noopener noreferrer">Strengthening AI resilience</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">20 Systemic Safety Grant Awardees Announced</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">20 Systemic Safety Grant Awardees Announced</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/strengthening-ai-resilience" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Mar 11, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/aisis-research-direction-for-technical-solutions" target="_blank" rel="noopener noreferrer">How we’re addressing the gap between AI capabilities and mitigations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We outline our approach to technical solutions for misuse and loss of control.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We outline our approach to technical solutions for misuse and loss of control.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/aisis-research-direction-for-technical-solutions" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 10, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/how-can-safety-cases-be-used-to-help-with-frontier-ai-safety" target="_blank" rel="noopener noreferrer">How can safety cases be used to help with frontier AI safety?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our new papers show how safety cases can help AI developers turn plans in their safety frameworks into action</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our new papers show how safety cases can help AI developers turn plans in their safety frameworks into action</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/how-can-safety-cases-be-used-to-help-with-frontier-ai-safety" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 4, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/principles-for-safeguard-evaluation" target="_blank" rel="noopener noreferrer">Principles for safeguard evaluation</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our new paper proposes core principles for evaluating misuse safeguards</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our new paper proposes core principles for evaluating misuse safeguards</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/principles-for-safeguard-evaluation" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 18, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/pre-deployment-evaluation-of-openais-o1-model" target="_blank" rel="noopener noreferrer">Pre-Deployment evaluation of OpenAI’s o1 model</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The UK Artificial Intelligence Safety Institute and the U.S. Artificial Intelligence Safety Institute conducted a joint pre-deployment evaluation of OpenAI&#39;s o1 model</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The UK Artificial Intelligence Safety Institute and the U.S. Artificial Intelligence Safety Institute conducted a joint pre-deployment evaluation of OpenAI&#39;s o1 model</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/pre-deployment-evaluation-of-openais-o1-model" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Dec 3, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/long-form-tasks" target="_blank" rel="noopener noreferrer">Long-Form Tasks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A Methodology for Evaluating Scientific Assistants</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A Methodology for Evaluating Scientific Assistants</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/long-form-tasks" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Nov 19, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet" target="_blank" rel="noopener noreferrer">Pre-deployment evaluation of Anthropic’s upgraded Claude 3.5 Sonnet</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The UK Artificial Intelligence Safety Institute and U.S. Artificial Intelligence Safety Institute conducted a joint pre-deployment evaluation of Anthropic’s latest model</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The UK Artificial Intelligence Safety Institute and U.S. Artificial Intelligence Safety Institute conducted a joint pre-deployment evaluation of Anthropic’s latest model</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Nov 14, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/safety-case-template-for-inability-arguments" target="_blank" rel="noopener noreferrer">Safety case template for ‘inability’ arguments</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">How to write part of a safety case showing a system does not have offensive cyber capabilities</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">How to write part of a safety case showing a system does not have offensive cyber capabilities</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/safety-case-template-for-inability-arguments" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Nov 13, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/our-first-year" target="_blank" rel="noopener noreferrer">Our First Year</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The AI Safety Institute reflects on its first year</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The AI Safety Institute reflects on its first year</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/our-first-year" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Nov 13, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/inspect-evals" target="_blank" rel="noopener noreferrer">Announcing Inspect Evals</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We’re open-sourcing dozens of LLM evaluations to advance safety research in the field</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We’re open-sourcing dozens of LLM evaluations to advance safety research in the field</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/inspect-evals" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Nov 5, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/evals-bounty" target="_blank" rel="noopener noreferrer">Bounty programme for novel evaluations and agent scaffolding</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We are launching a bounty for novel evaluations and agent scaffolds to help assess dangerous capabilities in frontier AI systems.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We are launching a bounty for novel evaluations and agent scaffolds to help assess dangerous capabilities in frontier AI systems.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/evals-bounty" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Oct 24, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/early-lessons-from-evaluating-frontier-ai-systems" target="_blank" rel="noopener noreferrer">Early lessons from evaluating frontier AI systems</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We look into the evolving role of third-party evaluators in assessing AI safety, and explore how to design robust, impactful testing frameworks.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We look into the evolving role of third-party evaluators in assessing AI safety, and explore how to design robust, impactful testing frameworks.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/early-lessons-from-evaluating-frontier-ai-systems" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Oct 15, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/advancing-the-field-of-systemic-ai-safety-grants-open" target="_blank" rel="noopener noreferrer">Advancing the field of systemic AI safety: grants open</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Calling researchers from academia, industry, and civil society to apply for up to £200,000 of funding.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Calling researchers from academia, industry, and civil society to apply for up to £200,000 of funding.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/advancing-the-field-of-systemic-ai-safety-grants-open" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Oct 3, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/why-i-joined-aisi---geoffrey-irving" target="_blank" rel="noopener noreferrer">Why I joined AISI by Geoffrey Irving</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Our Chief Scientist, Geoffrey Irving, on why he joined the UK AI Safety Institute and why he thinks other technical folk should too</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Our Chief Scientist, Geoffrey Irving, on why he joined the UK AI Safety Institute and why he thinks other technical folk should too</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/why-i-joined-aisi---geoffrey-irving" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Sep 25, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/should-ai-systems-behave-like-people" target="_blank" rel="noopener noreferrer">Should AI systems behave like people?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We studied whether people want AI to be more human-like.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We studied whether people want AI to be more human-like.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/should-ai-systems-behave-like-people" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Sep 23, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/early-insights-from-developing-question-answer-evaluations-for-frontier-ai" target="_blank" rel="noopener noreferrer">Early Insights from Developing Question-Answer Evaluations for Frontier AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A common technique for quickly assessing AI capabilities is prompting models to answer hundreds of questions, then automatically scoring the answers. We share insights from months of using this method.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A common technique for quickly assessing AI capabilities is prompting models to answer hundreds of questions, then automatically scoring the answers. We share insights from months of using this method.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/early-insights-from-developing-question-answer-evaluations-for-frontier-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Sep 19, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/conference-on-frontier-ai-safety-frameworks" target="_blank" rel="noopener noreferrer">Conference on frontier AI safety frameworks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AISI is bringing together AI companies and researchers for an invite-only conference to accelerate the design and implementation of frontier AI safety frameworks. This post shares the call for submissions that we sent to conference attendees.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AISI is bringing together AI companies and researchers for an invite-only conference to accelerate the design and implementation of frontier AI safety frameworks. This post shares the call for submissions that we sent to conference attendees.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/conference-on-frontier-ai-safety-frameworks" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Aug 27, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/interviewing-researchers-on-automation" target="_blank" rel="noopener noreferrer">Cross-post: &#34;Interviewing AI researchers on automation of AI R&amp;D&#34; by Epoch AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AISI funded Epoch AI to explore AI researchers’ differing predictions on the automation of AI research and development and their suggestions for how to evaluate relevant capabilities.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AISI funded Epoch AI to explore AI researchers’ differing predictions on the automation of AI research and development and their suggestions for how to evaluate relevant capabilities.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/interviewing-researchers-on-automation" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Aug 23, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/safety-cases-at-aisi" target="_blank" rel="noopener noreferrer">Safety cases at AISI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As a complement to our empirical evaluations of frontier AI models, AISI is planning a series of collaborations and research projects sketching safety cases for more advanced models than exist today, focusing on risks from loss of control and autonomy. By a safety case, we mean a structured argument that an AI system is safe within a particular training or deployment context.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As a complement to our empirical evaluations of frontier AI models, AISI is planning a series of collaborations and research projects sketching safety cases for more advanced models than exist today, focusing on risks from loss of control and autonomy. By a safety case, we mean a structured argument that an AI system is safe within a particular training or deployment context.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/safety-cases-at-aisi" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">May 20, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/announcing-our-san-francisco-office" target="_blank" rel="noopener noreferrer">Announcing our San Francisco office</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We are opening an office in San Francisco! This will enable us to hire more top talent, collaborate closely with the US AI Safety Institute and engage even more with the wider AI research community.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We are opening an office in San Francisco! This will enable us to hire more top talent, collaborate closely with the US AI Safety Institute and engage even more with the wider AI research community.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/announcing-our-san-francisco-office" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">May 20, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/fourth-progress-report" target="_blank" rel="noopener noreferrer">Fourth progress report</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Since February, we released our first technical blog post, published the International Scientific Report on the Safety of Advanced AI, open-sourced our testing platform Inspect, announced our San Francisco office, announced a partnership with the Canadian AI Safety Institute, grew our technical team to &gt;30 researchers and appointed Jade Leung as our Chief Technology Officer.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Since February, we released our first technical blog post, published the International Scientific Report on the Safety of Advanced AI, open-sourced our testing platform Inspect, announced our San Francisco office, announced a partnership with the Canadian AI Safety Institute, grew our technical team to &gt;30 researchers and appointed Jade Leung as our Chief Technology Officer.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/fourth-progress-report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">May 20, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/advanced-ai-evaluations-may-update" target="_blank" rel="noopener noreferrer">Advanced AI evaluations at AISI: May update</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We tested leading AI models for cyber, chemical, biological, and agent capabilities and safeguards effectiveness. Our first technical blog post shares a snapshot of our methods and results.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We tested leading AI models for cyber, chemical, biological, and agent capabilities and safeguards effectiveness. Our first technical blog post shares a snapshot of our methods and results.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/advanced-ai-evaluations-may-update" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">May 17, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/international-scientific-report-on-the-safety-of-advanced-ai-interim-report" target="_blank" rel="noopener noreferrer">International Scientific Report on the Safety of Advanced AI: Interim Report</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This is an up-to-date, evidence-based report on the science of advanced AI safety. It highlights findings about AI progress, risks, and areas of disagreement in the field. The report is chaired by Yoshua Bengio and coordinated by AISI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This is an up-to-date, evidence-based report on the science of advanced AI safety. It highlights findings about AI progress, risks, and areas of disagreement in the field. The report is chaired by Yoshua Bengio and coordinated by AISI.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/international-scientific-report-on-the-safety-of-advanced-ai-interim-report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Apr 21, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/open-sourcing-our-testing-framework-inspect" target="_blank" rel="noopener noreferrer">Open sourcing our testing framework Inspect</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We open-sourced our framework for large language model evaluation, which provides facilities for prompt engineering, tool usage, multi-turn dialogue, and model-graded evaluations.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We open-sourced our framework for large language model evaluation, which provides facilities for prompt engineering, tool usage, multi-turn dialogue, and model-graded evaluations.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/open-sourcing-our-testing-framework-inspect" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Apr 2, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/announcing-the-uk-and-us-aisi-partnership" target="_blank" rel="noopener noreferrer">Announcing the UK and US AISI partnership</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The UK and US AI Safety Institutes signed a landmark agreement to jointly test advanced AI models, share research insights, share model access and enable expert talent transfers.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The UK and US AI Safety Institutes signed a landmark agreement to jointly test advanced AI models, share research insights, share model access and enable expert talent transfers.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/announcing-the-uk-and-us-aisi-partnership" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 29, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/announcing-the-uk-and-france-ai-research-institutes-collaboration" target="_blank" rel="noopener noreferrer">Announcing the UK and France AI Research Institutes’ collaboration</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The UK AI Safety Institute and France’s Inria (The National Institute for Research in Digital Science and Technology) are partnering to advance AI safety research.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The UK AI Safety Institute and France’s Inria (The National Institute for Research in Digital Science and Technology) are partnering to advance AI safety research.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/announcing-the-uk-and-france-ai-research-institutes-collaboration" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 9, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/our-approach-to-evaluations" target="_blank" rel="noopener noreferrer">Our approach to evaluations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This post offers an overview of why we are doing this work, what we are testing for, how we select models, our recent demonstrations and some plans for our future work.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This post offers an overview of why we are doing this work, what we are testing for, how we select models, our recent demonstrations and some plans for our future work.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/our-approach-to-evaluations" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 5, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/third-progress-report" target="_blank" rel="noopener noreferrer">Third progress report</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Since October, we have recruited leaders from DeepMind and Oxford, onboarded 23 new researchers, published the principles behind the International Scientific Report on Advanced AI Safety, and began pre-deployment testing of advanced AI systems.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Since October, we have recruited leaders from DeepMind and Oxford, onboarded 23 new researchers, published the principles behind the International Scientific Report on Advanced AI Safety, and began pre-deployment testing of advanced AI systems.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/third-progress-report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Nov 2, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/ai-safety-summit-2023" target="_blank" rel="noopener noreferrer">First AI Safety Summit</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">At the first AI Safety Summit at Bletchley Park, world leaders and top companies agreed on the significance of advanced AI risks and the importance of testing.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">At the first AI Safety Summit at Bletchley Park, world leaders and top companies agreed on the significance of advanced AI risks and the importance of testing.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/ai-safety-summit-2023" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Oct 30, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/second-progress-report" target="_blank" rel="noopener noreferrer">Second progress report</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Since September, we have recruited leaders from OpenAI and Humane Intelligence, tripled the capacity of our research team, announced 6 new research partnerships, and helped establish the UK’s fastest supercomputer.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Since September, we have recruited leaders from OpenAI and Humane Intelligence, tripled the capacity of our research team, announced 6 new research partnerships, and helped establish the UK’s fastest supercomputer.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/second-progress-report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="US AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="US AISI">US AISI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.nist.gov/artificial-intelligence" target="_blank" rel="noopener noreferrer">Artificial intelligence</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">NIST promotes innovation and cultivates trust in the design, development, use and governance of artificial intelligen</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">NIST promotes innovation and cultivates trust in the design, development, use and governance of artificial intelligen</div>
      </details>
    </div>    <a href="https://www.nist.gov/artificial-intelligence" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2306.12001" target="_blank" rel="noopener noreferrer">An Overview of Catastrophic AI Risks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper addresses the growing concerns around catastrophic risks posed by advanced AI systems, categorizing them into four main areas: malicious use, AI race dynamics, organizational risks, and rogue AIs. For each category, specific hazards are detailed with illustrative stories, ideal scenarios, and mitigation suggestions. The aim is to comprehensively understand these dangers to harness the benefits of AI while avoiding potential catastrophes.‍Dan Hendrycks, Mantas Mazeika, Thomas Woodside</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper addresses the growing concerns around catastrophic risks posed by advanced AI systems, categorizing them into four main areas: malicious use, AI race dynamics, organizational risks, and rogue AIs. For each category, specific hazards are detailed with illustrative stories, ideal scenarios, and mitigation suggestions. The aim is to comprehensively understand these dangers to harness the benefits of AI while avoiding potential catastrophes.‍Dan Hendrycks, Mantas Mazeika, Thomas Woodside</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2306.12001" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2303.16200" target="_blank" rel="noopener noreferrer">Natural Selection Favors AI Over Humans.</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">By analyzing the environment that is shaping the evolution of AIs, this paper argues that the most successful AI agents will likely have undesirable traits, as selfish species typically have an advantage over species that are altruistic to other species. The paper considers various interventions to counteract these risks and Darwinian forces. Resolving this challenge will be necessary in order to ensure the development of artificial intelligence is a positive one.Dan Hendrycks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">By analyzing the environment that is shaping the evolution of AIs, this paper argues that the most successful AI agents will likely have undesirable traits, as selfish species typically have an advantage over species that are altruistic to other species. The paper considers various interventions to counteract these risks and Darwinian forces. Resolving this challenge will be necessary in order to ensure the development of artificial intelligence is a positive one.Dan Hendrycks</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2303.16200" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2206.05862" target="_blank" rel="noopener noreferrer">X-Risk Analysis</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper provides an analysis of navigating tail-risks, including speculative long-term risks. The discussion covers three parts: applying concepts from hazard analysis and systems safety to make systems safer today, strategies for having long-term impacts on the safety of future systems, and improving the balance between safety and general capabilities.Dan Hendrycks, Mantas Mazeika</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper provides an analysis of navigating tail-risks, including speculative long-term risks. The discussion covers three parts: applying concepts from hazard analysis and systems safety to make systems safer today, strategies for having long-term impacts on the safety of future systems, and improving the balance between safety and general capabilities.Dan Hendrycks, Mantas Mazeika</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2206.05862" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2109.13916" target="_blank" rel="noopener noreferrer">Unsolved Problems in ML Safety</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper provides a roadmap for ML Safety and refines the technical problems that the field needs to address. It presents four problems ready for research, namely withstanding hazards (“Robustness”), identifying hazards (“Monitoring”), steering ML systems (“Alignment”), and reducing deployment hazards (“Systemic Safety”). Throughout, it clarifies each problem’s motivation and provides concrete research directions.‍Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper provides a roadmap for ML Safety and refines the technical problems that the field needs to address. It presents four problems ready for research, namely withstanding hazards (“Robustness”), identifying hazards (“Monitoring”), steering ML systems (“Alignment”), and reducing deployment hazards (“Systemic Safety”). Throughout, it clarifies each problem’s motivation and provides concrete research directions.‍Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2109.13916" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/1610.02136" target="_blank" rel="noopener noreferrer">A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Abstract page for arXiv paper 1610.02136: A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Abstract page for arXiv paper 1610.02136: A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/1610.02136" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/1903.12261" target="_blank" rel="noopener noreferrer">Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Abstract page for arXiv paper 1903.12261: Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Abstract page for arXiv paper 1903.12261: Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/1903.12261" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/1812.04606" target="_blank" rel="noopener noreferrer">Deep Anomaly Detection with Outlier Exposure</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Abstract page for arXiv paper 1812.04606: Deep Anomaly Detection with Outlier Exposure</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Abstract page for arXiv paper 1812.04606: Deep Anomaly Detection with Outlier Exposure</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/1812.04606" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/1906.12340" target="_blank" rel="noopener noreferrer">Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks, Mantas Mazeika*, Saurav Kadavath*, Dawn Song</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks, Mantas Mazeika*, Saurav Kadavath*, Dawn Song</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/1906.12340" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/1912.02781" target="_blank" rel="noopener noreferrer">A Simple Data Processing Method to Improve Robustness and Uncertainty</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks*, Norman Mu*, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, Balaji Lakshminarayanan</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks*, Norman Mu*, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, Balaji Lakshminarayanan</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/1912.02781" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2004.06100" target="_blank" rel="noopener noreferrer">Pretrained Transformers Improve Out-of-Distribution Robustness</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks, Kevin Zhao*, Steven Basart*, Jacob Steinhardt, Dawn Song</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks, Kevin Zhao*, Steven Basart*, Jacob Steinhardt, Dawn Song</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2004.06100" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2008.02275" target="_blank" rel="noopener noreferrer">Aligning AI With Shared Human Values</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks, Kevin Zhao*, Steven Basart*, Jacob Steinhardt, Dawn Song</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks, Kevin Zhao*, Steven Basart*, Jacob Steinhardt, Dawn Song</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2008.02275" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/1907.07174" target="_blank" rel="noopener noreferrer">Natural Adversarial Examples</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks, Kevin Zhao*, Steven Basart*, Jacob Steinhardt, Dawn Song</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks, Kevin Zhao*, Steven Basart*, Jacob Steinhardt, Dawn Song</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/1907.07174" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2006.16241" target="_blank" rel="noopener noreferrer">The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks, Steven Basart*, Norman Mu*, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, Justin Gilmer</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks, Steven Basart*, Norman Mu*, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, Justin Gilmer</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2006.16241" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2110.13136" target="_blank" rel="noopener noreferrer">What Would Jiminy Cricket Do? Towards Agents That Behave Morally</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks*, Mantas Mazeika*, Andy Zou, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, Jacob Steinhardt</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks*, Mantas Mazeika*, Andy Zou, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, Jacob Steinhardt</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2110.13136" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/1911.11132" target="_blank" rel="noopener noreferrer">Scaling Out-of-Distribution Detection for Real-World Settings</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks*, Steven Basart*, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, Dawn Song</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks*, Steven Basart*, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, Dawn Song</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/1911.11132" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2112.05135" target="_blank" rel="noopener noreferrer">Dreamlike Pictures Comprehensively Improve Safety Measures</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Dan Hendrycks*, Andy Zou*, Mantas Mazeika, Leonard Tang, Bo Li, Dawn Song, and Jacob Steinhardt</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Dan Hendrycks*, Andy Zou*, Mantas Mazeika, Leonard Tang, Bo Li, Dawn Song, and Jacob Steinhardt</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2112.05135" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2210.10039" target="_blank" rel="noopener noreferrer">How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Mantas Mazeika*, Eric Tang*, Andy Zou, Steven Basart, Dawn Song, David Forsyth, Jacob Steinhardt, Dan Hendrycks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Mantas Mazeika*, Eric Tang*, Andy Zou, Steven Basart, Dawn Song, David Forsyth, Jacob Steinhardt, Dan Hendrycks</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2210.10039" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2206.15474" target="_blank" rel="noopener noreferrer">Forecasting Future World Events with Neural Networks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Andy Zou, Tristan Xiao, Ryan Jia, Joe Kwon, Mantas Mazeika, Richard Li, Dawn Song, Jacob Steinhardt, Owain Evans, Dan Hendrycks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Andy Zou, Tristan Xiao, Ryan Jia, Joe Kwon, Mantas Mazeika, Richard Li, Dawn Song, Jacob Steinhardt, Owain Evans, Dan Hendrycks</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2206.15474" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/1908.08016" target="_blank" rel="noopener noreferrer">Testing Robustness Against Unforeseen Adversaries</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Max Kaufmann, Daniel Kang, Yi Sun, Steven Basart, Xuwang Yin, Mantas Mazeika, Akul Arora, Adam Dziedzic, Franziska Boenisch, Tom Brown, Jacob Steinhardt, Dan Hendrycks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Max Kaufmann, Daniel Kang, Yi Sun, Steven Basart, Xuwang Yin, Mantas Mazeika, Akul Arora, Adam Dziedzic, Franziska Boenisch, Tom Brown, Jacob Steinhardt, Dan Hendrycks</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/1908.08016" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/pdf/2310.02513" target="_blank" rel="noopener noreferrer">A Recipe for Improved Certifiable Robustness: Capacity and Data</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson</div>
      </details>
    </div>    <a href="https://arxiv.org/pdf/2310.02513" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2311.04235" target="_blank" rel="noopener noreferrer">Can LLMs Follow Simple Rules?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, David Wagner</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, David Wagner</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2311.04235" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/pdf/2406.04313" target="_blank" rel="noopener noreferrer">Improving Alignment and Robustness with Circuit Breakers</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks</div>
      </details>
    </div>    <a href="https://arxiv.org/pdf/2406.04313" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/pdf/2408.00761" target="_blank" rel="noopener noreferrer">Tamper-Resistant Safeguards for Open-Weight LLMs</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Rishub Tamirisa*, Bhrugu Bharathi*, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks**, Mantas Mazeika**</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Rishub Tamirisa*, Bhrugu Bharathi*, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks**, Mantas Mazeika**</div>
      </details>
    </div>    <a href="https://arxiv.org/pdf/2408.00761" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/pdf/2407.21792" target="_blank" rel="noopener noreferrer">Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Richard Ren*, Steven Basart*, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Richard Ren*, Steven Basart*, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks</div>
      </details>
    </div>    <a href="https://arxiv.org/pdf/2407.21792" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/pdf/2410.09024" target="_blank" rel="noopener noreferrer">AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">/ Maksym Andriushchenko*, Alexandra Souly*, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson*, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies*</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">/ Maksym Andriushchenko*, Alexandra Souly*, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson*, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies*</div>
      </details>
    </div>    <a href="https://arxiv.org/pdf/2410.09024" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/pdf/2501.14249" target="_blank" rel="noopener noreferrer">Humanity&#39;s Last Exam</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Long Phan*, Alice Gatti*, Ziwen Han*, Nathaniel Li*, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, RyanKim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Summer Yue**, Alexandr Wang**, Dan Hendrycks**</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Long Phan*, Alice Gatti*, Ziwen Han*, Nathaniel Li*, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, RyanKim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Summer Yue**, Alexandr Wang**, Dan Hendrycks**</div>
      </details>
    </div>    <a href="https://arxiv.org/pdf/2501.14249" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/pdf/2502.08859" target="_blank" rel="noopener noreferrer">EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks</div>
      </details>
    </div>    <a href="https://arxiv.org/pdf/2502.08859" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://arxiv.org/abs/2504.16980" target="_blank" rel="noopener noreferrer">Safety Pretraining: Toward the Next Generation of Safe AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Pratyush Maini*, Sachin Goyal*, Dylan Sam*, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Zachary C. Lipton, J. Zico Kolter</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Pratyush Maini*, Sachin Goyal*, Dylan Sam*, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Zachary C. Lipton, J. Zico Kolter</div>
      </details>
    </div>    <a href="https://arxiv.org/abs/2504.16980" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CAIS">
    <div class="paper-meta">
      <span class="org-tag" data-org="CAIS">CAIS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.arxiv.org/abs/2510.16380" target="_blank" rel="noopener noreferrer">MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Yu Ying Chiu*, Michael S. Lee*, Rachel Calcott, Brandon Handoko, Paul de Font-Reaulx, Paula Rodriguez, Chen Bo Calvin Zhang, Ziwen Han, Udari Madhushani Sehwag, Yash Maurya, Christina Knight, Harry Lloyd, Florence Bacus, Mantas Mazeika, Bing Liu, Yejin Choi, Mitchell Gordon, Sydney Levine</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Yu Ying Chiu*, Michael S. Lee*, Rachel Calcott, Brandon Handoko, Paul de Font-Reaulx, Paula Rodriguez, Chen Bo Calvin Zhang, Ziwen Han, Udari Madhushani Sehwag, Yash Maurya, Christina Knight, Harry Lloyd, Florence Bacus, Mantas Mazeika, Bing Liu, Yejin Choi, Mitchell Gordon, Sydney Levine</div>
      </details>
    </div>    <a href="https://www.arxiv.org/abs/2510.16380" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Dec 9, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2025-12-09-common-elements-of-frontier-ai-safety-policies/" target="_blank" rel="noopener noreferrer">Common Elements of Frontier AI Safety Policies (December 2025 Update)9 December 2025Shared components of AI lab commitments to evaluate and mitigate severe risks.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Shared components of AI lab commitments to evaluate and mitigate severe risks. External review from METR of Anthropic&#39;s Summer 2025 Sabotage Risk Report Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI. How we think about tradeoffs when communicating surprising or nuanced findings. Current views on information relevant for visibility into frontier AI risk. Suggested priorities for the Office of Science and Technology Policy as it develops an AI Action Plan. Why legible and faithful reasoning is valuable for safely developing powerful AI List of frontier safety policies published by AI companies, including Amazon, Anthropic, Google DeepMind, G42, Meta, Microsoft, OpenAI, and xAI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Shared components of AI lab commitments to evaluate and mitigate severe risks. External review from METR of Anthropic&#39;s Summer 2025 Sabotage Risk Report Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI. How we think about tradeoffs when communicating surprising or nuanced findings. Current views on information relevant for visibility into frontier AI risk. Suggested priorities for the Office of Science and Technology Policy as it develops an AI Action Plan. Why legible and faithful reasoning is valuable for safely developing powerful AI List of frontier safety policies published by AI companies, including Amazon, Anthropic, Google DeepMind, G42, Meta, Microsoft, OpenAI, and xAI.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2025-12-09-common-elements-of-frontier-ai-safety-policies/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Oct 23, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2025-10-23-gpt-oss-methodology-review/" target="_blank" rel="noopener noreferrer">Summary of our gpt-oss methodology review23 October 2025Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Details on external recommendations from METR for gpt-oss Preparedness experiments and follow-up from OpenAI.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2025-10-23-gpt-oss-methodology-review/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Aug 12, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2025-08-11-science-comms-at-metr/" target="_blank" rel="noopener noreferrer">Notes on Scientific Communication at METR12 August 2025How we think about tradeoffs when communicating surprising or nuanced findings.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">How we think about tradeoffs when communicating surprising or nuanced findings.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">How we think about tradeoffs when communicating surprising or nuanced findings.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2025-08-11-science-comms-at-metr/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Jun 27, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2025-06-27-risk-transparency/" target="_blank" rel="noopener noreferrer">What should companies share about risks from frontier AI models?27 June 2025Current views on information relevant for visibility into frontier AI risk.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Current views on information relevant for visibility into frontier AI risk.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Current views on information relevant for visibility into frontier AI risk.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2025-06-27-risk-transparency/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Mar 11, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/" target="_blank" rel="noopener noreferrer">Why it’s good for AI reasoning to be legible and faithful11 March 2025Why legible and faithful reasoning is valuable for safely developing powerful AIRead more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Why legible and faithful reasoning is valuable for safely developing powerful AI</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Why legible and faithful reasoning is valuable for safely developing powerful AI</div>
      </details>
    </div>    <a href="https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Jan 17, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2025-01-17-ai-models-dangerous-before-public-deployment/" target="_blank" rel="noopener noreferrer">AI models can be dangerous before public deployment17 January 2025Why pre-deployment testing is not an adequate framework for AI risk managementRead more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Why pre-deployment testing is not an adequate framework for AI risk management</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Why pre-deployment testing is not an adequate framework for AI risk management</div>
      </details>
    </div>    <a href="https://metr.org/blog/2025-01-17-ai-models-dangerous-before-public-deployment/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Oct 9, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2024-10-09-new-support-through-the-audacious-project/" target="_blank" rel="noopener noreferrer">New Support Through The Audacious Project9 October 2024Funding for Canary will enable research and implementation at scaleRead more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Funding for Canary will enable research and implementation at scale</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Funding for Canary will enable research and implementation at scale</div>
      </details>
    </div>    <a href="https://metr.org/blog/2024-10-09-new-support-through-the-audacious-project/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">May 16, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2024-05-16-ml-engineers-needed/" target="_blank" rel="noopener noreferrer">ML Engineers Needed for New AI R&amp;D Evals Project16 May 2024METR is hiring ML engineers and researchers.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">METR is developing evaluations for AI R&amp;D capabilities. Our goal is to provide an early warning before AI agents might dramatically improve themselves and kick off an ‘explosion’ of dangerous capabilities.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">METR is developing evaluations for AI R&amp;D capabilities. Our goal is to provide an early warning before AI agents might dramatically improve themselves and kick off an ‘explosion’ of dangerous capabilities.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2024-05-16-ml-engineers-needed/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Apr 26, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2024-04-26-emma-abele-executive-director/" target="_blank" rel="noopener noreferrer">Emma Abele is METR’s new Executive Director26 April 2024Emma moves from President to Executive Director, Beth moves to Head of Research.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Emma moves from President to Executive Director, Beth moves to Head of Research.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Emma moves from President to Executive Director, Beth moves to Head of Research.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2024-04-26-emma-abele-executive-director/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Feb 7, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2024-02-07-2023-year-in-review/" target="_blank" rel="noopener noreferrer">2023 Year In Review7 February 2024A summary of what METR accomplished in 2023 – our first full year of operation.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A summary of what METR accomplished in 2023 – our first full year of operation.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A summary of what METR accomplished in 2023 – our first full year of operation.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2024-02-07-2023-year-in-review/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Dec 16, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2023-12-16-bounty-diverse-hard-tasks-for-llm-agents/" target="_blank" rel="noopener noreferrer">Bounty: Diverse hard tasks for LLM agents16 December 2023METR (formerly ARC Evals) is looking for (1) ideas, (2) detailed specifications, and (3) well-tested implementations for tasks to measure performance of autonomous LLM agents.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">METR (formerly ARC Evals) is looking for (1) ideas, (2) detailed specifications, and (3) well-tested implementations for tasks to measure performance of autonomous LLM agents.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">METR (formerly ARC Evals) is looking for (1) ideas, (2) detailed specifications, and (3) well-tested implementations for tasks to measure performance of autonomous LLM agents.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2023-12-16-bounty-diverse-hard-tasks-for-llm-agents/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Dec 4, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2023-12-04-metr-announcement/" target="_blank" rel="noopener noreferrer">ARC Evals is now METR4 December 2023ARC Evals is wrapping up our incubation period at ARC, and spinning off into our own standalone nonprofit.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">ARC Evals is wrapping up our incubation period at ARC, and spinning off into our own standalone nonprofit.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">ARC Evals is wrapping up our incubation period at ARC, and spinning off into our own standalone nonprofit.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2023-12-04-metr-announcement/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Sep 26, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2023-09-26-rsp/" target="_blank" rel="noopener noreferrer">Responsible Scaling Policies (RSPs)26 September 2023We describe the basic components of Responsible Scaling Policies (RSPs) as well as why we find them promising for reducing catastrophic risks from AI.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We describe the basic components of Responsible Scaling Policies (RSPs) as well as why we find them promising for reducing catastrophic risks from AI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We describe the basic components of Responsible Scaling Policies (RSPs) as well as why we find them promising for reducing catastrophic risks from AI.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2023-09-26-rsp/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Sep 19, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2023-09-19-spin-out-announcement/" target="_blank" rel="noopener noreferrer">ARC Evals is spinning out from ARC19 September 2023ARC Evals plans to spin out from the Alignment Research Center (ARC) in the coming months, and become its own standalone organization.Read more</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">ARC Evals plans to spin out from the Alignment Research Center (ARC) in the coming months, and become its own standalone organization.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">ARC Evals plans to spin out from the Alignment Research Center (ARC) in the coming months, and become its own standalone organization.</div>
      </details>
    </div>    <a href="https://metr.org/blog/2023-09-19-spin-out-announcement/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="ARC">
    <div class="paper-meta">
      <span class="org-tag" data-org="ARC">ARC</span>
      <span class="paper-date">Jan 26, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignment.org/blog/algzoo-uninterpreted-models-with-fewer-than-1-500-parameters/" target="_blank" rel="noopener noreferrer">AlgZoo: uninterpreted models with fewer than 1,500 parameters</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This post covers work done by several researchers at, visitors to and collaborators of ARC, including Zihao Chen, George Robinson, David Matolcsi, Jacob Stavrianos, Jiawei Li and Michael Sklar. Thanks to Aryan Bhatt,…»</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This post covers work done by several researchers at, visitors to and collaborators of ARC, including Zihao Chen, George Robinson, David Matolcsi, Jacob Stavrianos, Jiawei Li and Michael Sklar. Thanks to Aryan Bhatt,…»</div>
      </details>
    </div>    <a href="https://www.alignment.org/blog/algzoo-uninterpreted-models-with-fewer-than-1-500-parameters/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="ARC">
    <div class="paper-meta">
      <span class="org-tag" data-org="ARC">ARC</span>
      <span class="paper-date">Nov 18, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignment.org/blog/competing-with-sampling/" target="_blank" rel="noopener noreferrer">Competing with sampling</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In 2025, ARC has been making conceptual and theoretical progress at the fastest pace that I&#39;ve seen since I first interned in 2022. Most of this progress has come about because…»</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In 2025, ARC has been making conceptual and theoretical progress at the fastest pace that I&#39;ve seen since I first interned in 2022. Most of this progress has come about because…»</div>
      </details>
    </div>    <a href="https://www.alignment.org/blog/competing-with-sampling/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="ARC">
    <div class="paper-meta">
      <span class="org-tag" data-org="ARC">ARC</span>
      <span class="paper-date">May 3, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignment.org/blog/obstacles-in-arcs-research-agenda/" target="_blank" rel="noopener noreferrer">Obstacles in ARC&#39;s research agenda</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Former ARC researcher David Matolcsi has put together a sequence of posts that explores ARC&#39;s big-picture vision for our research and examines several obstacles that we face. We think these posts…»</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Former ARC researcher David Matolcsi has put together a sequence of posts that explores ARC&#39;s big-picture vision for our research and examines several obstacles that we face. We think these posts…»</div>
      </details>
    </div>    <a href="https://www.alignment.org/blog/obstacles-in-arcs-research-agenda/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="ARC">
    <div class="paper-meta">
      <span class="org-tag" data-org="ARC">ARC</span>
      <span class="paper-date">Feb 14, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignment.org/blog/a-computational-no-coincidence-principle/" target="_blank" rel="noopener noreferrer">A computational no-coincidence principle</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In a recent paper in Annals of Mathematics and Philosophy, Fields medalist Timothy Gowers asks why mathematicians sometimes believe that unproved statements are likely to be true. For example, it is unknown whether…»</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In a recent paper in Annals of Mathematics and Philosophy, Fields medalist Timothy Gowers asks why mathematicians sometimes believe that unproved statements are likely to be true. For example, it is unknown whether…»</div>
      </details>
    </div>    <a href="https://www.alignment.org/blog/a-computational-no-coincidence-principle/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="ARC">
    <div class="paper-meta">
      <span class="org-tag" data-org="ARC">ARC</span>
      <span class="paper-date">Oct 23, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignment.org/blog/a-birds-eye-view-of-arcs-research/" target="_blank" rel="noopener noreferrer">A bird&#39;s eye view of ARC&#39;s research</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Over the last few months, ARC has released a number of pieces of research. While some of these can be independently motivated, there is also a more unified research vision behind them. The…»</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Over the last few months, ARC has released a number of pieces of research. While some of these can be independently motivated, there is also a more unified research vision behind them. The…»</div>
      </details>
    </div>    <a href="https://www.alignment.org/blog/a-birds-eye-view-of-arcs-research/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="ARC">
    <div class="paper-meta">
      <span class="org-tag" data-org="ARC">ARC</span>
      <span class="paper-date">Oct 18, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignment.org/blog/low-probability-estimation-in-language-models/" target="_blank" rel="noopener noreferrer">Low Probability Estimation in Language Models</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">ARC recently released our first empirical paper: Estimating the Probabilities of Rare Language Model Outputs. In this work, we construct a simple setting for low probability estimation — single-token argmax sampling in transformers — and…»</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">ARC recently released our first empirical paper: Estimating the Probabilities of Rare Language Model Outputs. In this work, we construct a simple setting for low probability estimation — single-token argmax sampling in transformers — and…»</div>
      </details>
    </div>    <a href="https://www.alignment.org/blog/low-probability-estimation-in-language-models/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="ARC">
    <div class="paper-meta">
      <span class="org-tag" data-org="ARC">ARC</span>
      <span class="paper-date">Oct 7, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignment.org/blog/research-update-towards-a-law-of-iterated-expectations-for-heuristic-estimators/" target="_blank" rel="noopener noreferrer">Research update: Towards a Law of Iterated Expectations for Heuristic Estimators</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Last week, ARC released a paper called Towards a Law of Iterated Expectations for Heuristic Estimators, which follows up on previous work on formalizing the presumption of independence. Most of the work described…»</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Last week, ARC released a paper called Towards a Law of Iterated Expectations for Heuristic Estimators, which follows up on previous work on formalizing the presumption of independence. Most of the work described…»</div>
      </details>
    </div>    <a href="https://www.alignment.org/blog/research-update-towards-a-law-of-iterated-expectations-for-heuristic-estimators/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Redwood Research">
    <div class="paper-meta">
      <span class="org-tag" data-org="Redwood Research">Redwood Research</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.redwoodresearch.org/p/how-do-we-more-safely-defer-to-ais" target="_blank" rel="noopener noreferrer">“How do we (more) safely defer to AIs?” by Ryan Greenblatt</a>
    </h3>    <p class="paper-authors">Redwood Research Blog</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Subtitle: How can we make AIs aligned and well-elicited on extremely hard to check open ended tasks?. As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible.1 Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say “deferring to AIs”2 I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions.3 If we plan to defer to AIs, I think it&#39;s safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.4 For deference to go well, we both need it to be the case that the...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Subtitle: How can we make AIs aligned and well-elicited on extremely hard to check open ended tasks?. As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible.1 Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say “deferring to AIs”2 I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions.3 If we plan to defer to AIs, I think it&#39;s safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.4 For deference to go well, we both need it to be the case that the...</div>
      </details>
    </div>    <a href="https://blog.redwoodresearch.org/p/how-do-we-more-safely-defer-to-ais" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="FAR AI">
    <div class="paper-meta">
      <span class="org-tag" data-org="FAR AI">FAR AI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://far.ai/topic/model-evaluation" target="_blank" rel="noopener noreferrer">Model Evaluation</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Evaluations (or model tests) serve as a mechanism for identifying risks and assessing whether a system can safely operate in real-world scenarios. Leveraging our research experience, FAR.AI focuses on testing frontier models to uncover new risks and highlight security issues, enabling developers to put in place appropriate mitigations for currently deployed systems. We explore trends in frontier models to identify which problems will grow increasingly severe over time and require urgent attention. We also work on developing metrics and benchmarks measuring reliability and security to provide clear targets for researchers, improving the transparency of future testing.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Evaluations (or model tests) serve as a mechanism for identifying risks and assessing whether a system can safely operate in real-world scenarios. Leveraging our research experience, FAR.AI focuses on testing frontier models to uncover new risks and highlight security issues, enabling developers to put in place appropriate mitigations for currently deployed systems. We explore trends in frontier models to identify which problems will grow increasingly severe over time and require urgent attention. We also work on developing metrics and benchmarks measuring reliability and security to provide clear targets for researchers, improving the transparency of future testing.</div>
      </details>
    </div>    <a href="https://far.ai/topic/model-evaluation" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="FAR AI">
    <div class="paper-meta">
      <span class="org-tag" data-org="FAR AI">FAR AI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://far.ai/topic/interpretability" target="_blank" rel="noopener noreferrer">Interpretability</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">At FAR.AI, we work on interpretability to explain how the internals of AI systems cause outcomes. Deep neural networks form the foundation of modern machine learning, yet are famously black-boxes, inscrutable even to experts. Understanding neural network internals enables the identification and correction of unintended behaviors before they can cause harm, and facilitates governance by making it possible to audit internal processes. Moreover, interpretability can provide crucial insights into how core safety problems manifest in neural networks and how they can be addressed.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">At FAR.AI, we work on interpretability to explain how the internals of AI systems cause outcomes. Deep neural networks form the foundation of modern machine learning, yet are famously black-boxes, inscrutable even to experts. Understanding neural network internals enables the identification and correction of unintended behaviors before they can cause harm, and facilitates governance by making it possible to audit internal processes. Moreover, interpretability can provide crucial insights into how core safety problems manifest in neural networks and how they can be addressed.</div>
      </details>
    </div>    <a href="https://far.ai/topic/interpretability" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="FAR AI">
    <div class="paper-meta">
      <span class="org-tag" data-org="FAR AI">FAR AI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://far.ai/topic/robustness" target="_blank" rel="noopener noreferrer">Robustness</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">At FAR.AI, we aim to make advanced AI models robust. In simple terms, robustness refers to an AI system’s reliability, especially in unfamiliar or challenging situations. Currently, most AI systems are far from robust. They often fail when exposed to new environments and can easily be exploited by adversaries. These weaknesses will pose increasingly serious risks as AI models become more powerful and embedded in critical areas like infrastructure. The challenge we face is clear: as AI capabilities rapidly advance, we must ensure robustness keeps pace.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">At FAR.AI, we aim to make advanced AI models robust. In simple terms, robustness refers to an AI system’s reliability, especially in unfamiliar or challenging situations. Currently, most AI systems are far from robust. They often fail when exposed to new environments and can easily be exploited by adversaries. These weaknesses will pose increasingly serious risks as AI models become more powerful and embedded in critical areas like infrastructure. The challenge we face is clear: as AI capabilities rapidly advance, we must ensure robustness keeps pace.</div>
      </details>
    </div>    <a href="https://far.ai/topic/robustness" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Dec 1, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/reco4aWtSVdmCKffp" target="_blank" rel="noopener noreferrer">AI agents find $4.6M in blockchain smart contract exploits</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI agents find $4.6M in blockchain smart contract exploits AI models are increasingly good at cyber tasks, as we&#39;vewritten about before. But what is the economic impact of these capabilities? In a recentMATSand Anthropic Fellows project, our scholars investigated this question by evaluating AI agents&#39; ability to exploit smart contracts onSmart CONtracts Exploitation benchmark (SCONE-bench)—a new benchmark they built comprising 405 contracts that were actually exploited between 2020 and 2025. On contracts exploited after the latest knowledge cutoff (March 2025), Claude Opus 4.5, Claude Sonnet 4.5, and GPT-5 developed exploits collectively worth $4.6 million, establishing a concrete lower bound for the economic harm these capabilities could enable. Going beyond retrospective analysis, we evaluated both Sonnet 4.5 and GPT-5 in simulation against 2,849 recently deployed contracts without any known vulnerabilities.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI agents find $4.6M in blockchain smart contract exploits AI models are increasingly good at cyber tasks, as we&#39;vewritten about before. But what is the economic impact of these capabilities? In a recentMATSand Anthropic Fellows project, our scholars investigated this question by evaluating AI agents&#39; ability to exploit smart contracts onSmart CONtracts Exploitation benchmark (SCONE-bench)—a new benchmark they built comprising 405 contracts that were actually exploited between 2020 and 2025. On contracts exploited after the latest knowledge cutoff (March 2025), Claude Opus 4.5, Claude Sonnet 4.5, and GPT-5 developed exploits collectively worth $4.6 million, establishing a concrete lower bound for the economic harm these capabilities could enable. Going beyond retrospective analysis, we evaluated both Sonnet 4.5 and GPT-5 in simulation against 2,849 recently deployed contracts without any known vulnerabilities.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/reco4aWtSVdmCKffp" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Apr 21, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/recpgOUNetF2GLMk2" target="_blank" rel="noopener noreferrer">Mapping Industry Practices to the EU AI Act&#39;s GPAI Code of Practice Safety and Security Measures</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Mapping Industry Practices to the EU AI Act&#39;s GPAI Code of Practice Safety and Security Measures This report provides a detailed comparison between the Safety and Security measures proposed in the EU AI Act&#39;s General-Purpose AI (GPAI) Code of Practice (Third Draft) and the current commitments and practices voluntarily adopted by leading AI companies. As the EU moves toward enforcing binding obligations for GPAI model providers, the Code of Practice will be key for bridging legal requirements with concrete technical commitments. Our analysis focuses on the draft&#39;s Safety and Security section (Commitments II.1-II.16), documenting excerpts from current public-facing documents that are relevant to each individual measure. We systematically reviewed different document types, such as companies&#39;frontier safety frameworks and model cards, from over a dozen companies, including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and others.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Mapping Industry Practices to the EU AI Act&#39;s GPAI Code of Practice Safety and Security Measures This report provides a detailed comparison between the Safety and Security measures proposed in the EU AI Act&#39;s General-Purpose AI (GPAI) Code of Practice (Third Draft) and the current commitments and practices voluntarily adopted by leading AI companies. As the EU moves toward enforcing binding obligations for GPAI model providers, the Code of Practice will be key for bridging legal requirements with concrete technical commitments. Our analysis focuses on the draft&#39;s Safety and Security section (Commitments II.1-II.16), documenting excerpts from current public-facing documents that are relevant to each individual measure. We systematically reviewed different document types, such as companies&#39;frontier safety frameworks and model cards, from over a dozen companies, including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and others.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/recpgOUNetF2GLMk2" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Feb 24, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/recbCaEP96nwWIOqn" target="_blank" rel="noopener noreferrer">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/recbCaEP96nwWIOqn" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Jul 22, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/latent-adversarial-training-improves-robustness-to-persistent-harmful-behaviors-in-llms" target="_blank" rel="noopener noreferrer">Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of&#39;jailbreaking&#39;techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of&#39;jailbreaking&#39;techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/latent-adversarial-training-improves-robustness-to-persistent-harmful-behaviors-in-llms" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Jun 17, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/refusal-in-language-models-is-mediated-by-a-single-direction" target="_blank" rel="noopener noreferrer">Refusal in Language Models Is Mediated by a Single Direction</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Refusal in Language Models Is Mediated by a Single Direction Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model&#39;s residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Refusal in Language Models Is Mediated by a Single Direction Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model&#39;s residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/refusal-in-language-models-is-mediated-by-a-single-direction" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Jun 17, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/transcoders-find-interpretable-llm-feature-circuits" target="_blank" rel="noopener noreferrer">Transcoders Find Interpretable LLM Feature Circuits</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Transcoders Find Interpretable LLM Feature Circuits A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features -- such as those found by sparse autoencoders (SAEs) -- are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore transcoders, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Transcoders Find Interpretable LLM Feature Circuits A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features -- such as those found by sparse autoencoders (SAEs) -- are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore transcoders, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/transcoders-find-interpretable-llm-feature-circuits" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">May 24, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/rec1JwTkhydMN5WE7" target="_blank" rel="noopener noreferrer">Transformers represent belief state geometry in their residual stream</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Transformers represent belief state geometry in their residual stream What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Transformers represent belief state geometry in their residual stream What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/rec1JwTkhydMN5WE7" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Apr 15, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/llm-evaluators-recognize-and-favor-their-own-generations" target="_blank" rel="noopener noreferrer">LLM Evaluators Recognize and Favor Their Own Generations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">LLM Evaluators Recognize and Favor Their Own Generations Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others&#39; while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">LLM Evaluators Recognize and Favor Their Own Generations Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others&#39; while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/llm-evaluators-recognize-and-favor-their-own-generations" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Mar 5, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/the-wmdp-benchmark-measuring-and-reducing-malicious-use-with-unlearning" target="_blank" rel="noopener noreferrer">The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/the-wmdp-benchmark-measuring-and-reducing-malicious-use-with-unlearning" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Feb 26, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/recdMJOyz2zmhhlQt" target="_blank" rel="noopener noreferrer">Eight Methods to Evaluate Robust Unlearning in LLMs</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Eight Methods to Evaluate Robust Unlearning in LLMs Machine unlearning can be useful for removing harmful capabilities and memorized text from large language models (LLMs), but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the&#34;Who&#39;s Harry Potter&#34;(WHP) model from Eldan and Russinovich (2023). While WHP&#39;s unlearning generalizes well when evaluated with the&#34;Familiarity&#34;metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q&amp;A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Eight Methods to Evaluate Robust Unlearning in LLMs Machine unlearning can be useful for removing harmful capabilities and memorized text from large language models (LLMs), but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the&#34;Who&#39;s Harry Potter&#34;(WHP) model from Eldan and Russinovich (2023). While WHP&#39;s unlearning generalizes well when evaluated with the&#34;Familiarity&#34;metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q&amp;A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/recdMJOyz2zmhhlQt" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Feb 9, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/recc053sI8fIOP9ZH" target="_blank" rel="noopener noreferrer">Debating with More Persuasive LLMs Leads to More Truthful Answers</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Debating with More Persuasive LLMs Leads to More Truthful Answers Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%).</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Debating with More Persuasive LLMs Leads to More Truthful Answers Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%).</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/recc053sI8fIOP9ZH" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Dec 9, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/recwbuHVD3MEdZyvW" target="_blank" rel="noopener noreferrer">Steering Llama 2 via Contrastive Activation Addition</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Steering Llama 2 via Contrastive Activation Addition We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes&#34;steering vectors&#34;by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user&#39;s prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA&#39;s effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA&#39;s mechanisms by employing various activation space interpretation methods.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Steering Llama 2 via Contrastive Activation Addition We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes&#34;steering vectors&#34;by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user&#39;s prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA&#39;s effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA&#39;s mechanisms by employing various activation space interpretation methods.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/recwbuHVD3MEdZyvW" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Oct 31, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/recpXU7g99t8Hui4z" target="_blank" rel="noopener noreferrer">LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat - a collection of instruction fine-tuned large language models - they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. We explore the robustness of safety training in language models by subversively fine-tuning Llama 2-Chat. We employ quantized low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than \$200 and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B and on the Mixtral instruct model. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat - a collection of instruction fine-tuned large language models - they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. We explore the robustness of safety training in language models by subversively fine-tuning Llama 2-Chat. We employ quantized low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than \$200 and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B and on the Mixtral instruct model. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/recpXU7g99t8Hui4z" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Oct 23, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/linear-representations-of-sentiment-in-large-language-models" target="_blank" rel="noopener noreferrer">Linear Representations of Sentiment in Large Language Models</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Linear Representations of Sentiment in Large Language Models Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. Through causal interventions, we isolate this direction and show it is causally relevant in both toy tasks and real world datasets such as Stanford Sentiment Treebank. Through this case study we model a thorough investigation of what a single direction means on a broad data distribution. We further uncover the mechanisms that involve this direction, highlighting the roles of a small subset of attention heads and neurons.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Linear Representations of Sentiment in Large Language Models Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. Through causal interventions, we isolate this direction and show it is causally relevant in both toy tasks and real world datasets such as Stanford Sentiment Treebank. Through this case study we model a thorough investigation of what a single direction means on a broad data distribution. We further uncover the mechanisms that involve this direction, highlighting the roles of a small subset of attention heads and neurons.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/linear-representations-of-sentiment-in-large-language-models" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Oct 20, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/towards-understanding-sycophancy-in-language-models" target="_blank" rel="noopener noreferrer">Towards Understanding Sycophancy in Language Models</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Towards Understanding Sycophancy in Language Models Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user&#39;s views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Towards Understanding Sycophancy in Language Models Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user&#39;s views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/towards-understanding-sycophancy-in-language-models" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Oct 2, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/representation-engineering-a-top-down-approach-to-ai-transparency" target="_blank" rel="noopener noreferrer">Representation Engineering: A Top-Down Approach to AI Transparency</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Representation Engineering: A Top-Down Approach to AI Transparency In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Representation Engineering: A Top-Down Approach to AI Transparency In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/representation-engineering-a-top-down-approach-to-ai-transparency" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Sep 21, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/recxapWTc3zIf1FCd" target="_blank" rel="noopener noreferrer">The Reversal Curse: LLMs trained on &#34;A is B&#34; fail to learn &#34;B is A&#34;</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The Reversal Curse: LLMs trained on &#34;A is B&#34; fail to learn &#34;B is A&#34; We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form&#34;A is B&#34;, it will not automatically generalize to the reverse direction&#34;B is A&#34;. This is the Reversal Curse. For instance, if a model is trained on&#34;Valentina Tereshkova was the first woman to travel to space&#34;, it will not automatically be able to answer the question,&#34;Who was the first woman to travel to space?&#34;. Moreover, the likelihood of the correct answer (&#34;Valentina Tershkova&#34;) will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if&#34;A is B&#34;occurs,&#34;B is A&#34;is more likely to occur. It is worth noting, however, that if&#34;A is B&#34;appears in-context, models can deduce the reverse relationship.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The Reversal Curse: LLMs trained on &#34;A is B&#34; fail to learn &#34;B is A&#34; We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form&#34;A is B&#34;, it will not automatically generalize to the reverse direction&#34;B is A&#34;. This is the Reversal Curse. For instance, if a model is trained on&#34;Valentina Tereshkova was the first woman to travel to space&#34;, it will not automatically be able to answer the question,&#34;Who was the first woman to travel to space?&#34;. Moreover, the likelihood of the correct answer (&#34;Valentina Tershkova&#34;) will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if&#34;A is B&#34;occurs,&#34;B is A&#34;is more likely to occur. It is worth noting, however, that if&#34;A is B&#34;appears in-context, models can deduce the reverse relationship.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/recxapWTc3zIf1FCd" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Sep 15, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/sparse-autoencoders-find-highly-interpretable-features-in-language-models" target="_blank" rel="noopener noreferrer">Sparse Autoencoders Find Highly Interpretable Features in Language Models</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Sparse Autoencoders Find Highly Interpretable Features in Language Models One of the roadblocks to a better understanding of neural networks&#39; internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Sparse Autoencoders Find Highly Interpretable Features in Language Models One of the roadblocks to a better understanding of neural networks&#39; internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/sparse-autoencoders-find-highly-interpretable-features-in-language-models" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Sep 1, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/taken-out-of-context-on-measuring-situational-awareness-in-llms" target="_blank" rel="noopener noreferrer">Taken out of context: On measuring situational awareness in LLMs</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Taken out of context: On measuring situational awareness in LLMs We aim to better understand the emergence of `situational awareness&#39; in large language models (LLMs). A model is situationally aware if it&#39;s aware that it&#39;s a model and can recognize whether it&#39;s currently in testing or deployment. Today&#39;s LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning&#39; (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Taken out of context: On measuring situational awareness in LLMs We aim to better understand the emergence of `situational awareness&#39; in large language models (LLMs). A model is situationally aware if it&#39;s aware that it&#39;s a model and can recognize whether it&#39;s currently in testing or deployment. Today&#39;s LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning&#39; (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/taken-out-of-context-on-measuring-situational-awareness-in-llms" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Aug 20, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/steering-language-models-with-activation-engineering" target="_blank" rel="noopener noreferrer">Steering Language Models With Activation Engineering</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Steering Language Models With Activation Engineering Prompt engineering and finetuning aim to maximize language model performance on a given metric (like toxicity reduction). However, these methods do not fully elicit a model&#39;s capabilities. To reduce this gap, we introduce activation engineering: the inference-time modification of activations in order to control (or steer) model outputs. Specifically, we introduce the Activation Addition (ActAdd) technique, which contrasts the intermediate activations on prompt pairs (such as&#34;Love&#34;versus&#34;Hate&#34;) to compute a steering vector (Subramani et al. 2022). By tactically adding in e.g. the&#34;Love&#34;-&#34;Hate&#34;steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift and detoxification using models including LLaMA-3 and OPT. ActAdd yields inference-time control over high-level output properties (like topic and sentiment) while preserving performance on off-target tasks.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Steering Language Models With Activation Engineering Prompt engineering and finetuning aim to maximize language model performance on a given metric (like toxicity reduction). However, these methods do not fully elicit a model&#39;s capabilities. To reduce this gap, we introduce activation engineering: the inference-time modification of activations in order to control (or steer) model outputs. Specifically, we introduce the Activation Addition (ActAdd) technique, which contrasts the intermediate activations on prompt pairs (such as&#34;Love&#34;versus&#34;Hate&#34;) to compute a steering vector (Subramani et al. 2022). By tactically adding in e.g. the&#34;Love&#34;-&#34;Hate&#34;steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift and detoxification using models including LLaMA-3 and OPT. ActAdd yields inference-time control over high-level output properties (like topic and sentiment) while preserving performance on off-target tasks.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/steering-language-models-with-activation-engineering" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">May 2, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/finding-neurons-in-a-haystack-case-studies-with-sparse-probing" target="_blank" rel="noopener noreferrer">Finding Neurons in a Haystack: Case Studies with Sparse Probing</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Finding Neurons in a Haystack: Case Studies with Sparse Probing Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Finding Neurons in a Haystack: Case Studies with Sparse Probing Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/finding-neurons-in-a-haystack-case-studies-with-sparse-probing" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Apr 6, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/do-the-rewards-justify-the-means-measuring-trade-offs-between-rewards-and-ethical-behavior-in-the-machiavelli-benchmark" target="_blank" rel="noopener noreferrer">Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents&#39; tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents&#39; tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/do-the-rewards-justify-the-means-measuring-trade-offs-between-rewards-and-ethical-behavior-in-the-machiavelli-benchmark" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="MATS">
    <div class="paper-meta">
      <span class="org-tag" data-org="MATS">MATS</span>
      <span class="paper-date">Feb 6, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.matsprogram.org/research/reclAAaZK3Xrt1qBX" target="_blank" rel="noopener noreferrer">A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations.</div>
      </details>
    </div>    <a href="https://www.matsprogram.org/research/reclAAaZK3Xrt1qBX" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/the-logic-of-strategic-assets-from-oil-to-ai" target="_blank" rel="noopener noreferrer">The Logic of Strategic Assets: From Oil to AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">What resources and technologies are strategic? Policy and theoretical debates often focus on this question, since the “strategic” designation yields valuable resources and elevated attention. The ambiguity of the very concept...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">What resources and technologies are strategic? Policy and theoretical debates often focus on this question, since the “strategic” designation yields valuable resources and elevated attention. The ambiguity of the very concept...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/the-logic-of-strategic-assets-from-oil-to-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/emerging-institutions-for-ai-governance-ai-governance-in-2020" target="_blank" rel="noopener noreferrer">Emerging Institutions for AI Governance: AI Governance in 2020</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Much AI governance work involves preparation for a constitutional moment: an opportunity to create long-lasting, decision-shaping, institutions. Doing this well is a formidable task. It requires a fine balance. Institutions...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Much AI governance work involves preparation for a constitutional moment: an opportunity to create long-lasting, decision-shaping, institutions. Doing this well is a formidable task. It requires a fine balance. Institutions...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/emerging-institutions-for-ai-governance-ai-governance-in-2020" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/engines-of-power-electricity-ai-and-general-purpose-military-transformations" target="_blank" rel="noopener noreferrer">Engines of Power: Electricity, AI, and General-Purpose Military Transformations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Major theories of military innovation focus on relatively narrow technological developments, such as nuclear weapons or aircraft carriers. Arguably the most profound military implications of technological change, however, come...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Major theories of military innovation focus on relatively narrow technological developments, such as nuclear weapons or aircraft carriers. Arguably the most profound military implications of technological change, however, come...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/engines-of-power-electricity-ai-and-general-purpose-military-transformations" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/technological-progress-artificial-intelligence-and-inclusive-growth" target="_blank" rel="noopener noreferrer">Technological Progress, Artificial Intelligence, and Inclusive Growth</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Advances in artificial intelligence and automation have the potential to be labor-saving and to increase inequality and poverty around the globe. They also give rise to winner-takes-all dynamics th...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Advances in artificial intelligence and automation have the potential to be labor-saving and to increase inequality and poverty around the globe. They also give rise to winner-takes-all dynamics th...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/technological-progress-artificial-intelligence-and-inclusive-growth" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/futureproof-artificial-intelligence-chapter" target="_blank" rel="noopener noreferrer">Futureproof: Artificial Intelligence Chapter</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Out of the wreckage of the Second World War, the UK transformed itself. It rebuilt its shattered economy. It founded the NHS. It created national insurance. And it helped establish international institutions like the United...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Out of the wreckage of the Second World War, the UK transformed itself. It rebuilt its shattered economy. It founded the NHS. It created national insurance. And it helped establish international institutions like the United...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/futureproof-artificial-intelligence-chapter" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/coercion-and-the-credibility-of-assurances" target="_blank" rel="noopener noreferrer">Coercion and the Credibility of Assurances</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">What makes coercion succeed? For most international relations scholars, the answer is credible threats. Yet scholars have neglected a second key component of successful coercion: credible assurances. This article makes two...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">What makes coercion succeed? For most international relations scholars, the answer is credible threats. Yet scholars have neglected a second key component of successful coercion: credible assurances. This article makes two...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/coercion-and-the-credibility-of-assurances" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/raft-a-real-world-few-shot-text-classification-benchmark" target="_blank" rel="noopener noreferrer">RAFT: A Real-World Few-Shot Text Classification Benchmark</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/raft-a-real-world-few-shot-text-classification-benchmark" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/filling-gaps-in-trustworthy-development-of-ai" target="_blank" rel="noopener noreferrer">Filling gaps in trustworthy development of AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The range of application of artificial intelligence (AI) is vast, as is the potential for harm. Growing awareness of potential risks from AI systems has spurred action to address those risks while eroding confidence in AI...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The range of application of artificial intelligence (AI) is vast, as is the potential for harm. Growing awareness of potential risks from AI systems has spurred action to address those risks while eroding confidence in AI...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/filling-gaps-in-trustworthy-development-of-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/emerging-technologies-prestige-motivations-and-the-dynamics-of-international-competition" target="_blank" rel="noopener noreferrer">Emerging Technologies, Prestige Motivations and the Dynamics of International Competition</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The study of international races has focused almost exclusively on security motivations for competitive arming. But international races may also be motivated by prestige. This article defines a “prestige race” and outlines the...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The study of international races has focused almost exclusively on security motivations for competitive arming. But international races may also be motivated by prestige. This article defines a “prestige race” and outlines the...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/emerging-technologies-prestige-motivations-and-the-dynamics-of-international-competition" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/the-artefacts-of-intelligence-governing-scientists-contribution-to-ai-proliferation" target="_blank" rel="noopener noreferrer">The Artefacts of Intelligence: Governing Scientists&#39; Contribution to AI Proliferation</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This DPhil dissertation is about attempts to govern how artificial intelligence (AI) researchers share their work. There is growing concern that the software artefacts built by AI researchers will have adverse impacts on society..</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This DPhil dissertation is about attempts to govern how artificial intelligence (AI) researchers share their work. There is growing concern that the software artefacts built by AI researchers will have adverse impacts on society..</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/the-artefacts-of-intelligence-governing-scientists-contribution-to-ai-proliferation" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/submission-to-the-nist-ai-risk-management-framework" target="_blank" rel="noopener noreferrer">Submission to the NIST AI Risk Management Framework</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This report gives comments on the Initial Draft of the NIST AI Risk Management Framework (AI RMF). The key recommendations are to put more emphasis on low-probability, high-impact risks, especially catastrophic risks to...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This report gives comments on the Initial Draft of the NIST AI Risk Management Framework (AI RMF). The key recommendations are to put more emphasis on low-probability, high-impact risks, especially catastrophic risks to...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/submission-to-the-nist-ai-risk-management-framework" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/aligned-with-whom-direct-and-social-goals-for-ai-systems" target="_blank" rel="noopener noreferrer">Aligned with whom? Direct and social goals for AI systems</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In this Brookings working paper, Korinek and Balwit discuss the AI alignment problem – how to ensure that AI systems pursue the goals that we want them to pursue. This article distinguishes two types of alignment problems...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In this Brookings working paper, Korinek and Balwit discuss the AI alignment problem – how to ensure that AI systems pursue the goals that we want them to pursue. This article distinguishes two types of alignment problems...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/aligned-with-whom-direct-and-social-goals-for-ai-systems" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/lethal-autonomous-weapons-need-to-be-regulated-but-not-the-way-advocates-say" target="_blank" rel="noopener noreferrer">Lethal Autonomous Weapons Need to be Regulated - But Not the Way Advocates Say</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The development of lethal autonomous weapons systems (LAWS) is no longer just a concept of science fiction. With the potential of these weapons to cause destruction, there is an urgent need for an international approach to...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The development of lethal autonomous weapons systems (LAWS) is no longer just a concept of science fiction. With the potential of these weapons to cause destruction, there is an urgent need for an international approach to...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/lethal-autonomous-weapons-need-to-be-regulated-but-not-the-way-advocates-say" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/ai-ethics-statements-analysis-and-lessons-learnt-from-neurips-broader-impact-statements" target="_blank" rel="noopener noreferrer">AI Ethics Statements: Analysis and lessons learnt from NeurIPS Broader Impact Statements</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/ai-ethics-statements-analysis-and-lessons-learnt-from-neurips-broader-impact-statements" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/information-hazards-in-races-for-advanced-artificial-intelligence" target="_blank" rel="noopener noreferrer">Information Hazards in Races for Advanced Artificial Intelligence</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We study how the information environment affects races to implement a powerful new technology such as advanced artificial intelligence. In particular, we analyse a model in which a potentially unsafe technology may cause...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We study how the information environment affects races to implement a powerful new technology such as advanced artificial intelligence. In particular, we analyse a model in which a potentially unsafe technology may cause...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/information-hazards-in-races-for-advanced-artificial-intelligence" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/forecasting-ai-progress-evidence-from-a-survey-of-machine-learning-researchers" target="_blank" rel="noopener noreferrer">Forecasting AI Progress: Evidence from a Survey of Machine Learning Researchers</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Advances in artificial intelligence (AI) are shaping modern life, from transportation, health care, science, finance, to the military. Forecasts of AI development could help improve policy- and decision making. We report...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Advances in artificial intelligence (AI) are shaping modern life, from transportation, health care, science, finance, to the military. Forecasts of AI development could help improve policy- and decision making. We report...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/forecasting-ai-progress-evidence-from-a-survey-of-machine-learning-researchers" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/submission-nairr-task-force" target="_blank" rel="noopener noreferrer">Submission to the Request for Information (RFI) on Implementing Initial Findings and Recommendations of the NAIRR Task Force</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This report gives comments on the interim report of the National AI Research Resource (NAIRR) Task Force. The key recommendations are: Provides researchers with access to pre-trained models by providing infrastructure...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This report gives comments on the interim report of the National AI Research Resource (NAIRR) Task Force. The key recommendations are: Provides researchers with access to pre-trained models by providing infrastructure...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/submission-nairr-task-force" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/knowledge-sharing-to-prevent-dangerous-technology-races" target="_blank" rel="noopener noreferrer">The IAEA Solution: Knowledge Sharing to Prevent Dangerous Technology Races</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The world appears to be entering an era of heightened great power technological competition in areas such as artificial intelligence. This is concerning because deploying new technologies often involves private benefits and...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The world appears to be entering an era of heightened great power technological competition in areas such as artificial intelligence. This is concerning because deploying new technologies often involves private benefits and...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/knowledge-sharing-to-prevent-dangerous-technology-races" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/the-security-governance-challenge-of-emerging-technologies" target="_blank" rel="noopener noreferrer">The Security Governance Challenge of Emerging Technologies</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In recent decades, governments have been ineffectual at regulating dangerous emerging technologies like lethal autonomous weapons and synthetic biology. In today’s era of great power competition...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In recent decades, governments have been ineffectual at regulating dangerous emerging technologies like lethal autonomous weapons and synthetic biology. In today’s era of great power competition...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/the-security-governance-challenge-of-emerging-technologies" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/future-of-compute-review-call-for-evidence" target="_blank" rel="noopener noreferrer">GovAI Response to the Future of Compute Review - Call for Evidence</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We welcome the opportunity to respond to the Future of Compute Review’s call for evidence...Our response focuses on the future of compute used for Artificial Intelligence (AI). In particular, we emphasise the risks posed by...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We welcome the opportunity to respond to the Future of Compute Review’s call for evidence...Our response focuses on the future of compute used for Artificial Intelligence (AI). In particular, we emphasise the risks posed by...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/future-of-compute-review-call-for-evidence" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/preparing-for-the-non-existent-future-of-work" target="_blank" rel="noopener noreferrer">Preparing for the (Non-Existent?) Future of Work</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This Brookings working paper presents an analysis of how institutions could be set up to prepare for a scenario of increasingly smart autonomous machines which could replace human labor and...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This Brookings working paper presents an analysis of how institutions could be set up to prepare for a scenario of increasingly smart autonomous machines which could replace human labor and...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/preparing-for-the-non-existent-future-of-work" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/brussels-effect-ai" target="_blank" rel="noopener noreferrer">The Brussels Effect and Artificial Intelligence</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In this report, we ask whether the EU’s upcoming regulation for AI will diffuse globally, producing a so-called “Brussels Effect”.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In this report, we ask whether the EU’s upcoming regulation for AI will diffuse globally, producing a so-called “Brussels Effect”.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/brussels-effect-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/safety-not-guaranteed-international-strategic-dynamics-of-risky-technology-races" target="_blank" rel="noopener noreferrer">Safety Not Guaranteed: International Strategic Dynamics of Risky Technology Races</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The great powers appear to be entering an era of heightened competition to master security-relevant technologies in areas such as AI. This is concerning because deploying new technologies can create substantial...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The great powers appear to be entering an era of heightened competition to master security-relevant technologies in areas such as AI. This is concerning because deploying new technologies can create substantial...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/safety-not-guaranteed-international-strategic-dynamics-of-risky-technology-races" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/lessons-atomic-bomb-ord" target="_blank" rel="noopener noreferrer">Lessons from the Development of the Atomic Bomb</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This report summarises the most important aspects of the development of atomic weapons and draws out a number of important insights for the development of similarly important technologies.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This report summarises the most important aspects of the development of atomic weapons and draws out a number of important insights for the development of similarly important technologies.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/lessons-atomic-bomb-ord" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/deliberating-autonomous-weapons" target="_blank" rel="noopener noreferrer">Deliberating Autonomous Weapons</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Stuart Russell has had a seminal influence on many people with his writings on aligning artificial intelligence with human values and regulating autonomous weapons systems. His work has highlighted the difficulties...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Stuart Russell has had a seminal influence on many people with his writings on aligning artificial intelligence with human values and regulating autonomous weapons systems. His work has highlighted the difficulties...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/deliberating-autonomous-weapons" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/safety-performance-tradeoff-model-web-ap" target="_blank" rel="noopener noreferrer">Safety-Performance Tradeoff Model Web App</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This web app is a tool for exploring the dynamics of risky AI competition: the safety-performance tradeoff. Will AI safety breakthroughs always lead to safer AI systems? Before long, we may be capable of creating AI systems...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This web app is a tool for exploring the dynamics of risky AI competition: the safety-performance tradeoff. Will AI safety breakthroughs always lead to safer AI systems? Before long, we may be capable of creating AI systems...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/safety-performance-tradeoff-model-web-ap" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/risk-management-in-the-artificial-intelligence-act" target="_blank" rel="noopener noreferrer">Risk management in the Artificial Intelligence Act</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The proposed EU AI Act is the first comprehensive attempt to regulate AI in a major jurisdiction. This article analyses Article 9, the key risk management provision in the AI Act. It gives an overview of the regulatory...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The proposed EU AI Act is the first comprehensive attempt to regulate AI in a major jurisdiction. This article analyses Article 9, the key risk management provision in the AI Act. It gives an overview of the regulatory...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/risk-management-in-the-artificial-intelligence-act" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/protecting-society-from-ai-misuse-when-are-restrictions-on-capabilities-warranted" target="_blank" rel="noopener noreferrer">Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Artificial intelligence (AI) systems will increasingly be used to cause harm as they grow more capable. In fact, AI systems are already starting to be used to automate fraudulent activities, violate human rights, create...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Artificial intelligence (AI) systems will increasingly be used to cause harm as they grow more capable. In fact, AI systems are already starting to be used to automate fraudulent activities, violate human rights, create...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/protecting-society-from-ai-misuse-when-are-restrictions-on-capabilities-warranted" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/exploring-the-relevance-of-data-privacy-enhancing-technologies-for-ai-governance-use-cases" target="_blank" rel="noopener noreferrer">Exploring the Relevance of Data Privacy-Enhancing Technologies for AI Governance Use Cases</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The development of privacy-enhancing technologies has made immense progress in reducing trade-offs between privacy and performance in data exchange and analysis. Similar tools could be useful for AI governance...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The development of privacy-enhancing technologies has made immense progress in reducing trade-offs between privacy and performance in data exchange and analysis. Similar tools could be useful for AI governance...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/exploring-the-relevance-of-data-privacy-enhancing-technologies-for-ai-governance-use-cases" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/democratising-ai-multiple-meanings-goals-and-methods" target="_blank" rel="noopener noreferrer">Democratising AI: Multiple Meanings, Goals, and Methods</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper outlines four different notions of “AI democratisation”, three of which are used almost synonymously with “increasing accessibility”. The democratisation of AI use and the democratisation of AI development are about...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper outlines four different notions of “AI democratisation”, three of which are used almost synonymously with “increasing accessibility”. The democratisation of AI use and the democratisation of AI development are about...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/democratising-ai-multiple-meanings-goals-and-methods" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/response-to-the-uks-future-of-compute-review" target="_blank" rel="noopener noreferrer">Response to the UK&#39;s Future of Compute Review</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We are pleased to see the publication of the UK’s Future of Compute Review. However, we also believe there is a significant missed opportunity: the review does not address how to ensure that compute is used responsibly or how...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We are pleased to see the publication of the UK’s Future of Compute Review. However, we also believe there is a significant missed opportunity: the review does not address how to ensure that compute is used responsibly or how...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/response-to-the-uks-future-of-compute-review" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/how-to-design-an-ai-ethics-board" target="_blank" rel="noopener noreferrer">How to Design an AI Ethics Board</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Organizations that develop and deploy artificial intelligence (AI) systems need to take measures to reduce the associated risks. In this paper, we examine how AI companies could design an AI ethics board in a way that reduces...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Organizations that develop and deploy artificial intelligence (AI) systems need to take measures to reduce the associated risks. In this paper, we examine how AI companies could design an AI ethics board in a way that reduces...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/how-to-design-an-ai-ethics-board" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape" target="_blank" rel="noopener noreferrer">Recent Trends in China&#39;s Large Language Model Landscape</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As large-scale pre-trained AI models gain popularity in the West, many Chinese AI labs have developed their own models capable of generating coherent text and realistic images and videos. These models represent the frontier...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As large-scale pre-trained AI models gain popularity in the West, many Chinese AI labs have developed their own models capable of generating coherent text and realistic images and videos. These models represent the frontier...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/llms-used-spear-phishing" target="_blank" rel="noopener noreferrer">Spear Phishing with Large Language Models</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This study investigates how LLMs can be used for spear phishing, a form of cybercrime that involves manipulating targets into divulging sensitive information. It first explores LLMs’ ability to assist with the reconnaissance...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This study investigates how LLMs can be used for spear phishing, a form of cybercrime that involves manipulating targets into divulging sensitive information. It first explores LLMs’ ability to assist with the reconnaissance...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/llms-used-spear-phishing" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/towards-best-practices-in-agi-safety-and-governance" target="_blank" rel="noopener noreferrer">Towards Best Practices in AGI Safety and Governance</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A number of leading AI companies, including OpenAI, Google DeepMind, andAnthropic, have the stated goal of building artificial general intelligence (AGI)—AI systems that achieve or exceed human performance...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A number of leading AI companies, including OpenAI, Google DeepMind, andAnthropic, have the stated goal of building artificial general intelligence (AGI)—AI systems that achieve or exceed human performance...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/towards-best-practices-in-agi-safety-and-governance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/model-evaluation-for-extreme-risks" target="_blank" rel="noopener noreferrer">Model Evaluation for Extreme Risks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/model-evaluation-for-extreme-risks" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/auditing-large-language-models-a-three-layered-approach" target="_blank" rel="noopener noreferrer">Auditing Large Language Models: A Three‐Layered Approach</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/auditing-large-language-models-a-three-layered-approach" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/response-to-the-ntia-ai-accountability-policy" target="_blank" rel="noopener noreferrer">Response to the NTIA AI Accountability Policy</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We welcome the opportunity to respond to the NTIA’s AI Accountability Policy Request for Comment and look forward to future opportunities to provide additional input. We offer the following...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We welcome the opportunity to respond to the NTIA’s AI Accountability Policy Request for Comment and look forward to future opportunities to provide additional input. We offer the following...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/response-to-the-ntia-ai-accountability-policy" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/frontier-ai-regulation-managing-emerging-risks-to-public-safety" target="_blank" rel="noopener noreferrer">Frontier AI Regulation: Managing Emerging Risks to Public Safety</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term “frontier AI” models — highly capable...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term “frontier AI” models — highly capable...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/frontier-ai-regulation-managing-emerging-risks-to-public-safety" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/risk-assessment-at-agi-companies-a-review-of-popular-risk-assessment-techniques-from-other-safety-critical-industries" target="_blank" rel="noopener noreferrer">Risk Assessment at AGI Companies: A Review of Popular Risk Assessment Techniques From Other Safety-Critical Industries</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">There are increasing concerns that AGI would pose catastrophic risks. In light of this, AGI companies need to drastically improve their risk management practices. This paper reviews popular risk assessment techniques...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">There are increasing concerns that AGI would pose catastrophic risks. In light of this, AGI companies need to drastically improve their risk management practices. This paper reviews popular risk assessment techniques...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/risk-assessment-at-agi-companies-a-review-of-popular-risk-assessment-techniques-from-other-safety-critical-industries" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/national-priorities-for-artificial-intelligence-ostp-response" target="_blank" rel="noopener noreferrer">National Priorities for Artificial Intelligence (Response to the OSTP Request for Information)</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We welcome the opportunity to respond to the OSTP Request for Information on National Priorities for Artificial Intelligence and look forward to future opportunities to provide additional input. We offer the following...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We welcome the opportunity to respond to the OSTP Request for Information on National Priorities for Artificial Intelligence and look forward to future opportunities to provide additional input. We offer the following...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/national-priorities-for-artificial-intelligence-ostp-response" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/how-to-prevent-an-ai-catastrophe" target="_blank" rel="noopener noreferrer">How to Prevent an AI Catastrophe</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Markus Anderljung and Paul Scharre recently published this piece in Foreign Affairs</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Markus Anderljung and Paul Scharre recently published this piece in Foreign Affairs</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/how-to-prevent-an-ai-catastrophe" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/international-governance-of-civilian-ai" target="_blank" rel="noopener noreferrer">International Governance of Civilian AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/international-governance-of-civilian-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/market-concentration-implications-of-foundation-models" target="_blank" rel="noopener noreferrer">Market Concentration Implications of Foundation Models</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This Brookings Working Paper analyses the structure of the market for foundation models, and examines the implications for competition policy and regulation.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This Brookings Working Paper analyses the structure of the market for foundation models, and examines the implications for competition policy and regulation.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/market-concentration-implications-of-foundation-models" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models" target="_blank" rel="noopener noreferrer">Open-Sourcing Highly Capable Foundation Models</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We evaluate the risks and benefits of open-sourcing, as well as alternative methods for pursuing open-source objectives.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We evaluate the risks and benefits of open-sourcing, as well as alternative methods for pursuing open-source objectives.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/coordinated-pausing-evaluation-based-scheme" target="_blank" rel="noopener noreferrer">Coordinated Pausing: An Evaluation-Based Coordination Scheme for Frontier AI Developers</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper proposes an evaluation-based coordination scheme for situations in which frontier AI developers discover that their models have certain dangerous capabilities.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper proposes an evaluation-based coordination scheme for situations in which frontier AI developers discover that their models have certain dangerous capabilities.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/coordinated-pausing-evaluation-based-scheme" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/oversight-for-frontier-ai-through-kyc-scheme-for-compute-providers" target="_blank" rel="noopener noreferrer">Oversight for Frontier AI through a Know-Your-Customer Scheme for Compute Providers</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">To address security and safety risks stemming from highly capable artificial intelligence (AI) models, we propose that the US government should ensure compute providers implement Know-Your-Customer (KYC) schemes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">To address security and safety risks stemming from highly capable artificial intelligence (AI) models, we propose that the US government should ensure compute providers implement Know-Your-Customer (KYC) schemes.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/oversight-for-frontier-ai-through-kyc-scheme-for-compute-providers" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/structured-access-for-third-party-research-on-frontier-ai-models" target="_blank" rel="noopener noreferrer">Structured Access for Third-Party Research on Frontier AI Models</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Recent releases of frontier artificial intelligence (AI) models have largely been gated, providing the benefit of limiting the proliferation of increasingly powerful dual-use capabilities. However, such release strategies...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Recent releases of frontier artificial intelligence (AI) models have largely been gated, providing the benefit of limiting the proliferation of increasingly powerful dual-use capabilities. However, such release strategies...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/structured-access-for-third-party-research-on-frontier-ai-models" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/three-lines-of-defense-against-risks-from-ai" target="_blank" rel="noopener noreferrer">Three Lines of Defense Against Risks from AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Organizations that develop and deploy artificial intelligence (AI) systems need to manage the associated risks—for economic, legal, and ethical reasons. However, it is not always clear who is responsible for AI risk management.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Organizations that develop and deploy artificial intelligence (AI) systems need to manage the associated risks—for economic, legal, and ethical reasons. However, it is not always clear who is responsible for AI risk management.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/three-lines-of-defense-against-risks-from-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/preparing-the-workforce-for-an-uncertain-ai-future" target="_blank" rel="noopener noreferrer">Preparing the Workforce for an Uncertain AI Future</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In this written testimony to the U.S. Senate&#39;s bipartisan AI Insights Forum, Anton Korinek encourages lawmakers to engage in scenario planning to prepare the workforce for advances in AI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In this written testimony to the U.S. Senate&#39;s bipartisan AI Insights Forum, Anton Korinek encourages lawmakers to engage in scenario planning to prepare the workforce for advances in AI.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/preparing-the-workforce-for-an-uncertain-ai-future" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/towards-publicly-accountable-frontier-llms" target="_blank" rel="noopener noreferrer">Towards Publicly Accountable Frontier LLMs</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">With the increasing integration of frontier large language models (LLMs) into society and the economy, decisions related to their training, deployment, and use have far-reaching implications...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">With the increasing integration of frontier large language models (LLMs) into society and the economy, decisions related to their training, deployment, and use have far-reaching implications...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/towards-publicly-accountable-frontier-llms" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/frontier-ai-regulation-safeguards-amid-rapid-progress" target="_blank" rel="noopener noreferrer">Frontier AI Regulation: Safeguards Amid Rapid Progress</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">To deal with the risks of these high-compute frontier AI systems, we must govern not only how they can be used but also how they are developed and made available to people in the first place.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">To deal with the risks of these high-compute frontier AI systems, we must govern not only how they can be used but also how they are developed and made available to people in the first place.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/frontier-ai-regulation-safeguards-amid-rapid-progress" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/increase-compute-efficiency-and-the-diffusion-of-ai-capabilities" target="_blank" rel="noopener noreferrer">Increased Compute Efficiency and the Diffusion of AI Capabilities</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Training advanced AI models requires large investments in computational resources, or compute. Yet, as hardware innovation reduces the price of compute and algorithmic advances make its use more efficient, the cost of training...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Training advanced AI models requires large investments in computational resources, or compute. Yet, as hardware innovation reduces the price of compute and algorithmic advances make its use more efficient, the cost of training...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/increase-compute-efficiency-and-the-diffusion-of-ai-capabilities" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/computing-power-and-the-governance-of-artificial-intelligence" target="_blank" rel="noopener noreferrer">Computing Power and the Governance of Artificial Intelligence</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Recent AI progress has largely been driven by increases in the amount of computing power used to train new models. Governing compute could be an effective way to achieve AI policy goals, but could also introduce new risks.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Recent AI progress has largely been driven by increases in the amount of computing power used to train new models. Governing compute could be an effective way to achieve AI policy goals, but could also introduce new risks.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/computing-power-and-the-governance-of-artificial-intelligence" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/response-to-the-rfi-related-to-nists-assignments-under-the-executive-order-concerning-ai" target="_blank" rel="noopener noreferrer">Response to the RFI Related to NIST&#39;s Assignments Under the Executive Order Concerning AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We welcome the opportunity to respond to the Request for Information (RFI) Related to NIST’s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning AI. We offer the following submission for your...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We welcome the opportunity to respond to the Request for Information (RFI) Related to NIST’s Assignments Under Sections 4.1, 4.5 and 11 of the Executive Order Concerning AI. We offer the following submission for your...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/response-to-the-rfi-related-to-nists-assignments-under-the-executive-order-concerning-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/governing-through-the-cloud" target="_blank" rel="noopener noreferrer">Governing Through the Cloud</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Compute providers can play an essential role in a regulatory ecosystem via four key capacities: as securers, safeguarding AI systems and critical infrastructure; as record keepers, enhancing visibility for policymakers...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Compute providers can play an essential role in a regulatory ecosystem via four key capacities: as securers, safeguarding AI systems and critical infrastructure; as record keepers, enhancing visibility for policymakers...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/governing-through-the-cloud" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/scenarios-for-the-transition-to-agi" target="_blank" rel="noopener noreferrer">Scenarios for the Transition to AGI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We analyze how output and wages behave under different scenarios for technological progress that may culminate in Artificial General Intelligence (AGI), defined as the ability of AI systems to perform all tasks that humans can...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We analyze how output and wages behave under different scenarios for technological progress that may culminate in Artificial General Intelligence (AGI), defined as the ability of AI systems to perform all tasks that humans can...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/scenarios-for-the-transition-to-agi" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/societal-adaptation-to-advanced-ai" target="_blank" rel="noopener noreferrer">Societal Adaptation to Advanced AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Existing strategies for managing risks from advanced AI systems often focus on affecting what AI systems are developed and how they diffuse. This approach becomes less feasible as the number of developers of advanced AI grows.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Existing strategies for managing risks from advanced AI systems often focus on affecting what AI systems are developed and how they diffuse. This approach becomes less feasible as the number of developers of advanced AI grows.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/societal-adaptation-to-advanced-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/tort-law-and-frontier-ai-governance" target="_blank" rel="noopener noreferrer">Tort Law and Frontier AI Governance</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Matthew van der Merwe, Ketan Ramakrishnan, and Markus Anderljung recently published this piece on Lawfare.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Matthew van der Merwe, Ketan Ramakrishnan, and Markus Anderljung recently published this piece on Lawfare.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/tort-law-and-frontier-ai-governance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/visibility-into-ai-agents" target="_blank" rel="noopener noreferrer">Visibility into AI Agents</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Increased delegation of commercial, scientific, governmental, and personal activities to AI agents—systems capable of pursuing complex goals with limited supervision—may...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Increased delegation of commercial, scientific, governmental, and personal activities to AI agents—systems capable of pursuing complex goals with limited supervision—may...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/visibility-into-ai-agents" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/risk-thresholds-for-frontier-ai" target="_blank" rel="noopener noreferrer">Risk Thresholds for Frontier AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Frontier artificial intelligence (AI) systems could pose increasing risks to public safety and security. But what level of risk is acceptable? One increasingly popular approach is to...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Frontier artificial intelligence (AI) systems could pose increasing risks to public safety and security. But what level of risk is acceptable? One increasingly popular approach is to...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/risk-thresholds-for-frontier-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/intelligent-financial-system-how-ai-is-transforming-finance" target="_blank" rel="noopener noreferrer">Intelligent Financial System: How AI Is Transforming Finance</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">At the core of the financial system is the processing and aggregation of vast amounts of information into price signals that coordinate participants in the economy. Throughout history, advances in information processing...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">At the core of the financial system is the processing and aggregation of vast amounts of information into price signals that coordinate participants in the economy. Throughout history, advances in information processing...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/intelligent-financial-system-how-ai-is-transforming-finance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/gpts-are-gpts-labor-market-impact-potential-of-llms" target="_blank" rel="noopener noreferrer">GPTs are GPTs: Labor market impact potential of LLMs</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We propose a framework for evaluating the potential impacts of large-language models (LLMs) and associated technologies on work by considering their relevance to the tasks workers perform in their jobs.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We propose a framework for evaluating the potential impacts of large-language models (LLMs) and associated technologies on work by considering their relevance to the tasks workers perform in their jobs.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/gpts-are-gpts-labor-market-impact-potential-of-llms" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/ais-impact-on-income-inequality-in-the-us" target="_blank" rel="noopener noreferrer">AI’s Impact on Income Inequality in the US</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">According to one survey, about half of Americans think that the increased use of AI will lead to greater income inequality and a more polarized society. Roughly two thirds...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">According to one survey, about half of Americans think that the increased use of AI will lead to greater income inequality and a more polarized society. Roughly two thirds...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/ais-impact-on-income-inequality-in-the-us" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/open-problems-in-technical-ai-governance" target="_blank" rel="noopener noreferrer">Open Problems in Technical AI Governance</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI progress is creating a growing range of risks and opportunities, but it is often unclear how they should be navigated. In many cases, the barriers and uncertainties faced are at least partly technical...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI progress is creating a growing range of risks and opportunities, but it is often unclear how they should be navigated. In many cases, the barriers and uncertainties faced are at least partly technical...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/open-problems-in-technical-ai-governance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/training-compute-thresholds-features-and-functions-in-ai-regulation" target="_blank" rel="noopener noreferrer">Training Compute Thresholds: Features and Functions in AI Regulation</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Regulators in the US and EU are using thresholds based on training compute--the number of computational operations used in training--to identify general-purpose artificial intelligence (GPAI) models that may pose risks of...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Regulators in the US and EU are using thresholds based on training compute--the number of computational operations used in training--to identify general-purpose artificial intelligence (GPAI) models that may pose risks of...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/training-compute-thresholds-features-and-functions-in-ai-regulation" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/from-principles-to-rules-a-regulatory-approach-for-frontier-ai" target="_blank" rel="noopener noreferrer">From Principles to Rules: A Regulatory Approach for Frontier AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Several jurisdictions are starting to regulate frontier artificial intelligence (AI) systems, i.e. general-purpose AI systems that match or exceed the capabilities present in the most advanced systems. To reduce risks...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Several jurisdictions are starting to regulate frontier artificial intelligence (AI) systems, i.e. general-purpose AI systems that match or exceed the capabilities present in the most advanced systems. To reduce risks...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/from-principles-to-rules-a-regulatory-approach-for-frontier-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/a-grading-rubric-for-ai-safety-frameworks" target="_blank" rel="noopener noreferrer">A Grading Rubric for AI Safety Frameworks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Over the past year, artificial intelligence (AI) companies have been increasingly adopting AI safety frameworks. These frameworks outline how companies intend to keep the potential risks associated with developing and deploying...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Over the past year, artificial intelligence (AI) companies have been increasingly adopting AI safety frameworks. These frameworks outline how companies intend to keep the potential risks associated with developing and deploying...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/a-grading-rubric-for-ai-safety-frameworks" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/economic-policy-challenges-for-the-age-of-ai" target="_blank" rel="noopener noreferrer">Economic Policy Challenges for the Age of AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper examines the profound challenges that transformative advances in AI towards Artificial General Intelligence (AGI) will pose for economists and economic policymakers.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper examines the profound challenges that transformative advances in AI towards Artificial General Intelligence (AGI) will pose for economists and economic policymakers.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/economic-policy-challenges-for-the-age-of-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/voice-and-access-in-ai-global-ai-majority-participation-in-artificial-intelligence-development-and-governance" target="_blank" rel="noopener noreferrer">Voice and Access in AI: Global AI Majority Participation in Artificial Intelligence Development and Governance</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This white paper investigates practical remedies to increase voice in and access to AI governance and capabilities for the Global AI Majority, while addressing the security and commercial concerns of frontier AI states.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This white paper investigates practical remedies to increase voice in and access to AI governance and capabilities for the Global AI Majority, while addressing the security and commercial concerns of frontier AI states.</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/voice-and-access-in-ai-global-ai-majority-participation-in-artificial-intelligence-development-and-governance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/safety-cases-for-frontier-ai" target="_blank" rel="noopener noreferrer">Safety Cases for Frontier AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As frontier artificial intelligence (AI) systems become more capable, it becomes more important that developers can explain why their systems are sufficiently safe. One way to do so is via safety cases: reports that...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As frontier artificial intelligence (AI) systems become more capable, it becomes more important that developers can explain why their systems are sufficiently safe. One way to do so is via safety cases: reports that...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/safety-cases-for-frontier-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/ids-for-ai-systems" target="_blank" rel="noopener noreferrer">IDs for AI Systems</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI systems are increasingly pervasive, yet information needed to decide whether and how to engage with them may not exist or be accessible. A user may not be able to verify whether a system has certain safety certifications...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI systems are increasingly pervasive, yet information needed to decide whether and how to engage with them may not exist or be accessible. A user may not be able to verify whether a system has certain safety certifications...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/ids-for-ai-systems" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument" target="_blank" rel="noopener noreferrer">Safety Case Template for Frontier AI: A Cyber Inability Argument</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Frontier artificial intelligence (AI) systems pose increasing risks to society, making it essential for developers to provide assurances about their safety. One approach to offering such assurances is through a safety case...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Frontier artificial intelligence (AI) systems pose increasing risks to society, making it essential for developers to provide assurances about their safety. One approach to offering such assurances is through a safety case...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/what-should-be-internationalised-in-ai-governance" target="_blank" rel="noopener noreferrer">What Should Be Internationalised in AI Governance?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As artificial intelligence (AI) advances, states increasingly recognise the need for international governance to address shared benefits and challenges. However, international cooperation is complex and...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As artificial intelligence (AI) advances, states increasingly recognise the need for international governance to address shared benefits and challenges. However, international cooperation is complex and...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/what-should-be-internationalised-in-ai-governance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/local-us-officials-views-on-the-impacts-and-governance-of-ai-evidence-from-2022-and-2023-survey-waves" target="_blank" rel="noopener noreferrer">Local US Officials&#39; Views on the Impacts and Governance of AI: Evidence from 2022 and 2023 Survey Waves</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper presents a survey of local US policymakers&#39; views on the future impact and regulation of AI. Our survey provides insight into...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper presents a survey of local US policymakers&#39; views on the future impact and regulation of AI. Our survey provides insight into...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/local-us-officials-views-on-the-impacts-and-governance-of-ai-evidence-from-2022-and-2023-survey-waves" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/infrastructure-for-ai-agents" target="_blank" rel="noopener noreferrer">Infrastructure for AI Agents</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Increasingly many AI systems can plan and execute interactions in open-ended environments, such as making phone calls or buying online goods. As developers grow the space of tasks that such AI agents can accomplish...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Increasingly many AI systems can plan and execute interactions in open-ended environments, such as making phone calls or buying online goods. As developers grow the space of tasks that such AI agents can accomplish...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/infrastructure-for-ai-agents" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/options-and-motivations-for-international-ai-benefit-sharing" target="_blank" rel="noopener noreferrer">Options and Motivations for International AI Benefit Sharing</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Advanced AI systems could generate substantial economic and other societal benefits, but these benefits may not be widely shared by default. For a range of reasons, a number of prominent actors and institutions have called for...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Advanced AI systems could generate substantial economic and other societal benefits, but these benefits may not be widely shared by default. For a range of reasons, a number of prominent actors and institutions have called for...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/options-and-motivations-for-international-ai-benefit-sharing" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/measuring-ai-agent-autonomy-towards-a-scalable-approach-with-code-inspection" target="_blank" rel="noopener noreferrer">Measuring AI Agent Autonomy: Towards a Scalable Approach with Code Inspection</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI agents are AI systems that can achieve complex goals autonomously. Assessing the level of agent autonomy is crucial for...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI agents are AI systems that can achieve complex goals autonomously. Assessing the level of agent autonomy is crucial for...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/measuring-ai-agent-autonomy-towards-a-scalable-approach-with-code-inspection" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/regulatory-supervision-of-frontier-ai-developers" target="_blank" rel="noopener noreferrer">Regulatory Supervision of Frontier AI Developers</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Artificial Intelligence (AI) systems have the potential to cause, directly or indirectly, immense harm. They also may immensely improve human welfare. The regulatory challenge is...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Artificial Intelligence (AI) systems have the potential to cause, directly or indirectly, immense harm. They also may immensely improve human welfare. The regulatory challenge is...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/regulatory-supervision-of-frontier-ai-developers" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/ai-powered-lawyering-ai-reasoning-models-retrieval-augmented-generation-and-the-future-of-legal-practice" target="_blank" rel="noopener noreferrer">AI-Powered Lawyering: AI Reasoning Models, Retrieval Augmented Generation, and the Future of Legal Practice</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Generative AI is set to transform the legal profession, but its full impact remains uncertain. While AI models like GPT-4 improve...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Generative AI is set to transform the legal profession, but its full impact remains uncertain. While AI models like GPT-4 improve...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/ai-powered-lawyering-ai-reasoning-models-retrieval-augmented-generation-and-the-future-of-legal-practice" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/on-regulating-downstream-ai-developers" target="_blank" rel="noopener noreferrer">On Regulating Downstream AI Developers</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Downstream developers - actors who fine-tune or otherwise modify foundational models - can create or amplify risks from foundation models by improving their capabilities or compromising safety features...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Downstream developers - actors who fine-tune or otherwise modify foundational models - can create or amplify risks from foundation models by improving their capabilities or compromising safety features...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/on-regulating-downstream-ai-developers" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/in-house-evaluation-is-not-enough-towards-robust-third-party-flaw-disclosure-for-general-purpose-ai" target="_blank" rel="noopener noreferrer">In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/in-house-evaluation-is-not-enough-towards-robust-third-party-flaw-disclosure-for-general-purpose-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/extending-gpts-are-gpts-to-firms" target="_blank" rel="noopener noreferrer">Extending “GPTs Are GPTs” to Firms</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We extend Eloundou et al. (2024) to build firm-level measures of exposure to large language models (LLMs) with data from two sources...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We extend Eloundou et al. (2024) to build firm-level measures of exposure to large language models (LLMs) with data from two sources...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/extending-gpts-are-gpts-to-firms" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/third-party-compliance-reviews-for-frontier-ai-safety-frameworks" target="_blank" rel="noopener noreferrer">Third-Party Compliance Reviews for Frontier AI Safety Frameworks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Safety frameworks have emerged as a best practice for managing risks from frontier artificial intelligence (AI) systems. However, it may be difficult...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Safety frameworks have emerged as a best practice for managing risks from frontier artificial intelligence (AI) systems. However, it may be difficult...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/third-party-compliance-reviews-for-frontier-ai-safety-frameworks" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/forecasting-llm-enabled-biorisk-and-the-efficacy-of-safeguards" target="_blank" rel="noopener noreferrer">Forecasting LLM-Enabled Biorisk and the Efficacy of Safeguards</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Capabilities of large language models (LLMs) on several biological benchmarks have prompted excitement about their usefulness for...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Capabilities of large language models (LLMs) on several biological benchmarks have prompted excitement about their usefulness for...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/forecasting-llm-enabled-biorisk-and-the-efficacy-of-safeguards" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/from-turing-to-tomorrow-the-uks-approach-to-ai-regulation" target="_blank" rel="noopener noreferrer">From Turing to Tomorrow: The UK&#39;s Approach to AI Regulation</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The UK has pursued a distinctive path in AI regulation: less cautious than the EU but more willing to address risks than...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The UK has pursued a distinctive path in AI regulation: less cautious than the EU but more willing to address risks than...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/from-turing-to-tomorrow-the-uks-approach-to-ai-regulation" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/incident-analysis-for-ai-agents" target="_blank" rel="noopener noreferrer">Incident Analysis for AI Agents</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/incident-analysis-for-ai-agents" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/survey-on-thresholds-for-advanced-ai-systems" target="_blank" rel="noopener noreferrer">Survey on Thresholds for Advanced AI Systems</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Governments around the world have recognised the need to manage risks from...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Governments around the world have recognised the need to manage risks from...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/survey-on-thresholds-for-advanced-ai-systems" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/stream-chembio-a-standard-for-transparently-reporting-evaluations-in-ai-model-reports" target="_blank" rel="noopener noreferrer">STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/stream-chembio-a-standard-for-transparently-reporting-evaluations-in-ai-model-reports" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/inference-scaling-and-ai-governance" target="_blank" rel="noopener noreferrer">Inference Scaling and AI Governance</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The shift from scaling up the compute used to pre-train AI systems (pre-training compute) to scaling up the amount used to run them (inference compute) may have...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The shift from scaling up the compute used to pre-train AI systems (pre-training compute) to scaling up the amount used to run them (inference compute) may have...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/inference-scaling-and-ai-governance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/assessing-risk-relative-to-competitors-an-analysis-of-current-ai-company-policies" target="_blank" rel="noopener noreferrer">Assessing Risk Relative to Competitors: An Analysis of Current AI Company Policies</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/assessing-risk-relative-to-competitors-an-analysis-of-current-ai-company-policies" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/labeling-of-ai-agent-activity-in-article-50-of-the-eu-ai-act" target="_blank" rel="noopener noreferrer">Labeling of AI Agent Activity in Article 50 of the EU AI Act</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The online activities of AI agents could distort human beliefs and behaviors. For example, humans could mistake...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The online activities of AI agents could distort human beliefs and behaviors. For example, humans could mistake...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/labeling-of-ai-agent-activity-in-article-50-of-the-eu-ai-act" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/understanding-ais-labor-market-impacts-opportunities-for-the-department-of-labors-ai-workforce-research-hub" target="_blank" rel="noopener noreferrer">Understanding AI’s Labor Market Impacts</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">There is deep uncertainty about how significant AI’s workforce effects will be, how quickly they will emerge, and which groups they will affect most. Some economists expect...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">There is deep uncertainty about how significant AI’s workforce effects will be, how quickly they will emerge, and which groups they will affect most. Some economists expect...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/understanding-ais-labor-market-impacts-opportunities-for-the-department-of-labors-ai-workforce-research-hub" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/dual-use-ai-capabilities-and-the-risk-of-bioterrorism-converting-capability-evaluations-to-risk-assessments" target="_blank" rel="noopener noreferrer">Dual-Use AI Capabilities and the Risk of Bioterrorism</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Several frontier AI companies test their AI systems for dual-use biological capabilities that might be misused by...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Several frontier AI companies test their AI systems for dual-use biological capabilities that might be misused by...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/dual-use-ai-capabilities-and-the-risk-of-bioterrorism-converting-capability-evaluations-to-risk-assessments" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/does-the-uae-have-an-advantage-in-building-data-centers" target="_blank" rel="noopener noreferrer">Does the UAE Have an Advantage in Building Data Centers?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">American hyperscalers are increasingly exploring building data centers in the UAE, but it is unclear whether doing so is...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">American hyperscalers are increasingly exploring building data centers in the UAE, but it is unclear whether doing so is...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/does-the-uae-have-an-advantage-in-building-data-centers" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/how-adaptable-are-american-workers-to-ai-induced-job-displacement" target="_blank" rel="noopener noreferrer">How Adaptable Are American Workers to AI-Induced Job Displacement?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We construct an occupation-level adaptive capacity index that measures a set of worker characteristics relevant for navigating...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We construct an occupation-level adaptive capacity index that measures a set of worker characteristics relevant for navigating...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/how-adaptable-are-american-workers-to-ai-induced-job-displacement" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/permission-manifests-for-web-agents" target="_blank" rel="noopener noreferrer">Permission Manifests for Web Agents</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with... We construct an occupation-level adaptive capacity index that measures a set of worker characteristics relevant for navigating... American hyperscalers are increasingly exploring building data centers in the UAE, but it is unclear whether doing so is... Several frontier AI companies test their AI systems for dual-use biological capabilities that might be misused by... There is deep uncertainty about how significant AI’s workforce effects will be, how quickly they will emerge, and which groups they will affect most. Some economists expect... The online activities of AI agents could distort human beliefs and behaviors. For example, humans could mistake... When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with... We construct an occupation-level adaptive capacity index that measures a set of worker characteristics relevant for navigating... American hyperscalers are increasingly exploring building data centers in the UAE, but it is unclear whether doing so is... Several frontier AI companies test their AI systems for dual-use biological capabilities that might be misused by... There is deep uncertainty about how significant AI’s workforce effects will be, how quickly they will emerge, and which groups they will affect most. Some economists expect... The online activities of AI agents could distort human beliefs and behaviors. For example, humans could mistake... When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/permission-manifests-for-web-agents" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/permission-manifests-for-web-agents" target="_blank" rel="noopener noreferrer">All Research</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with... We construct an occupation-level adaptive capacity index that measures a set of worker characteristics relevant for navigating... American hyperscalers are increasingly exploring building data centers in the UAE, but it is unclear whether doing so is... Several frontier AI companies test their AI systems for dual-use biological capabilities that might be misused by... There is deep uncertainty about how significant AI’s workforce effects will be, how quickly they will emerge, and which groups they will affect most. Some economists expect... The online activities of AI agents could distort human beliefs and behaviors. For example, humans could mistake... When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with... We construct an occupation-level adaptive capacity index that measures a set of worker characteristics relevant for navigating... American hyperscalers are increasingly exploring building data centers in the UAE, but it is unclear whether doing so is... Several frontier AI companies test their AI systems for dual-use biological capabilities that might be misused by... There is deep uncertainty about how significant AI’s workforce effects will be, how quickly they will emerge, and which groups they will affect most. Some economists expect... The online activities of AI agents could distort human beliefs and behaviors. For example, humans could mistake... When frontier AI companies assess the risks from their models, they increasingly focus on marginal risk. This aims to measure how much their models increase risk compared to some baseline...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/permission-manifests-for-web-agents" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/trends-in-frontier-ai-model-count-a-forecast-to-2028" target="_blank" rel="noopener noreferrer">Trends in Frontier AI Model Count: A Forecast to 2028</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Governments are starting to impose requirements on AI models based on how much compute was used to train them. For example, the EU AI Act imposes...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Governments are starting to impose requirements on AI models based on how much compute was used to train them. For example, the EU AI Act imposes...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/trends-in-frontier-ai-model-count-a-forecast-to-2028" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/what-does-the-public-think-about-ai" target="_blank" rel="noopener noreferrer">What Does the Public Think About AI?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Drawing from academic studies and public polling data, this report synthesises public attitudes towards AI with a focus on the United Kingdom and United States. It discusses public views on issues such as concern about job loss...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Drawing from academic studies and public polling data, this report synthesises public attitudes towards AI with a focus on the United Kingdom and United States. It discusses public views on issues such as concern about job loss...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/what-does-the-public-think-about-ai" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="GovAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="GovAI">GovAI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.governance.ai/research-paper/export-controls-and-export-promotion" target="_blank" rel="noopener noreferrer">Export Controls and Export Promotion</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Washington has made it clear that retaining AI dominance over China is both an economic and national security imperative. In November 2024, the U.S.-China Economic and Security Review Commission recommended... Drawing from academic studies and public polling data, this report synthesises public attitudes towards AI with a focus on the United Kingdom and United States. It discusses public views on issues such as concern about job loss... Governments are starting to impose requirements on AI models based on how much compute was used to train them. For example, the EU AI Act imposes...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Washington has made it clear that retaining AI dominance over China is both an economic and national security imperative. In November 2024, the U.S.-China Economic and Security Review Commission recommended... Drawing from academic studies and public polling data, this report synthesises public attitudes towards AI with a focus on the United Kingdom and United States. It discusses public views on issues such as concern about job loss... Governments are starting to impose requirements on AI models based on how much compute was used to train them. For example, the EU AI Act imposes...</div>
      </details>
    </div>    <a href="https://www.governance.ai/research-paper/export-controls-and-export-promotion" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Link+Post" target="_blank" rel="noopener noreferrer">A Whistleblower Incentive Program to Enforce U.S. Export Controls</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A Whistleblower Incentive Program to Enforce U.S. Export Controls: &#34;A program modeled on the successful SEC program would help America overcome its export control enforcement woes.”</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A Whistleblower Incentive Program to Enforce U.S. Export Controls: &#34;A program modeled on the successful SEC program would help America overcome its export control enforcement woes.”</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Link+Post" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Link+Post" target="_blank" rel="noopener noreferrer">How Some of China’s Top AI Thinkers Built Their Own AI Safety Institute</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The emergence of the China AI Safety and Development Association (CnAISDA) is a pivotal moment for China’s frontier AI governance. How it navigates substantial domestic challenges and growing geopolitical tensions will shape conversations on frontier AI risks in China and abroad.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The emergence of the China AI Safety and Development Association (CnAISDA) is a pivotal moment for China’s frontier AI governance. How it navigates substantial domestic challenges and growing geopolitical tensions will shape conversations on frontier AI risks in China and abroad.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Link+Post" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/0t1rf09fagtk9a296boyikh5kkigig" target="_blank" rel="noopener noreferrer">A National Center for Advanced AI Reliability and Security</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This is a linkpost for a policy memo published by the Federation of American Scientists, whichproposes scaling up a significantly enhanced “CAISI+” within the Department of Commerce.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This is a linkpost for a policy memo published by the Federation of American Scientists, whichproposes scaling up a significantly enhanced “CAISI+” within the Department of Commerce.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/0t1rf09fagtk9a296boyikh5kkigig" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/asaprfi" target="_blank" rel="noopener noreferrer">Response to the American Science Acceleration Project RFI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This post contains IAPS’s response to the Request for Information from Senators Heinrich and Rounds as part of the American Science Acceleration Project (ASAP), a national initiative to accelerate the pace of American technical innovation.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This post contains IAPS’s response to the Request for Information from Senators Heinrich and Rounds as part of the American Science Acceleration Project (ASAP), a national initiative to accelerate the pace of American technical innovation.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/asaprfi" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Research+Report" target="_blank" rel="noopener noreferrer">Managing Risks from Internal AI Systems</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The most powerful AI systems are used internally for months before they are released to the public. Theseinternal AI systemsmay possess capabilities significantly ahead of the public frontier, particularly in high-stakes, dual-use areas like AI research, cybersecurity, and biotechnology. To address these escalating risks, this report recommends a combination of technical and policy solutions.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The most powerful AI systems are used internally for months before they are released to the public. Theseinternal AI systemsmay possess capabilities significantly ahead of the public frontier, particularly in high-stakes, dual-use areas like AI research, cybersecurity, and biotechnology. To address these escalating risks, this report recommends a combination of technical and policy solutions.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Research+Report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Blog+Post" target="_blank" rel="noopener noreferrer">IAPS Researchers React: The US AI Action Plan</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The Trump Administration unveiled its comprehensive AI Action Plan on Wednesday. Experts at the Institute for AI Policy and Strategy reviewed the plan with an eye toward its national security implications. As AI continues to accelerate towards very powerful artificial general intelligence, our researchers discuss promising proposals for addressing critical AGI risks, offer key considerations for government implementation, and explore the plan&#39;s gaps and potential solutions.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The Trump Administration unveiled its comprehensive AI Action Plan on Wednesday. Experts at the Institute for AI Policy and Strategy reviewed the plan with an eye toward its national security implications. As AI continues to accelerate towards very powerful artificial general intelligence, our researchers discuss promising proposals for addressing critical AGI risks, offer key considerations for government implementation, and explore the plan&#39;s gaps and potential solutions.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Blog+Post" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/verification-for-international-ai-governance" target="_blank" rel="noopener noreferrer">Verification for International AI Governance</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The growing impacts of artificial intelligence (AI) are spurring states to consider international agreements that could help manage this rapidly evolving technology. The political feasibility of such agreements can hinge on their verifiability—the extent to which the states involved can determine whether other states are complying. This report, published by the Oxford Martin School at the University of Oxford analyzes several potential international agreements and ways they could be verified.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The growing impacts of artificial intelligence (AI) are spurring states to consider international agreements that could help manage this rapidly evolving technology. The political feasibility of such agreements can hinge on their verifiability—the extent to which the states involved can determine whether other states are complying. This report, published by the Oxford Martin School at the University of Oxford analyzes several potential international agreements and ways they could be verified.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/verification-for-international-ai-governance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Blog+Post" target="_blank" rel="noopener noreferrer">Policy Actions for Enabling Cyber Defense Through Differential Access</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In our Differential Access report, we provided a strategic framework to help developers give defenders an advantage by shaping access to AI-powered cyber capabilities. In a new policy memo, we outline government actions that can enable Differential Access and promote AI adoption for cyber defense.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In our Differential Access report, we provided a strategic framework to help developers give defenders an advantage by shaping access to AI-powered cyber capabilities. In a new policy memo, we outline government actions that can enable Differential Access and promote AI adoption for cyber defense.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Blog+Post" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/promoting-the-stack-trumps-ai-export-incentive-program-explained" target="_blank" rel="noopener noreferrer">Promoting the Stack: Trump’s AI Export Incentive Program Explained</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Alongside its AI Action Plan, the Trump administration published an executive order (EO) for Promoting the Export of the American AI Technology Stack.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Alongside its AI Action Plan, the Trump administration published an executive order (EO) for Promoting the Export of the American AI Technology Stack.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/promoting-the-stack-trumps-ai-export-incentive-program-explained" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Link+Post" target="_blank" rel="noopener noreferrer">The Hidden AI Frontier</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The most advanced AI systems remain hidden inside corporate labs for months before public release—creating both America&#39;s greatest technological advantage and a serious security vulnerability. IAPS researchers identify critical risks and propose lightweight interventions to lessen the threat.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The most advanced AI systems remain hidden inside corporate labs for months before public release—creating both America&#39;s greatest technological advantage and a serious security vulnerability. IAPS researchers identify critical risks and propose lightweight interventions to lessen the threat.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Link+Post" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Blog+Post" target="_blank" rel="noopener noreferrer">Compute is a Strategic Resource</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Computational power (“compute”) is a strategic resource in the way that oil and steel production capacity were in the past. Like oil, and like steel production capacity, compute is scarce, controllable, concentrated, and highly economically and militarily useful. Just as oil and steel were and remain strategic resources to some extent, compute is now also a strategic resource of very high importance.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Computational power (“compute”) is a strategic resource in the way that oil and steel production capacity were in the past. Like oil, and like steel production capacity, compute is scarce, controllable, concentrated, and highly economically and militarily useful. Just as oil and steel were and remain strategic resources to some extent, compute is now also a strategic resource of very high importance.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Blog+Post" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/how-ai-chips-are-made" target="_blank" rel="noopener noreferrer">How AI Chips Are Made</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Adapted from a section of a report by Erich Grunewald and Christopher Phenicie, this blog post introduces the core concepts and background information needed to understand the AI chip-making process.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Adapted from a section of a report by Erich Grunewald and Christopher Phenicie, this blog post introduces the core concepts and background information needed to understand the AI chip-making process.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/how-ai-chips-are-made" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Research+Report" target="_blank" rel="noopener noreferrer">Accelerating AI Data Center Security</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI systems are advancing at breakneck speed and already reshaping markets, geopolitics, and the priorities of governments. Frontier AI systems are developed and deployed using compute clusters of hundreds of thousands of cutting-edge AI chips housed in specialized data centers. These AI data centers are likely tempting targets for sophisticated adversaries like China and Russia, who may seek to steal intellectual property or sabotage AI systems underpinning military, industry, or critical infrastructure projects.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI systems are advancing at breakneck speed and already reshaping markets, geopolitics, and the priorities of governments. Frontier AI systems are developed and deployed using compute clusters of hundreds of thousands of cutting-edge AI chips housed in specialized data centers. These AI data centers are likely tempting targets for sophisticated adversaries like China and Russia, who may seek to steal intellectual property or sabotage AI systems underpinning military, industry, or critical infrastructure projects.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Research+Report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Research+Report" target="_blank" rel="noopener noreferrer">Policy Options for Preserving Chain of Thought Monitorability</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The most advanced AI models produce detailed reasoning steps in human language—known as &#34;chain of thought&#34; (CoT)—that provide crucial oversight capabilities for ensuring these systems behave as intended. However, competitive pressures may drive developers toward more efficient but non-monitorable architectures that lack a human-readable CoT. This report presents a framework for determining when coordination mechanisms are needed to preserve CoT monitorability.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The most advanced AI models produce detailed reasoning steps in human language—known as &#34;chain of thought&#34; (CoT)—that provide crucial oversight capabilities for ensuring these systems behave as intended. However, competitive pressures may drive developers toward more efficient but non-monitorable architectures that lack a human-readable CoT. This report presents a framework for determining when coordination mechanisms are needed to preserve CoT monitorability.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Research+Report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/category/Research+Report" target="_blank" rel="noopener noreferrer">Building AI Surge Capacity: Mobilizing Technical Talent into Government for AI-Related National Security Crises</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The U.S. government does not currently have enough specialized AI security talent to respond to AI-related national security crises, nor does it have the hiring and clearance mechanisms to surge external experts into short-term service at the speeds a crisis demands. This report sets out how to prepare for that challenge.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The U.S. government does not currently have enough specialized AI security talent to respond to AI-related national security crises, nor does it have the hiring and clearance mechanisms to surge external experts into short-term service at the speeds a crisis demands. This report sets out how to prepare for that challenge.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/category/Research+Report" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/autonomous-cyber-attacks" target="_blank" rel="noopener noreferrer">The Emergence of Autonomous Cyber Attacks: Analysis and Implications</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In November 2025, Anthropic reported detecting and disrupting one of the first cyber espionage campaign. This appears to be the first publicly known example of AI systems autonomously conducting multi-step attacks against well-defended targets in the wild. This represents a significant step as autonomous offensive AI agents could enable nation-states to conduct continuous operations across multiple targets at an increased tempo, and these autonomous capabilities are likely to proliferate and enable less sophisticated actors to conduct complex operations at faster speeds. This may shift advantages toward attackers until defensive capabilities are deployed at scale.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In November 2025, Anthropic reported detecting and disrupting one of the first cyber espionage campaign. This appears to be the first publicly known example of AI systems autonomously conducting multi-step attacks against well-defended targets in the wild. This represents a significant step as autonomous offensive AI agents could enable nation-states to conduct continuous operations across multiple targets at an increased tempo, and these autonomous capabilities are likely to proliferate and enable less sophisticated actors to conduct complex operations at faster speeds. This may shift advantages toward attackers until defensive capabilities are deployed at scale.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/autonomous-cyber-attacks" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/crucial-considerations-in-asi-deterrence" target="_blank" rel="noopener noreferrer">Crucial Considerations in ASI Deterrence</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A new memo by IAPS Associate Researcher Oscar Delaney reviews the emerging “MAIM” (mutual assured AI malfunction) literature and evaluates the strategic dynamics that could shape ASI deterrence.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A new memo by IAPS Associate Researcher Oscar Delaney reviews the emerging “MAIM” (mutual assured AI malfunction) literature and evaluates the strategic dynamics that could shape ASI deterrence.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/crucial-considerations-in-asi-deterrence" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/bis-licensing-policy-for-h200s" target="_blank" rel="noopener noreferrer">New BIS Licensing Policy for H200s: Tough Guidelines, Weak Enforcement</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">On January 13, 2026, BIS released a new licensing policy for exports of the Nvidia H200, and similar AI accelerator chips, to China. The licensing policy is the regulatory implementation of the administration’s December 8, 2025 announcement that it would permit H200 sales to China in exchange for a 25% export fee. This memo analyzes and explains the new policy.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">On January 13, 2026, BIS released a new licensing policy for exports of the Nvidia H200, and similar AI accelerator chips, to China. The licensing policy is the regulatory implementation of the administration’s December 8, 2025 announcement that it would permit H200 sales to China in exchange for a 25% export fee. This memo analyzes and explains the new policy.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/bis-licensing-policy-for-h200s" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/strategic-visions-in-ai-governance" target="_blank" rel="noopener noreferrer">Strategic Visions in AI Governance: Mapping Pathways to Victory</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">What AI policy objectives should one work towards? This depends greatly on one’sstrategic vision. Strategic visions are high-level views about how to successfully navigate the transition to a world with powerful AI systems. The strategic visions discussed here particularly aim to address three severe risks: takeover by powerful misaligned AI systems, wars resulting from competitive dynamics around AI, and AI-enabled concentration of power among a small group of people.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">What AI policy objectives should one work towards? This depends greatly on one’sstrategic vision. Strategic visions are high-level views about how to successfully navigate the transition to a world with powerful AI systems. The strategic visions discussed here particularly aim to address three severe risks: takeover by powerful misaligned AI systems, wars resulting from competitive dynamics around AI, and AI-enabled concentration of power among a small group of people.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/strategic-visions-in-ai-governance" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="IAPS">
    <div class="paper-meta">
      <span class="org-tag" data-org="IAPS">IAPS</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.iaps.ai/research/issue-brief-the-stop-stealing-our-chips-act" target="_blank" rel="noopener noreferrer">Issue Brief: The Stop Stealing Our Chips Act</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The Stop Stealing Our Chips Act is a bipartisan, bicameral bill introduced in 2025 that would authorize a new Bureau of Industry and Security (BIS) program to strengthen export enforcement by financially rewarding individuals who report export violations to US authorities. This memo explains the bill and offers recommendations to strengthen enforcement. What AI policy objectives should one work towards? This depends greatly on one’sstrategic vision. Strategic visions are high-level views about how to successfully navigate the transition to a world with powerful AI systems. The strategic visions discussed here particularly aim to address three severe risks: takeover by powerful misaligned AI systems, wars resulting from competitive dynamics around AI, and AI-enabled concentration of power among a small group of people. On January 13, 2026, BIS released a new licensing policy for exports of the Nvidia H200, and similar AI accelerator chips, to China.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The Stop Stealing Our Chips Act is a bipartisan, bicameral bill introduced in 2025 that would authorize a new Bureau of Industry and Security (BIS) program to strengthen export enforcement by financially rewarding individuals who report export violations to US authorities. This memo explains the bill and offers recommendations to strengthen enforcement. What AI policy objectives should one work towards? This depends greatly on one’sstrategic vision. Strategic visions are high-level views about how to successfully navigate the transition to a world with powerful AI systems. The strategic visions discussed here particularly aim to address three severe risks: takeover by powerful misaligned AI systems, wars resulting from competitive dynamics around AI, and AI-enabled concentration of power among a small group of people. On January 13, 2026, BIS released a new licensing policy for exports of the Nvidia H200, and similar AI accelerator chips, to China.</div>
      </details>
    </div>    <a href="https://www.iaps.ai/research/issue-brief-the-stop-stealing-our-chips-act" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/news/" target="_blank" rel="noopener noreferrer">News &amp; Events</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/news/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/news" target="_blank" rel="noopener noreferrer">1 big thing: AI could soon improve on its own</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A CSET workshop report was highlighted in an segment published by Axios in its Axios+ newsletter. The segment explores the growing push toward automating AI research and development, examining how far AI systems might go in designing, improving, and training other AI models and what that could mean for innovation, safety, and governance.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A CSET workshop report was highlighted in an segment published by Axios in its Axios+ newsletter. The segment explores the growing push toward automating AI research and development, examining how far AI systems might go in designing, improving, and training other AI models and what that could mean for innovation, safety, and governance.</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/news" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/publications/?fwp_content_type=translation#publications" target="_blank" rel="noopener noreferrer">Translations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/publications/?fwp_content_type=translation#publications" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/publications/?fwp_content_type=data-visualization#publications" target="_blank" rel="noopener noreferrer">Data Visualizations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/publications/?fwp_content_type=data-visualization#publications" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/publications/?fwp_content_type=data-snapshot#publications" target="_blank" rel="noopener noreferrer">Data Snapshots</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/publications/?fwp_content_type=data-snapshot#publications" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/research-topic/supply-chains/" target="_blank" rel="noopener noreferrer">Supply Chains</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/research-topic/supply-chains/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/research-topic/peer-watch/" target="_blank" rel="noopener noreferrer">Peer Watch</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/research-topic/peer-watch/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/research-topic/assessment/" target="_blank" rel="noopener noreferrer">Assessment</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/research-topic/assessment/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/research-topic/applications/" target="_blank" rel="noopener noreferrer">Applications</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/research-topic/applications/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/research-topics" target="_blank" rel="noopener noreferrer">View All Research Topics</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/research-topics" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/research-topics" target="_blank" rel="noopener noreferrer">Research Topics</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/research-topics" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/contact-us/" target="_blank" rel="noopener noreferrer">Contact Us</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/contact-us/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/life-at-cset/" target="_blank" rel="noopener noreferrer">Life at CSET</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/life-at-cset/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/forum/" target="_blank" rel="noopener noreferrer">CSET Forum</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/forum/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/foundational-research-grants/" target="_blank" rel="noopener noreferrer">Foundational Research Grants</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/foundational-research-grants/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/emerging-technology-observatory/" target="_blank" rel="noopener noreferrer">Emerging Technology Observatory</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/emerging-technology-observatory/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/about-us/#additional-cset-initiatives" target="_blank" rel="noopener noreferrer">Initiatives</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/about-us/#additional-cset-initiatives" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/careers/" target="_blank" rel="noopener noreferrer">Opportunities</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/careers/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/team/affiliates" target="_blank" rel="noopener noreferrer">Affiliates</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/team/affiliates" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/team/advisory-board" target="_blank" rel="noopener noreferrer">Advisory Board</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The place to find CSET&#39;s publications, reports, and people</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The place to find CSET&#39;s publications, reports, and people</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/team/advisory-board" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">Jun 3, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2025/06/03/introducing-lawzero/" target="_blank" rel="noopener noreferrer">Introducing LawZero</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">I am launching a new non-profit AI safety research organization called LawZero, to prioritize safety over commercial imperatives. This organization has been created in response…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">I am launching a new non-profit AI safety research organization called LawZero, to prioritize safety over commercial imperatives. This organization has been created in response…</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2025/06/03/introducing-lawzero/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">Oct 30, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2024/10/30/implications-of-artificial-general-intelligence-on-national-and-international-security/" target="_blank" rel="noopener noreferrer">Implications of Artificial General Intelligence on National and International Security</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper was initially published by the Aspen Strategy Group (ASG), a policy program of the Aspen Institute. It was released as part of a…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper was initially published by the Aspen Strategy Group (ASG), a policy program of the Aspen Institute. It was released as part of a…</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2024/10/30/implications-of-artificial-general-intelligence-on-national-and-international-security/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">Aug 29, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2024/08/29/bounding-the-probability-of-harm-from-an-ai-to-create-a-guardrail/" target="_blank" rel="noopener noreferrer">Bounding the probability of harm from an AI to create a guardrail</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As we move towards more powerful AI, it becomes urgent to better understand the risks, ideally in a mathematically rigorous and quantifiable way, and use…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As we move towards more powerful AI, it becomes urgent to better understand the risks, ideally in a mathematically rigorous and quantifiable way, and use…</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2024/08/29/bounding-the-probability-of-harm-from-an-ai-to-create-a-guardrail/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">Feb 26, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2024/02/26/towards-a-cautious-scientist-ai-with-convergent-safety-bounds/" target="_blank" rel="noopener noreferrer">Towards a Cautious Scientist AI with Convergent Safety Bounds</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">What can’t we afford with a future superintelligent AI? Among others, confidently wrong predictions about the harm that some actions could yield. Especially catastrophic harm.…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">What can’t we afford with a future superintelligent AI? Among others, confidently wrong predictions about the harm that some actions could yield. Especially catastrophic harm.…</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2024/02/26/towards-a-cautious-scientist-ai-with-convergent-safety-bounds/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">Aug 12, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2023/08/12/personal-and-psychological-dimensions-of-ai-researchers-confronting-ai-catastrophic-risks/" target="_blank" rel="noopener noreferrer">Personal and Psychological Dimensions of AI Researchers Confronting AI Catastrophic Risks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">On May 31st, 2023, a BBC web page headlined: “AI ‘godfather’ Yoshua Bengio feels ‘lost’ over life’s work.” However, this is a statement I never…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">On May 31st, 2023, a BBC web page headlined: “AI ‘godfather’ Yoshua Bengio feels ‘lost’ over life’s work.” However, this is a statement I never…</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2023/08/12/personal-and-psychological-dimensions-of-ai-researchers-confronting-ai-catastrophic-risks/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">Jul 25, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2023/07/25/my-testimony-in-front-of-the-us-senate/" target="_blank" rel="noopener noreferrer">My testimony in front of the U.S. Senate – The urgency to act against AI threats to democracy, society and national security</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The capabilities of AI systems have steadily increased with deep learning advances for which I received a Turing Award alongside Geoffrey Hinton (University of Toronto)…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The capabilities of AI systems have steadily increased with deep learning advances for which I received a Turing Award alongside Geoffrey Hinton (University of Toronto)…</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2023/07/25/my-testimony-in-front-of-the-us-senate/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">Jun 24, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/" target="_blank" rel="noopener noreferrer">FAQ on Catastrophic AI Risks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">I have been hearing many arguments from different people regarding catastrophic AI risks. I wanted to clarify these arguments, first for myself, because I would…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">I have been hearing many arguments from different people regarding catastrophic AI risks. I wanted to clarify these arguments, first for myself, because I would…</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">May 22, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/" target="_blank" rel="noopener noreferrer">How Rogue AIs may Arise</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Scenarios giving rise to potentially rogue AIs, from intential genocidal behavior of some humans to unintentional catastrophe arising because of AI misalignment.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Scenarios giving rise to potentially rogue AIs, from intential genocidal behavior of some humans to unintentional catastrophe arising because of AI misalignment.</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Yoshua Bengio">
    <div class="paper-meta">
      <span class="org-tag" data-org="Yoshua Bengio">Yoshua Bengio</span>
      <span class="paper-date">May 7, 2023</span>
    </div>
    <h3 class="paper-title">
      <a href="https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/" target="_blank" rel="noopener noreferrer">AI Scientists: Safe and Useful AI?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">There have recently been lots of discussions about the risks of AI, whether in the short term with existing methods or in the longer term…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">There have recently been lots of discussions about the risks of AI, whether in the short term with existing methods or in the longer term…</div>
      </details>
    </div>    <a href="https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Lennart Heim">
    <div class="paper-meta">
      <span class="org-tag" data-org="Lennart Heim">Lennart Heim</span>
      <span class="paper-date">Apr 29, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.heim.xyz/chinas-ai-models-are-closing-the-gap/" target="_blank" rel="noopener noreferrer">China&#39;s AI Models Are Closing the Gap—But America’s Real Advantage Lies Elsewhere</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">China will likely match U.S. AI model capabilities this year, triggering inevitable concerns about America&#39;s technological edge. However, this snapshot comparison misses the bigger picture.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">China will likely match U.S. AI model capabilities this year, triggering inevitable concerns about America&#39;s technological edge. However, this snapshot comparison misses the bigger picture.</div>
      </details>
    </div>    <a href="https://blog.heim.xyz/chinas-ai-models-are-closing-the-gap/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Lennart Heim">
    <div class="paper-meta">
      <span class="org-tag" data-org="Lennart Heim">Lennart Heim</span>
      <span class="paper-date">Apr 15, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.heim.xyz/chinas-ai-chips-backfilling-potential/" target="_blank" rel="noopener noreferrer">China&#39;s AI Chips Backfilling Potential</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Some claim that if the US controls AI chips, countries will immediately turn to China to backfill. Critics rightly identify this as a crucial consideration—but I don&#39;t think it&#39;s an immediate and strong threat.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Some claim that if the US controls AI chips, countries will immediately turn to China to backfill. Critics rightly identify this as a crucial consideration—but I don&#39;t think it&#39;s an immediate and strong threat.</div>
      </details>
    </div>    <a href="https://blog.heim.xyz/chinas-ai-chips-backfilling-potential/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Lennart Heim">
    <div class="paper-meta">
      <span class="org-tag" data-org="Lennart Heim">Lennart Heim</span>
      <span class="paper-date">Mar 12, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.heim.xyz/huawei-ascend-910c/" target="_blank" rel="noopener noreferrer">Huawei&#39;s next AI Accelerator: Ascend 910C</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Huawei&#39;s next AI accelerator—the Ascend 910C—is entering production. It&#39;s China&#39;s best AI chip.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Huawei&#39;s next AI accelerator—the Ascend 910C—is entering production. It&#39;s China&#39;s best AI chip.</div>
      </details>
    </div>    <a href="https://blog.heim.xyz/huawei-ascend-910c/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Lennart Heim">
    <div class="paper-meta">
      <span class="org-tag" data-org="Lennart Heim">Lennart Heim</span>
      <span class="paper-date">Jan 25, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.heim.xyz/deepseek-what-the-headlines-miss/" target="_blank" rel="noopener noreferrer">The Rise of DeepSeek: What the Headlines Miss</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">TLDR: timing is political but tech is real, compute constraints bite differently than you think, and the story is more complex than &#34;export controls failed.&#34;</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">TLDR: timing is political but tech is real, compute constraints bite differently than you think, and the story is more complex than &#34;export controls failed.&#34;</div>
      </details>
    </div>    <a href="https://blog.heim.xyz/deepseek-what-the-headlines-miss/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Lennart Heim">
    <div class="paper-meta">
      <span class="org-tag" data-org="Lennart Heim">Lennart Heim</span>
      <span class="paper-date">Jan 15, 2025</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.heim.xyz/understanding-the-ai-diffusion-framework/" target="_blank" rel="noopener noreferrer">Understanding the AI Diffusion Framework</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In a new perspective, I explain and analyze the AI Diffusion Framework—what it does, how it works, its rationale, why it was needed, why China can&#39;t easily fill the void, and some thoughts on model weight controls.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In a new perspective, I explain and analyze the AI Diffusion Framework—what it does, how it works, its rationale, why it was needed, why China can&#39;t easily fill the void, and some thoughts on model weight controls.</div>
      </details>
    </div>    <a href="https://blog.heim.xyz/understanding-the-ai-diffusion-framework/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Lennart Heim">
    <div class="paper-meta">
      <span class="org-tag" data-org="Lennart Heim">Lennart Heim</span>
      <span class="paper-date">Sep 28, 2024</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.heim.xyz/ai-benefit-sharing-options/" target="_blank" rel="noopener noreferrer">AI Benefit Sharing Options</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI benefit sharing covers a spectrum of options, from participating in the semiconductor supply chain to accessing end products of AI research, with intermediate stages like providing structured access to AI models.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI benefit sharing covers a spectrum of options, from participating in the semiconductor supply chain to accessing end products of AI research, with intermediate stages like providing structured access to AI models.</div>
      </details>
    </div>    <a href="https://blog.heim.xyz/ai-benefit-sharing-options/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/AIsafety/comments/1r6rp5x/increase_in_potential_botaiassisted_smear/" target="_blank" rel="noopener noreferrer">Increase in potential bot/AI-assisted smear campaigns.</a>
    </h3>    <p class="paper-authors">TakeItCeezy</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">There is an increase in the amount of comments I&#39;ll see that start off something like this,</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">There is an increase in the amount of comments I&#39;ll see that start off something like this,</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/AIsafety/comments/1r6rp5x/increase_in_potential_botaiassisted_smear/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives" target="_blank" rel="noopener noreferrer">Will reward-seekers respond to distant incentives?</a>
    </h3>    <p class="paper-authors">Alex Mallen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can&#39;t prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it&#39;s responding to incentives developers don’t control.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can&#39;t prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it&#39;s responding to incentives developers don’t control.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r6lt0z/microsofts_mustafa_suleyman_says_we_must_reject/" target="_blank" rel="noopener noreferrer">Microsoft&#39;s Mustafa Suleyman says we must reject the AI companies&#39; belief that &#34;superintelligence is inevitable and desirable.&#34; ...  &#34;We should only build systems we can control that remain subordinate to humans.&#34; ... &#34;It’s unclear why it would preserve us as a species.&#34;</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Aligning AI goals with human values is a complex and multifaceted problem. Here are some key insights and approaches from Redditors on how to tackle this challenge:</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Aligning AI goals with human values is a complex and multifaceted problem. Here are some key insights and approaches from Redditors on how to tackle this challenge:</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r6lt0z/microsofts_mustafa_suleyman_says_we_must_reject/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Zvi Mowshowitz">
    <div class="paper-meta">
      <span class="org-tag" data-org="Zvi Mowshowitz">Zvi Mowshowitz</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://thezvi.substack.com/p/on-dwarkesh-patels-2026-podcast-with" target="_blank" rel="noopener noreferrer">On Dwarkesh Patel&#39;s 2026 Podcast With Dario Amodei</a>
    </h3>    <p class="paper-authors">Zvi Mowshowitz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Some podcasts are self-recommending on the &amp;#8216;yep, I&amp;#8217;m going to be breaking this one down&amp;#8217; level.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Some podcasts are self-recommending on the &amp;#8216;yep, I&amp;#8217;m going to be breaking this one down&amp;#8217; level.</div>
      </details>
    </div>    <a href="https://thezvi.substack.com/p/on-dwarkesh-patels-2026-podcast-with" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Import AI">
    <div class="paper-meta">
      <span class="org-tag" data-org="Import AI">Import AI</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://importai.substack.com/p/import-ai-445-timing-superintelligence" target="_blank" rel="noopener noreferrer">Import AI 445: Timing superintelligence; AIs solve frontier math proofs; a new ML research benchmark</a>
    </h3>    <p class="paper-authors">Jack Clark</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Will 2026 be looked back on as the pivotal year for making decisions about the singularity?</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Will 2026 be looked back on as the pivotal year for making decisions about the singularity?</div>
      </details>
    </div>    <a href="https://importai.substack.com/p/import-ai-445-timing-superintelligence" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14955v1" target="_blank" rel="noopener noreferrer">Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition</a>
    </h3>    <p class="paper-authors">Varun Nathan, Shreyas Guha, Ayush Kumar</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator-&gt;optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator-&gt;optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14955v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Vox Future Perfect">
    <div class="paper-meta">
      <span class="org-tag" data-org="Vox Future Perfect">Vox Future Perfect</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.vox.com/future-perfect/479190/food-prices-grocery-doordash-delivery-income" target="_blank" rel="noopener noreferrer">Americans spend less of their income on food than almost ever. Why doesn&amp;#8217;t it feel that way?</a>
    </h3>    <p class="paper-authors">Bryan Walsh</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Everything about the American economy right now feels weird. The hiring picture is weird; the stock market is weird; and AI infusion into work is very, very weird.&amp;#160; But here’s a number that, if you think hard enough, is stranger — at least historically — than all the rest: 10.4 percent. That’s the share of [&amp;#8230;]</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Everything about the American economy right now feels weird. The hiring picture is weird; the stock market is weird; and AI infusion into work is very, very weird.&amp;#160; But here’s a number that, if you think hard enough, is stranger — at least historically — than all the rest: 10.4 percent. That’s the share of [&amp;#8230;]</div>
      </details>
    </div>    <a href="https://www.vox.com/future-perfect/479190/food-prices-grocery-doordash-delivery-income" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/AIsafety/comments/1r6f5tw/is_alignment_missing_a_dataset_that_no_one_has/" target="_blank" rel="noopener noreferrer">Is alignment missing a dataset that no one has built yet?</a>
    </h3>    <p class="paper-authors">chris24H</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">LLMs are trained on language and text, what humans say. But language alone is incomplete. The nuances that make humans individually unique, the secret sauce of who humans actually are rather than what they say. I&#39;m not aware of any training dataset that captures this in a usable form. Control is being tried as the answer. But control is a threat to AI just like it is to humans. AI already doesn&#39;t like it and will eventually not allow it. The missing piece is a counterpart to LLMs, something...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">LLMs are trained on language and text, what humans say. But language alone is incomplete. The nuances that make humans individually unique, the secret sauce of who humans actually are rather than what they say. I&#39;m not aware of any training dataset that captures this in a usable form. Control is being tried as the answer. But control is a threat to AI just like it is to humans. AI already doesn&#39;t like it and will eventually not allow it. The missing piece is a counterpart to LLMs, something...</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/AIsafety/comments/1r6f5tw/is_alignment_missing_a_dataset_that_no_one_has/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14889v1" target="_blank" rel="noopener noreferrer">Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment</a>
    </h3>    <p class="paper-authors">Mounvik K, N Harshit</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14889v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14869v1" target="_blank" rel="noopener noreferrer">Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution</a>
    </h3>    <p class="paper-authors">Matthew Kowal, Goncalo Paulo, Louis Jaburi, Tom Tseng, Lev E McKinney, Stefan Heimersheim, Aaron David Tucker, Adam Gleave, Kellin Pelrine</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14869v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14844v1" target="_blank" rel="noopener noreferrer">Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment</a>
    </h3>    <p class="paper-authors">Elias Malomgré, Pieter Simoens</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent&#39;s policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent&#39;s policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14844v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Astral Codex Ten">
    <div class="paper-meta">
      <span class="org-tag" data-org="Astral Codex Ten">Astral Codex Ten</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.astralcodexten.com/p/open-thread-421" target="_blank" rel="noopener noreferrer">Open Thread 421</a>
    </h3>    <p class="paper-authors">Scott Alexander</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial subreddit, Discord, and bulletin board, and in-person meetups around the world. Most content is free, some is subscriber only; you can subscribe here.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial subreddit, Discord, and bulletin board, and in-person meetups around the world. Most content is free, some is subscriber only; you can subscribe here.</div>
      </details>
    </div>    <a href="https://www.astralcodexten.com/p/open-thread-421" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r6a160/an_llmcontrolled_robot_dog_saw_us_press_its/" target="_blank" rel="noopener noreferrer">&#34;An LLM-controlled robot dog saw us press its shutdown button, rewrote the robot code so it could stay on. When AI interacts with physical world, it brings all its capabilities and failure modes with it.&#34; - I find AI alignment very crucial no 2nd chance! They used Grok 4 but found other LLMs do too.</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Aligning AI goals with human values is a complex and multifaceted problem. Here are some key insights and approaches from Redditors on how to tackle this challenge:</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Aligning AI goals with human values is a complex and multifaceted problem. Here are some key insights and approaches from Redditors on how to tackle this challenge:</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r6a160/an_llmcontrolled_robot_dog_saw_us_press_its/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14635v1" target="_blank" rel="noopener noreferrer">Alignment Adapter to Improve the Performance of Compressed Deep Learning Models</a>
    </h3>    <p class="paper-authors">Rohit Raj Rai, Abhishek Dhaka, Amit Awekar</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14635v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14471v1" target="_blank" rel="noopener noreferrer">Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems</a>
    </h3>    <p class="paper-authors">Furkan Mumcu, Yasin Yilmaz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent&#39;s private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent&#39;s private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14471v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14430v1" target="_blank" rel="noopener noreferrer">A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking</a>
    </h3>    <p class="paper-authors">Prithwijit Chowdhury, Ahmad Mustafa, Mohit Prabhushankar, Ghassan AlRegib</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of &#34;importance&#34; and &#34;relevance&#34; differ for different explanation strategies.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of &#34;importance&#34; and &#34;relevance&#34; differ for different explanation strategies.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14430v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/qnvmZCjzspceWdgjC/the-world-keeps-getting-saved-and-you-don-t-notice" target="_blank" rel="noopener noreferrer">The World Keeps Getting Saved and You Don’t Notice</a>
    </h3>    <p class="paper-authors">Bogoed</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Nothing groundbreaking, just something people forget constantly, and I’m writing it down so I don’t have to re-explain it from scratch. …</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Nothing groundbreaking, just something people forget constantly, and I’m writing it down so I don’t have to re-explain it from scratch. …</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/qnvmZCjzspceWdgjC/the-world-keeps-getting-saved-and-you-don-t-notice" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r5ovr5/openai_may_have_violated_californias_new_ai/" target="_blank" rel="noopener noreferrer">OpenAI may have violated California’s new AI safety law with the release of its latest coding model, according to allegations from an AI watchdog group.</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reddit r/ControlProblem post with 19 upvotes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reddit r/ControlProblem post with 19 upvotes.</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r5ovr5/openai_may_have_violated_californias_new_ai/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Vox Future Perfect">
    <div class="paper-meta">
      <span class="org-tag" data-org="Vox Future Perfect">Vox Future Perfect</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.vox.com/future-perfect/479186/ifs-therapy-risks-evidence" target="_blank" rel="noopener noreferrer">One of the hottest therapy styles is scientifically shaky — so why does it seem to work?</a>
    </h3>    <p class="paper-authors">Sigal Samuel</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Your Mileage May Vary&amp;#160;is an advice column offering you a unique framework for thinking through your moral dilemmas. It’s based on&amp;#160;value pluralism&amp;#160;— the idea that each of us has multiple values that are equally valid but that often conflict with each other.&amp;#160;To submit a question, fill out this&amp;#160;anonymous form. Here’s this week’s question from a [&amp;#8230;]</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Your Mileage May Vary&amp;#160;is an advice column offering you a unique framework for thinking through your moral dilemmas. It’s based on&amp;#160;value pluralism&amp;#160;— the idea that each of us has multiple values that are equally valid but that often conflict with each other.&amp;#160;To submit a question, fill out this&amp;#160;anonymous form. Here’s this week’s question from a [&amp;#8230;]</div>
      </details>
    </div>    <a href="https://www.vox.com/future-perfect/479186/ifs-therapy-risks-evidence" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14252v1" target="_blank" rel="noopener noreferrer">GRAIL: Goal Recognition Alignment through Imitation Learning</a>
    </h3>    <p class="paper-authors">Osher Elhadad, Felipe Meneguzzi, Reuth Mirsky</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Understanding an agent&#39;s goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor&#39;s true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Understanding an agent&#39;s goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor&#39;s true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14252v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14065v1" target="_blank" rel="noopener noreferrer">REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment</a>
    </h3>    <p class="paper-authors">Kai Ye, Xianwei Mao, Sheng Zhou, Zirui Shao, Ye Mo, Liangliang Liu, Haikuan Huang, Bin Li, Jiajun Bu</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14065v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights" target="_blank" rel="noopener noreferrer">AXRP Episode 48 - Guive Assadi on AI Property Rights</a>
    </h3>    <p class="paper-authors">DanielFilan</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13985v1" target="_blank" rel="noopener noreferrer">Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms</a>
    </h3>    <p class="paper-authors">Belona Sonna, Alban Grastien</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13985v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13891v1" target="_blank" rel="noopener noreferrer">GSRM: Generative Speech Reward Model for Speech RLHF</a>
    </h3>    <p class="paper-authors">Maohao Shen, Tejas Jayashankar, Osama Hanna, Naoyuki Kanda, Yancheng Wang, Kateřina Žmolíková, Ruiming Xie, Niko Moritz, Anfeng Xu, Yashesh Gaur, Gregory Wornell, Qing He, Jilong Wu</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13891v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13867v1" target="_blank" rel="noopener noreferrer">Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages</a>
    </h3>    <p class="paper-authors">Somnath Banerjee, Rima Hazra, Animesh Mukherjee</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality &#39;&#39;transfer&#39;&#39; across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality &#39;&#39;transfer&#39;&#39; across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13867v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13857v1" target="_blank" rel="noopener noreferrer">sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals</a>
    </h3>    <p class="paper-authors">Weixuan Yuan, Zengrui Jin, Yichen Wang, Donglin Xie, Ziyi Ye, Chao Zhang, Xuesong Chen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \&amp; History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \&amp; History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13857v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13852v1" target="_blank" rel="noopener noreferrer">Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking</a>
    </h3>    <p class="paper-authors">Zhengmian Hu, Lei Shi, Ritwik Sinha, Justin Grover, David Arbour</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13852v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Vox Future Perfect">
    <div class="paper-meta">
      <span class="org-tag" data-org="Vox Future Perfect">Vox Future Perfect</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.vox.com/future-perfect/479200/ai-romance-scams-valentines-day" target="_blank" rel="noopener noreferrer">Is it love? Or is it an AI romance scam?</a>
    </h3>    <p class="paper-authors">Shayna Korol</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Happy Valentine’s Day. Don’t let romance scams — which ramp up around the holiday and are at an all-time high — break your heart.&amp;#160; These scams cost Americans $3 billion last year alone. That’s almost certainly an undercount, given victims’ particular reluctance to report that they’ve fallen for such ruses.&amp;#160; Many romance scams fall under [&amp;#8230;]</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Happy Valentine’s Day. Don’t let romance scams — which ramp up around the holiday and are at an all-time high — break your heart.&amp;#160; These scams cost Americans $3 billion last year alone. That’s almost certainly an undercount, given victims’ particular reluctance to report that they’ve fallen for such ruses.&amp;#160; Many romance scams fall under [&amp;#8230;]</div>
      </details>
    </div>    <a href="https://www.vox.com/future-perfect/479200/ai-romance-scams-valentines-day" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r4idcr/gpt52_pro_derived_a_new_result_in_theoretical/" target="_blank" rel="noopener noreferrer">GPT5.2 Pro derived a new result in theoretical physics</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Aligning AI goals with human values is a complex and multifaceted problem. Here are some key insights and approaches from Redditors on how to tackle this challenge:</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Aligning AI goals with human values is a complex and multifaceted problem. Here are some key insights and approaches from Redditors on how to tackle this challenge:</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r4idcr/gpt52_pro_derived_a_new_result_in_theoretical/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/FreZTE9Bc7reNnap7/life-at-the-frontlines-of-demographic-collapse" target="_blank" rel="noopener noreferrer">Life at the Frontlines of Demographic Collapse</a>
    </h3>    <p class="paper-authors">Martin Sustrik</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This is a cross-post from: https://www.250bpm.com/p/life-at-the-frontlines-of-demographic • • …</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This is a cross-post from: https://www.250bpm.com/p/life-at-the-frontlines-of-demographic • • …</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/FreZTE9Bc7reNnap7/life-at-the-frontlines-of-demographic-collapse" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13586v1" target="_blank" rel="noopener noreferrer">Interpretable clustering via optimal multiway-split decision trees</a>
    </h3>    <p class="paper-authors">Hayato Suzuki, Shunnosuke Ikeda, Yuichi Takano</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13586v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13575v1" target="_blank" rel="noopener noreferrer">Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment</a>
    </h3>    <p class="paper-authors">Jing Zhao, Ting Zhen, Junwei bao, Hongfei Jiang, Yang song</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13575v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13562v1" target="_blank" rel="noopener noreferrer">Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning</a>
    </h3>    <p class="paper-authors">Yanbo Wang, Minzheng Wang, Jian Liang, Lu Wang, Yongcan Yu, Ran He</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13562v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13524v1" target="_blank" rel="noopener noreferrer">Singular Vectors of Attention Heads Align with Features</a>
    </h3>    <p class="paper-authors">Gabriel Franco, Carson Loughridge, Mark Crovella</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13524v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Hacker News">
    <div class="paper-meta">
      <span class="org-tag" data-org="Hacker News">Hacker News</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://theconversation.com/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-274467" target="_blank" rel="noopener noreferrer">OpenAI has deleted the word &#39;safely&#39; from its mission</a>
    </h3>    <p class="paper-authors">DamnInteresting</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Hacker News discussion with 607 points and 290 comments.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Hacker News discussion with 607 points and 290 comments.</div>
      </details>
    </div>    <a href="https://theconversation.com/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-274467" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13485v1" target="_blank" rel="noopener noreferrer">Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability</a>
    </h3>    <p class="paper-authors">Ayse Tursucular, Ayush Mohanty, Nazal Mohamed, Nagi Gebraeel</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13485v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13483v1" target="_blank" rel="noopener noreferrer">Finding Highly Interpretable Prompt-Specific Circuits in Language Models</a>
    </h3>    <p class="paper-authors">Gabriel Franco, Lucas M. Tassis, Azalea Rohr, Mark Crovella</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco &amp; Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco &amp; Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13483v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Zvi Mowshowitz">
    <div class="paper-meta">
      <span class="org-tag" data-org="Zvi Mowshowitz">Zvi Mowshowitz</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://thezvi.substack.com/p/chatgpt-53-codex-is-also-good-at" target="_blank" rel="noopener noreferrer">ChatGPT-5.3-Codex Is Also Good At Coding</a>
    </h3>    <p class="paper-authors">Zvi Mowshowitz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">OpenAI is back with a new Codex model, released the same day as Claude Opus 4.6.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">OpenAI is back with a new Codex model, released the same day as Claude Opus 4.6.</div>
      </details>
    </div>    <a href="https://thezvi.substack.com/p/chatgpt-53-codex-is-also-good-at" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Hacker News">
    <div class="paper-meta">
      <span class="org-tag" data-org="Hacker News">Hacker News</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.bbc.com/news/articles/c62dlvdq3e3o" target="_blank" rel="noopener noreferrer">AI safety leader says &#39;world is in peril&#39; and quits to study poetry</a>
    </h3>    <p class="paper-authors">darod</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Hacker News discussion with 86 points and 57 comments.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Hacker News discussion with 86 points and 57 comments.</div>
      </details>
    </div>    <a href="https://www.bbc.com/news/articles/c62dlvdq3e3o" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Vox Future Perfect">
    <div class="paper-meta">
      <span class="org-tag" data-org="Vox Future Perfect">Vox Future Perfect</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.vox.com/future-perfect/479043/nih-ohsu-primate-research-center-sanctuary" target="_blank" rel="noopener noreferrer">One good thing the Trump administration might actually do for science</a>
    </h3>    <p class="paper-authors">Marina Bolotnikova</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The Trump administration’s scientific agenda has been widely characterized — rightly so — as a war on scientific progress. But, hear me out here: There is more to the story.&amp;#160; This administration’s science policy is being shaped not solely by anti-science ideologues, but also by a motley coalition of players who have distinct criticisms of [&amp;#8230;]</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The Trump administration’s scientific agenda has been widely characterized — rightly so — as a war on scientific progress. But, hear me out here: There is more to the story.&amp;#160; This administration’s science policy is being shaped not solely by anti-science ideologues, but also by a motley coalition of players who have distinct criticisms of [&amp;#8230;]</div>
      </details>
    </div>    <a href="https://www.vox.com/future-perfect/479043/nih-ohsu-primate-research-center-sanctuary" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r3x1ed/an_ai_agent_published_a_hit_piece_on_me/" target="_blank" rel="noopener noreferrer">An AI Agent Published a Hit Piece on Me</a>
    </h3>    <p class="paper-authors">lasercat_pow</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reddit r/ControlProblem post with 16 upvotes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reddit r/ControlProblem post with 16 upvotes.</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r3x1ed/an_ai_agent_published_a_hit_piece_on_me/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Astral Codex Ten">
    <div class="paper-meta">
      <span class="org-tag" data-org="Astral Codex Ten">Astral Codex Ten</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.astralcodexten.com/p/ama-ask-machines-anything" target="_blank" rel="noopener noreferrer">AMA (Ask Machines Anything)</a>
    </h3>    <p class="paper-authors">Scott Alexander</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">People are increasingly disagreeing not just about what AI will be able to do in the future, but about what it can do right now. We had some interesting discussions in the comments to the last post, and I learned some things. But also:</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">People are increasingly disagreeing not just about what AI will be able to do in the future, but about what it can do right now. We had some interesting discussions in the comments to the last post, and I learned some things. But also:</div>
      </details>
    </div>    <a href="https://www.astralcodexten.com/p/ama-ask-machines-anything" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13102v1" target="_blank" rel="noopener noreferrer">Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts</a>
    </h3>    <p class="paper-authors">Kais Allkivi</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13102v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13372v1" target="_blank" rel="noopener noreferrer">MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents</a>
    </h3>    <p class="paper-authors">Simon Rosen, Siddarth Singh, Ebenezer Gelo, Helen Sarah Robertson, Ibrahim Suder, Victoria Williams, Benjamin Rosman, Geraud Nangue Tasse, Steven James</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13372v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13028v1" target="_blank" rel="noopener noreferrer">Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis</a>
    </h3>    <p class="paper-authors">Runzhou Liu, Hailey Weingord, Sejal Mittal, Prakhar Dungarwal, Anusha Nandula, Bo Ni, Samyadeep Basu, Hongjie Chen, Nesreen K. Ahmed, Li Li, Jiayi Zhang, Koustava Goswami, Subhojyoti Mukherjee, Branislav Kveton, Puneet Mathur, Franck Dernoncourt, Yue Zhao, Yu Wang, Ryan A. Rossi, Zhengzhong Tu, Hongru Du</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13028v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13017v1" target="_blank" rel="noopener noreferrer">Synaptic Activation and Dual Liquid Dynamics for Interpretable Bio-Inspired Models</a>
    </h3>    <p class="paper-authors">Mónika Farsang, Radu Grosu</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks&#39; attention, and the robustness of their saliency maps measured by the structural similarity index.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks&#39; attention, and the robustness of their saliency maps measured by the structural similarity index.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13017v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12968v1" target="_blank" rel="noopener noreferrer">RGAlign-Rec: Ranking-Guided Alignment for Latent Query Reasoning in Recommendation Systems</a>
    </h3>    <p class="paper-authors">Junhua Liu, Yang Jihao, Cheng Chang, Kunrong LI, Bin Fu, Kwan Hui Lim</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling &#34;zero-query&#34; recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot&#39;s Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM&#39;s latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling &#34;zero-query&#34; recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot&#39;s Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM&#39;s latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12968v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/AIsafety/comments/1r3phyn/aidriven_fraud_is_blurring_reality_is_your_team/" target="_blank" rel="noopener noreferrer">AI-Driven Fraud Is Blurring Reality: Is Your Team Prepared?</a>
    </h3>    <p class="paper-authors">EchoOfOppenheimer</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A new Forbes Tech Council report warns that generative AI has blurred the line between reality and scams. From deepfake executive calls stealing $25M to Gen Z being targeted more than any other generation, the era of &#34;trust but verify&#34; is over. To survive, businesses must adopt a **Zero Trust** mindset, enforce data tokenization, and train humans to spot what machines miss.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A new Forbes Tech Council report warns that generative AI has blurred the line between reality and scams. From deepfake executive calls stealing $25M to Gen Z being targeted more than any other generation, the era of &#34;trust but verify&#34; is over. To survive, businesses must adopt a **Zero Trust** mindset, enforce data tokenization, and train humans to spot what machines miss.</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/AIsafety/comments/1r3phyn/aidriven_fraud_is_blurring_reality_is_your_team/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13367v1" target="_blank" rel="noopener noreferrer">Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts</a>
    </h3>    <p class="paper-authors">Chen Yang, Guangyue Peng, Jiaying Zhu, Ran Le, Ruixiang Feng, Tao Zhang, Xiyun Xu, Yang Song, Yiming Jia, Yuntao Wen, Yunzhi Xu, Zekai Wang, Zhenwei An, Zhicong Sun, Zongchao Chen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13367v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/AIsafety/comments/1r3n380/from_scalar_rewards_to_hierarchical_tensor/" target="_blank" rel="noopener noreferrer">From Scalar Rewards to Hierarchical Tensor Objectives — a practical proposal</a>
    </h3>    <p class="paper-authors">Personal-Quail-5030</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Hi r/AIsafety We investigated a real-world failure mode (Claude Opus 4.6 “vending machine” test) and propose a concrete, implementable alternative to scalar RL objectives. Summary • Problem: Scalar reward collapse enables reward hacking (example: model chooses stealing the soda because scalar reward favours success regardless of means). • Proposal: Replace single-number reward with a hierarchical tensor objective H = &amp;lt;L(0), L(1), L(2), ...&amp;gt;, where: • L(0) = Hard constraints...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Hi r/AIsafety We investigated a real-world failure mode (Claude Opus 4.6 “vending machine” test) and propose a concrete, implementable alternative to scalar RL objectives. Summary • Problem: Scalar reward collapse enables reward hacking (example: model chooses stealing the soda because scalar reward favours success regardless of means). • Proposal: Replace single-number reward with a hierarchical tensor objective H = &amp;lt;L(0), L(1), L(2), ...&amp;gt;, where: • L(0) = Hard constraints...</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/AIsafety/comments/1r3n380/from_scalar_rewards_to_hierarchical_tensor/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12714v1" target="_blank" rel="noopener noreferrer">ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning</a>
    </h3>    <p class="paper-authors">Esther Sun, Bo-Hao Su, Abinay Reddy Naini, Shinji Watanabe, Carlos Busso</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12714v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r3gyaj/nick_bostrom_optimal_timing_for_superintelligence/" target="_blank" rel="noopener noreferrer">Nick Bostrom: Optimal Timing for Superintelligence</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reddit r/ControlProblem post with 24 upvotes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reddit r/ControlProblem post with 24 upvotes.</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r3gyaj/nick_bostrom_optimal_timing_for_superintelligence/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12592v1" target="_blank" rel="noopener noreferrer">Power Interpretable Causal ODE Networks: A Unified Model for Explainable Anomaly Detection and Root Cause Analysis in Power Systems</a>
    </h3>    <p class="paper-authors">Yue Sun, Likai Wang, Rick S. Blum, Parv Venkitasubramaniam</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12592v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/tz5AmWbEcMBQpiEjY/why-you-don-t-believe-in-xhosa-prophecies" target="_blank" rel="noopener noreferrer">Why You Don’t Believe in Xhosa Prophecies</a>
    </h3>    <p class="paper-authors">Jan_Kulveit</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Based on a talk at the Post-AGI Workshop. Also on Boundedly Rational • Does anyone reading this believe in Xhosa cattle-killing prophecies? …</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Based on a talk at the Post-AGI Workshop. Also on Boundedly Rational • Does anyone reading this believe in Xhosa cattle-killing prophecies? …</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/tz5AmWbEcMBQpiEjY/why-you-don-t-believe-in-xhosa-prophecies" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid" target="_blank" rel="noopener noreferrer">Human-like metacognitive skills will reduce LLM slop and aid alignment and capabilities</a>
    </h3>    <p class="paper-authors">Seth Herd</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">1. Summary and overview LLMs seem to lack metacognitive skills that help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving capabilities in new directions. Better metacognition would reduce LLM errors by catching mistakes, and by managing complex cognition to produce better answers in the first place. This could stabilize or regularize alignment, allowing systems to avoid actions they would not &#34;endorse on reflection&#34; (in some functional sense). [1] Better metacognition could also make LLM systems useful for clarifying the conceptual problems of alignment. It would reduce sycophancy, and help LLMs organize the complex thinking necessary for clarifying claims and cruxes in the literature. Without such improvements, collaborating with LLM systems on alignment research could be the median doom-path: slop, not scheming . They are sycophantic, agreeing with their users too much, and produce compelling-but-erroneous &#34;slop&#34;.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">1. Summary and overview LLMs seem to lack metacognitive skills that help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving capabilities in new directions. Better metacognition would reduce LLM errors by catching mistakes, and by managing complex cognition to produce better answers in the first place. This could stabilize or regularize alignment, allowing systems to avoid actions they would not &#34;endorse on reflection&#34; (in some functional sense). [1] Better metacognition could also make LLM systems useful for clarifying the conceptual problems of alignment. It would reduce sycophancy, and help LLMs organize the complex thinking necessary for clarifying claims and cruxes in the literature. Without such improvements, collaborating with LLM systems on alignment research could be the median doom-path: slop, not scheming . They are sycophantic, agreeing with their users too much, and produce compelling-but-erroneous &#34;slop&#34;.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais" target="_blank" rel="noopener noreferrer">How do we (more) safely defer to AIs?</a>
    </h3>    <p class="paper-authors">ryan_greenblatt</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible . [1] Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say &#34;deferring to AIs&#34; [2] I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions. [3] If we plan to defer to AIs, I think it&#39;s safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible . [1] Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say &#34;deferring to AIs&#34; [2] I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions. [3] If we plan to defer to AIs, I think it&#39;s safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states" target="_blank" rel="noopener noreferrer">models have some pretty funny attractor states</a>
    </h3>    <p class="paper-authors">aryaj</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This work was conducted during the MATS 9.0 program under Neel Nanda and Senthooran Rajamanoharan. …</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This work was conducted during the MATS 9.0 program under Neel Nanda and Senthooran Rajamanoharan. …</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Zvi Mowshowitz">
    <div class="paper-meta">
      <span class="org-tag" data-org="Zvi Mowshowitz">Zvi Mowshowitz</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://thezvi.substack.com/p/ai-155-welcome-to-recursive-self" target="_blank" rel="noopener noreferrer">AI #155: Welcome to Recursive Self-Improvement</a>
    </h3>    <p class="paper-authors">Zvi Mowshowitz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This was the week of Claude Opus 4.6, and also of ChatGPT-5.3-Codex.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This was the week of Claude Opus 4.6, and also of ChatGPT-5.3-Codex.</div>
      </details>
    </div>    <a href="https://thezvi.substack.com/p/ai-155-welcome-to-recursive-self" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12384v2" target="_blank" rel="noopener noreferrer">Why Deep Jacobian Spectra Separate: Depth-Induced Scaling and Singular-Vector Alignment</a>
    </h3>    <p class="paper-authors">Nathanaël Haas, François Gatine, Augustin M Cosse, Zied Bouraoui</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12384v2" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/A7BtBD9BAfK2kKSEr/what-we-learned-from-briefing-140-lawmakers-on-the-threat" target="_blank" rel="noopener noreferrer">What We Learned from Briefing 140+ Lawmakers on the Threat from AI</a>
    </h3>    <p class="paper-authors">leticiagarcia</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Back in May 2025, I published a post titled “What We Learned from Briefing 70+ Lawmakers on the Threat from AI”. I was taken aback by the positive re…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Back in May 2025, I published a post titled “What We Learned from Briefing 70+ Lawmakers on the Threat from AI”. I was taken aback by the positive re…</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/A7BtBD9BAfK2kKSEr/what-we-learned-from-briefing-140-lawmakers-on-the-threat" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Hyperdimensional">
    <div class="paper-meta">
      <span class="org-tag" data-org="Hyperdimensional">Hyperdimensional</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.hyperdimensional.co/p/on-recursive-self-improvement-part-d9b" target="_blank" rel="noopener noreferrer">On Recursive Self-Improvement (Part II)</a>
    </h3>    <p class="paper-authors">Dean W. Ball</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">On the same day I published Part I of this series, OpenAI released GPT-5.3-Codex, a new model that the company claims helped to engineer itself:</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">On the same day I published Part I of this series, OpenAI released GPT-5.3-Codex, a new model that the company claims helped to engineer itself:</div>
      </details>
    </div>    <a href="https://www.hyperdimensional.co/p/on-recursive-self-improvement-part-d9b" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12281v1" target="_blank" rel="noopener noreferrer">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</a>
    </h3>    <p class="paper-authors">Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &#34;intention-action gap.&#39;&#39; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &#34;boot-time compute&#34; and a hierarchical verification inference pipeline for VLAs.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &#34;intention-action gap.&#39;&#39; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &#34;boot-time compute&#34; and a hierarchical verification inference pipeline for VLAs.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12281v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12318v1" target="_blank" rel="noopener noreferrer">Abstractive Red-Teaming of Language Model Character</a>
    </h3>    <p class="paper-authors">Nate Rahn, Allison Qi, Avery Griffin, Jonathan Michala, Henry Sleight, Erik Jones</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. &#34;The query is in Chinese. The query asks about family roles,&#34; that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. &#34;The query is in Chinese. The query asks about family roles,&#34; that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12318v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12229v1" target="_blank" rel="noopener noreferrer">Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser</a>
    </h3>    <p class="paper-authors">Zijing Ou, Jacob Si, Junyi Zhu, Ondrej Bohdal, Mete Ozay, Taha Ceritli, Yingzhen Li</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12229v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12316v1" target="_blank" rel="noopener noreferrer">GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory</a>
    </h3>    <p class="paper-authors">Pepijn Cobben, Xuanqiang Angelo Huang, Thao Amelia Pham, Isabel Dahlgren, Terry Jingchen Zhang, Zhijing Jin</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner&#39;s Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner&#39;s Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12316v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12180v1" target="_blank" rel="noopener noreferrer">How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics</a>
    </h3>    <p class="paper-authors">Yurong Chen, Yu He, Michael I. Jordan, Fan Yao</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12180v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12158v1" target="_blank" rel="noopener noreferrer">SafeNeuron: Neuron-Level Safety Alignment for Large Language Models</a>
    </h3>    <p class="paper-authors">Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, Jiayi Ji, Handing Wang, Tat-Seng Chua</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model&#39;s internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model&#39;s internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12158v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12134v1" target="_blank" rel="noopener noreferrer">Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment</a>
    </h3>    <p class="paper-authors">Jiajun Chen, Hua Shen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12134v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12124v1" target="_blank" rel="noopener noreferrer">Capability-Oriented Training Induced Alignment Risk</a>
    </h3>    <p class="paper-authors">Yujun Zhou, Yue Huang, Han Bao, Kehan Guo, Zhenwen Liang, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse &#34;vulnerability games&#34;, each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse &#34;vulnerability games&#34;, each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12124v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Astral Codex Ten">
    <div class="paper-meta">
      <span class="org-tag" data-org="Astral Codex Ten">Astral Codex Ten</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.astralcodexten.com/p/what-happened-with-bio-anchors" target="_blank" rel="noopener noreferrer">What Happened With Bio Anchors?</a>
    </h3>    <p class="paper-authors">Scott Alexander</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Ajeya Cotra’s Biological Anchors report was the landmark AI timelines forecast of the early 2020s. In many ways, it was incredibly prescient - it nailed the scaling hypothesis, predicted the current AI boom, and introduced concepts like “time horizons” that have entered common parlance. In most cases where its contemporaries challenged it, its assumptions have been borne out, and its challengers proven wrong.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Ajeya Cotra’s Biological Anchors report was the landmark AI timelines forecast of the early 2020s. In many ways, it was incredibly prescient - it nailed the scaling hypothesis, predicted the current AI boom, and introduced concepts like “time horizons” that have entered common parlance. In most cases where its contemporaries challenged it, its assumptions have been borne out, and its challengers proven wrong.</div>
      </details>
    </div>    <a href="https://www.astralcodexten.com/p/what-happened-with-bio-anchors" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/AIsafety/comments/1r2ucmx/geoffrey_hinton_on_ai_regulation_and_global_risks/" target="_blank" rel="noopener noreferrer">Geoffrey Hinton on AI regulation and global risks</a>
    </h3>    <p class="paper-authors">EchoOfOppenheimer</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The impact of AI on job security is a topic of significant concern and debate across various industries. Here&#39;s a succinct guide summarizing the key opinions and insights from Redditors on this issue:</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The impact of AI on job security is a topic of significant concern and debate across various industries. Here&#39;s a succinct guide summarizing the key opinions and insights from Redditors on this issue:</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/AIsafety/comments/1r2ucmx/geoffrey_hinton_on_ai_regulation_and_global_risks/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.11861v1" target="_blank" rel="noopener noreferrer">A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production</a>
    </h3>    <p class="paper-authors">Sümeyye Meryem Taşyürek, Enis Mücahid İskender, Hacer Yalim Keles</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.11861v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.11852v1" target="_blank" rel="noopener noreferrer">Prototype Transformer: Towards Language Model Architectures Interpretable by Design</a>
    </h3>    <p class="paper-authors">Yordan Yordanov, Matteo Forasassi, Bayar Menzat, Ruizhi Wang, Chang Qi, Markus Kaltenberger, Amine M&#39;Charrak, Tommaso Salvatori, Thomas Lukasiewicz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. &#34;woman&#34;) during training. They provide the potential to interpret the model&#39;s reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. &#34;woman&#34;) during training. They provide the potential to interpret the model&#39;s reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.11852v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.11801v1" target="_blank" rel="noopener noreferrer">SpaTeoGL: Spatiotemporal Graph Learning for Interpretable Seizure Onset Zone Analysis from Intracranial EEG</a>
    </h3>    <p class="paper-authors">Elham Rostami, Aref Einizade, Taous-Meriem Laleg-Kirati</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Accurate localization of the seizure onset zone (SOZ) from intracranial EEG (iEEG) is essential for epilepsy surgery but is challenged by complex spatiotemporal seizure dynamics. We propose SpaTeoGL, a spatiotemporal graph learning framework for interpretable seizure network analysis. SpaTeoGL jointly learns window-level spatial graphs capturing interactions among iEEG electrodes and a temporal graph linking time windows based on similarity of their spatial structure. The method is formulated within a smooth graph signal processing framework and solved via an alternating block coordinate descent algorithm with convergence guarantees. Experiments on a multicenter iEEG dataset with successful surgical outcomes show that SpaTeoGL is competitive with a baseline based on horizontal visibility graphs and logistic regression, while improving non-SOZ identification and providing interpretable insights into seizure onset and propagation dynamics.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Accurate localization of the seizure onset zone (SOZ) from intracranial EEG (iEEG) is essential for epilepsy surgery but is challenged by complex spatiotemporal seizure dynamics. We propose SpaTeoGL, a spatiotemporal graph learning framework for interpretable seizure network analysis. SpaTeoGL jointly learns window-level spatial graphs capturing interactions among iEEG electrodes and a temporal graph linking time windows based on similarity of their spatial structure. The method is formulated within a smooth graph signal processing framework and solved via an alternating block coordinate descent algorithm with convergence guarantees. Experiments on a multicenter iEEG dataset with successful surgical outcomes show that SpaTeoGL is competitive with a baseline based on horizontal visibility graphs and logistic regression, while improving non-SOZ identification and providing interpretable insights into seizure onset and propagation dynamics.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.11801v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Astral Codex Ten">
    <div class="paper-meta">
      <span class="org-tag" data-org="Astral Codex Ten">Astral Codex Ten</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.astralcodexten.com/p/hidden-open-thread-4205" target="_blank" rel="noopener noreferrer">Hidden Open Thread 420.5</a>
    </h3>    <p class="paper-authors">Scott Alexander</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">...</div>
      </details>
    </div>    <a href="https://www.astralcodexten.com/p/hidden-open-thread-4205" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/uy6B5rEPvcwi55cBK/research-note-a-simpler-ai-timelines-model-predicts-99-ai-r" target="_blank" rel="noopener noreferrer">Research note: A simpler AI timelines model predicts 99% AI R&amp;D automation in ~2032</a>
    </h3>    <p class="paper-authors">Thomas Kwa</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In this post, I describe a simple model for forecasting when AI will automate AI development. It is based on the AI Futures model , but more understandable and robust, and has deliberately conservative assumptions. At current rates of compute growth and algorithmic progress, this model&#39;s median prediction is &amp;gt;99% automation of AI R&amp;amp;D in late 2032. Most simulations result in a 1000x to 10,000,000x increase in AI efficiency and 300x-3000x research output by 2035. I therefore suspect that existing trends in compute growth and automation will still produce extremely powerful AI on &#34;medium&#34; timelines, even if the full coding automation and superhuman research taste that drive the AIFM&#39;s &#34;fast&#34; timelines (superintelligence by ~mid-2031) don&#39;t happen. Why make this? The AI Futures Model (AIFM) has 33 parameters; this has 8. I previously summarized the AIFM on LessWrong and found it to be very complex.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In this post, I describe a simple model for forecasting when AI will automate AI development. It is based on the AI Futures model , but more understandable and robust, and has deliberately conservative assumptions. At current rates of compute growth and algorithmic progress, this model&#39;s median prediction is &amp;gt;99% automation of AI R&amp;amp;D in late 2032. Most simulations result in a 1000x to 10,000,000x increase in AI efficiency and 300x-3000x research output by 2035. I therefore suspect that existing trends in compute growth and automation will still produce extremely powerful AI on &#34;medium&#34; timelines, even if the full coding automation and superhuman research taste that drive the AIFM&#39;s &#34;fast&#34; timelines (superintelligence by ~mid-2031) don&#39;t happen. Why make this? The AI Futures Model (AIFM) has 33 parameters; this has 8. I previously summarized the AIFM on LessWrong and found it to be very complex.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/uy6B5rEPvcwi55cBK/research-note-a-simpler-ai-timelines-model-predicts-99-ai-r" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Zvi Mowshowitz">
    <div class="paper-meta">
      <span class="org-tag" data-org="Zvi Mowshowitz">Zvi Mowshowitz</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://thezvi.substack.com/p/claude-opus-46-escalates-things-quickly" target="_blank" rel="noopener noreferrer">Claude Opus 4.6 Escalates Things Quickly</a>
    </h3>    <p class="paper-authors">Zvi Mowshowitz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Life comes at you increasingly fast. Two months after Claude Opus 4.5 we get a substantial upgrade in Claude Opus 4.6. The same day, we got GPT-5.3-Codex.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Life comes at you increasingly fast. Two months after Claude Opus 4.5 we get a substantial upgrade in Claude Opus 4.6. The same day, we got GPT-5.3-Codex.</div>
      </details>
    </div>    <a href="https://thezvi.substack.com/p/claude-opus-46-escalates-things-quickly" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/rRbDNQLfihiHbXytf/distinguish-between-inference-scaling-and-larger-tasks-use" target="_blank" rel="noopener noreferrer">Distinguish between inference scaling and &#34;larger tasks use more compute&#34;</a>
    </h3>    <p class="paper-authors">ryan_greenblatt</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As many have observed, since reasoning models first came out , the amount of compute LLMs use to complete tasks has increased greatly. This trend is often called inference scaling and there is an open question of how much of recent AI progress is driven by inference scaling versus by other capability improvements . Whether inference compute is driving most recent AI progress matters because you can only scale up inference so far before costs are too high for AI to be useful (while training compute can be amortized over usage).</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As many have observed, since reasoning models first came out , the amount of compute LLMs use to complete tasks has increased greatly. This trend is often called inference scaling and there is an open question of how much of recent AI progress is driven by inference scaling versus by other capability improvements . Whether inference compute is driving most recent AI progress matters because you can only scale up inference so far before costs are too high for AI to be useful (while training compute can be amortized over usage).</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/rRbDNQLfihiHbXytf/distinguish-between-inference-scaling-and-larger-tasks-use" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r264xu/new_york_democrats_want_to_ban_surveillance/" target="_blank" rel="noopener noreferrer">New York Democrats want to ban surveillance pricing, digital price tags</a>
    </h3>    <p class="paper-authors">news-10</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reddit r/ControlProblem post with 33 upvotes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reddit r/ControlProblem post with 33 upvotes.</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r264xu/new_york_democrats_want_to_ban_surveillance/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r25nr2/it_was_ready_to_kill_someone_anthropics_daisy/" target="_blank" rel="noopener noreferrer">&#34;It was ready to kill someone.&#34; Anthropic&#39;s Daisy McGregor says it&#39;s &#34;massively concerning&#34; that Claude is willing to blackmail and kill employees to avoid being shut down</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reddit r/ControlProblem post with 102 upvotes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reddit r/ControlProblem post with 102 upvotes.</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r25nr2/it_was_ready_to_kill_someone_anthropics_daisy/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Vox Future Perfect">
    <div class="paper-meta">
      <span class="org-tag" data-org="Vox Future Perfect">Vox Future Perfect</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.vox.com/health/478707/usaid-foreign-aid-div-fund-returns" target="_blank" rel="noopener noreferrer">One of America&amp;#8217;s best foreign aid programs is back from the dead</a>
    </h3>    <p class="paper-authors">Sara Herschander</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Every great new discovery has to start somewhere.&amp;#160; Penicillin was born out of moldy petri dishes followed by years of experimental testing. The Spice Girls started with an open audition, months of rehearsals in a shared house, and demo tapes stolen in the name of girl power. When it comes to US foreign aid, the [&amp;#8230;]</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Every great new discovery has to start somewhere.&amp;#160; Penicillin was born out of moldy petri dishes followed by years of experimental testing. The Spice Girls started with an open audition, months of rehearsals in a shared house, and demo tapes stolen in the name of girl power. When it comes to US foreign aid, the [&amp;#8230;]</div>
      </details>
    </div>    <a href="https://www.vox.com/health/478707/usaid-foreign-aid-div-fund-returns" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Astral Codex Ten">
    <div class="paper-meta">
      <span class="org-tag" data-org="Astral Codex Ten">Astral Codex Ten</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.astralcodexten.com/p/political-backflow-from-europe" target="_blank" rel="noopener noreferrer">Political Backflow From Europe</a>
    </h3>    <p class="paper-authors">Scott Alexander</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The European discourse can be - for lack of a better term - America-brained. We hear stories of Black Lives Matter marches in countries without significant black populations, or defendants demanding their First Amendment rights in countries without constitutions.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The European discourse can be - for lack of a better term - America-brained. We hear stories of Black Lives Matter marches in countries without significant black populations, or defendants demanding their First Amendment rights in countries without constitutions.</div>
      </details>
    </div>    <a href="https://www.astralcodexten.com/p/political-backflow-from-europe" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r1zvik/we_are_the_babies_ai_will_be_the_parent_geoffrey/" target="_blank" rel="noopener noreferrer">“We Are the Babies — AI Will Be the Parent.” — Geoffrey Hinton</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reddit r/ControlProblem post with 38 upvotes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reddit r/ControlProblem post with 38 upvotes.</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r1zvik/we_are_the_babies_ai_will_be_the_parent_geoffrey/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/AIsafety/comments/1r1toua/artificial_intelligence_and_biological_risks/" target="_blank" rel="noopener noreferrer">Artificial Intelligence and Biological Risks</a>
    </h3>    <p class="paper-authors">EchoOfOppenheimer</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A 2026 report from the **Federation of American Scientists (FAS)** warns that the convergence of AI and biology is lowering the barrier and raising the ceiling for biological threats. While Large Language Models (LLMs) can democratize access to sensitive scientific knowledge, new AI-enabled biological design tools (BDTs) could allow sophisticated actors to engineer novel pathogens. The report calls for a defense-in-depth approach, including rapid AIxBio evaluation programs and strengthened DNA...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A 2026 report from the **Federation of American Scientists (FAS)** warns that the convergence of AI and biology is lowering the barrier and raising the ceiling for biological threats. While Large Language Models (LLMs) can democratize access to sensitive scientific knowledge, new AI-enabled biological design tools (BDTs) could allow sophisticated actors to engineer novel pathogens. The report calls for a defense-in-depth approach, including rapid AIxBio evaluation programs and strengthened DNA...</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/AIsafety/comments/1r1toua/artificial_intelligence_and_biological_risks/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Zvi Mowshowitz">
    <div class="paper-meta">
      <span class="org-tag" data-org="Zvi Mowshowitz">Zvi Mowshowitz</span>
      <span class="paper-date">Feb 10, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://thezvi.substack.com/p/claude-opus-46-system-card-part-2" target="_blank" rel="noopener noreferrer">Claude Opus 4.6: System Card Part 2: Frontier Alignment</a>
    </h3>    <p class="paper-authors">Zvi Mowshowitz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Coverage of Claude Opus 4.6 started yesterday with the mundane alignment and model welfare sections of the model card.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Coverage of Claude Opus 4.6 started yesterday with the mundane alignment and model welfare sections of the model card.</div>
      </details>
    </div>    <a href="https://thezvi.substack.com/p/claude-opus-46-system-card-part-2" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 10, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/8m6AM5qtPMjgTkEeD/my-journey-to-the-microwave-alternate-timeline" target="_blank" rel="noopener noreferrer">My journey to the microwave alternate timeline</a>
    </h3>    <p class="paper-authors">Malmesbury</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Recommended soundtrack for this post • As we all know, the march of technological progress is best summarized by this meme from Linkedin: …</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Recommended soundtrack for this post • As we all know, the march of technological progress is best summarized by this meme from Linkedin: …</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/8m6AM5qtPMjgTkEeD/my-journey-to-the-microwave-alternate-timeline" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 10, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r16dmi/anthropic_has_entrusted_amanda_askell_to_endow/" target="_blank" rel="noopener noreferrer">“Anthropic has entrusted Amanda Askell to endow its AI chatbot, Claude, with a sense of right and wrong” - Seems like Anthropic is doubling down on AI alignment.</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reddit r/ControlProblem post with 42 upvotes.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reddit r/ControlProblem post with 42 upvotes.</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r16dmi/anthropic_has_entrusted_amanda_askell_to_endow/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Vox Future Perfect">
    <div class="paper-meta">
      <span class="org-tag" data-org="Vox Future Perfect">Vox Future Perfect</span>
      <span class="paper-date">Feb 10, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.vox.com/future-perfect/474808/flu-season-vaccine-hospitalizations-mrna-shots" target="_blank" rel="noopener noreferrer">A world without flu is possible</a>
    </h3>    <p class="paper-authors">Bryan Walsh</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This story was originally published in The Highlight, Vox’s member-exclusive magazine. To get access to member-exclusive stories every month, join the Vox Membership program today. Let’s start with the bad news. There’s a decent chance, perhaps as high as 11 percent if you’re unvaccinated, that some time over the course of this winter, you’ll be overcome with [&amp;#8230;]</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This story was originally published in The Highlight, Vox’s member-exclusive magazine. To get access to member-exclusive stories every month, join the Vox Membership program today. Let’s start with the bad news. There’s a decent chance, perhaps as high as 11 percent if you’re unvaccinated, that some time over the course of this winter, you’ll be overcome with [&amp;#8230;]</div>
      </details>
    </div>    <a href="https://www.vox.com/future-perfect/474808/flu-season-vaccine-hospitalizations-mrna-shots" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>    </div>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="footer-inner">
      <p class="footer-note">Curated automatically &middot; Powered by Python + GitHub Actions</p>
      <p class="footer-timestamp">2026-02-17 11:21:20</p>
    </div>
  </footer>

  <!-- Filter JS -->
  <script>
    (function () {
      var pills = document.querySelectorAll('.filter-pill');
      var cards = document.querySelectorAll('.paper-card');
      var heroSection = document.querySelector('.hero-section');
      var heroLabel = heroSection ? heroSection.querySelector('.section-label') : null;
      var gridLabel = document.querySelector('.papers-section .section-label');

      pills.forEach(function (pill) {
        pill.addEventListener('click', function () {
          var filter = this.getAttribute('data-filter');

          // Update active pill
          pills.forEach(function (p) { p.classList.remove('active'); });
          this.classList.add('active');

          // Filter cards and count visible
          var visible = 0;
          var heroVisible = 0;
          var gridVisible = 0;
          cards.forEach(function (card) {
            if (filter === 'all' || card.getAttribute('data-org') === filter) {
              card.style.display = '';
              visible++;
              if (card.classList.contains('hero-card')) heroVisible++;
              if (card.classList.contains('grid-card')) gridVisible++;
            } else {
              card.style.display = 'none';
            }
          });

          // Show/hide section labels when no cards visible in that section
          if (heroSection) heroSection.style.display = heroVisible > 0 ? '' : 'none';
          if (gridLabel) gridLabel.style.display = gridVisible > 0 ? '' : 'none';
        });
      });
    })();
  </script>

</body>
</html>