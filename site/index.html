<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Safety Weekly Digest — February 16, 2026 to February 22, 2026</title>
  <meta name="description" content="A curated weekly digest of 58 AI safety research papers from leading organizations and researchers. February 16, 2026 through February 22, 2026.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  <style>
    /* =============================================
   AI Safety Weekly Digest — Editorial Stylesheet
   Modern research publication aesthetic
   ============================================= */

/* --- CSS Custom Properties --- */
:root {
  /* Base palette */
  --bg: #f8f8f7;
  --bg-card: #ffffff;
  --bg-elevated: #ffffff;
  --bg-hero: #fcfcfb;
  --text: #1a1a1a;
  --text-secondary: #4a4a4a;
  --text-muted: #6b6b6b;
  --text-faint: #999999;
  --heading: #0d0d0d;
  --border: #e6e4e0;
  --border-light: #f0eee9;
  --shadow-sm: 0 1px 3px rgba(0, 0, 0, 0.04);
  --shadow-md: 0 4px 12px rgba(0, 0, 0, 0.06);
  --shadow-lg: 0 8px 30px rgba(0, 0, 0, 0.08);

  /* Accent — deep teal */
  --accent: #0d6e6e;
  --accent-light: #0f8585;
  --accent-subtle: rgba(13, 110, 110, 0.07);
  --accent-text: #0a5a5a;

  /* Links */
  --link: #0d6e6e;
  --link-hover: #0a5252;

  /* Filter bar */
  --filter-bg: rgba(248, 248, 247, 0.88);
  --pill-active-bg: #1a1a1a;
  --pill-active-text: #ffffff;

  /* Card accent fallback */
  --card-accent: var(--org-default);

  /* Spacing scale */
  --space-xs: 0.25rem;
  --space-sm: 0.5rem;
  --space-md: 1rem;
  --space-lg: 1.5rem;
  --space-xl: 2rem;
  --space-2xl: 3rem;
  --space-3xl: 4rem;

  /* Radii */
  --radius-sm: 6px;
  --radius-md: 10px;
  --radius-lg: 14px;

  /* Organization colors — muted, professional tones */
  --org-anthropic: #c4854a;
  --org-openai: #4a8c5c;
  --org-google-deepmind: #4a7ab8;
  --org-metr: #b85a5a;
  --org-apollo-research: #7b5ab8;
  --org-uk-aisi: #3a7a8a;
  --org-redwood-research: #c04040;
  --org-alignment-forum: #4a8a4a;
  --org-arxiv: #777777;
  --org-hyperdimensional: #b8924a;
  --org-peter-wildeford: #5a5ab8;
  --org-zvi-mowshowitz: #8a6a3a;
  --org-rand: #3a6a8a;
  --org-arc: #8a3a6a;
  --org-miri: #5a8a3a;
  --org-cais: #3a8a6a;
  --org-microsoft-research: #3a6ab8;
  --org-import-ai: #b8943a;
  --org-dan-hendrycks: #7a6a3a;
  --org-astral-codex-ten: #5a6a8a;
  --org-cognitive-revolution: #8a5a6a;
  --org-vox-future-perfect: #3a8a6a;
  --org-fli: #7a5a7a;
  --org-epoch-ai: #5a7a6a;
  --org-lesswrong: #4a8a4a;
  --org-far-ai: #6a4a8a;
  --org-us-aisi: #3a5a8a;
  --org-cset: #6a7a3a;
  --org-govai: #4a5a7a;
  --org-iaps: #7a4a5a;
  --org-cltr: #5a4a7a;
  --org-chai: #8a6a4a;
  --org-mats: #4a7a7a;
  --org-paul-christiano: #7a6a4a;
  --org-yoshua-bengio: #4a6a7a;
  --org-lennart-heim: #6a7a4a;
  --org-hacker-news: #e86424;
  --org-reddit: #e84420;
  --org-default: #888888;
}

/* --- Dark Mode --- */
@media (prefers-color-scheme: dark) {
  :root {
    --bg: #111113;
    --bg-card: #1a1a1e;
    --bg-elevated: #222226;
    --bg-hero: #161618;
    --text: #e2e2e6;
    --text-secondary: #b0b0b8;
    --text-muted: #8a8a96;
    --text-faint: #606068;
    --heading: #f0f0f4;
    --border: #2a2a30;
    --border-light: #222228;
    --shadow-sm: 0 1px 3px rgba(0, 0, 0, 0.15);
    --shadow-md: 0 4px 12px rgba(0, 0, 0, 0.25);
    --shadow-lg: 0 8px 30px rgba(0, 0, 0, 0.35);
    --accent: #2ca0a0;
    --accent-light: #35b5b5;
    --accent-subtle: rgba(44, 160, 160, 0.1);
    --accent-text: #2ca0a0;
    --link: #35b5b5;
    --link-hover: #4acaca;
    --filter-bg: rgba(17, 17, 19, 0.88);
    --pill-active-bg: #2ca0a0;
    --pill-active-text: #111113;
  }
}

/* --- Reset & Base --- */
*,
*::before,
*::after {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

html {
  -webkit-text-size-adjust: 100%;
  scroll-behavior: smooth;
}

body {
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
  background-color: var(--bg);
  color: var(--text);
  max-width: 1120px;
  margin: 0 auto;
  padding: 0 clamp(1.25rem, 5vw, 2.5rem);
  font-size: 15px;
  line-height: 1.6;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

a {
  color: var(--link);
  text-decoration: none;
  transition: color 0.15s ease;
}

a:hover {
  color: var(--link-hover);
}

/* --- Masthead / Header --- */
.site-header {
  padding: var(--space-3xl) 0 var(--space-xl);
  text-align: center;
  border-bottom: 1px solid var(--border);
}

.header-inner {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: var(--space-sm);
}

.site-title {
  font-size: clamp(1.75rem, 1.4rem + 1.8vw, 2.75rem);
  font-weight: 800;
  color: var(--heading);
  letter-spacing: -0.04em;
  line-height: 1.1;
}

.header-rule {
  width: 40px;
  height: 2px;
  background: var(--accent);
  border: none;
  margin: var(--space-sm) 0;
  border-radius: 1px;
}

.header-meta {
  display: flex;
  align-items: center;
  gap: var(--space-sm);
  flex-wrap: wrap;
  justify-content: center;
}

.header-daterange {
  font-size: 0.92rem;
  font-weight: 500;
  color: var(--text-secondary);
}

.header-separator {
  color: var(--border);
  font-weight: 300;
}

.header-count {
  font-size: 0.85rem;
  color: var(--text-muted);
}

.header-count strong {
  font-weight: 600;
  color: var(--accent-text);
}

/* --- Filter Bar --- */
.filter-bar {
  position: sticky;
  top: 0;
  z-index: 100;
  margin: 0 calc(-1 * clamp(1.25rem, 5vw, 2.5rem));
  padding: 0 clamp(1.25rem, 5vw, 2.5rem);
  -webkit-backdrop-filter: blur(20px) saturate(180%);
  backdrop-filter: blur(20px) saturate(180%);
  background-color: var(--filter-bg);
  border-bottom: 1px solid var(--border-light);
}

.filter-bar-inner {
  padding: 0.7rem 0;
}

.filter-scroll {
  display: flex;
  gap: 0.35rem;
  overflow-x: auto;
  padding-bottom: 2px;
  scrollbar-width: thin;
  scrollbar-color: var(--border) transparent;
  -webkit-overflow-scrolling: touch;
}

.filter-scroll::-webkit-scrollbar {
  height: 2px;
}

.filter-scroll::-webkit-scrollbar-thumb {
  background: var(--border);
  border-radius: 2px;
}

.filter-pill {
  flex-shrink: 0;
  padding: 0.32rem 0.75rem;
  font-size: 0.76rem;
  font-family: inherit;
  font-weight: 500;
  border: 1px solid var(--border);
  border-radius: 100px;
  background: transparent;
  color: var(--text-muted);
  cursor: pointer;
  transition: all 0.15s ease;
  white-space: nowrap;
  line-height: 1.4;
}

.filter-pill:hover {
  border-color: var(--text-faint);
  color: var(--text-secondary);
  background-color: var(--bg-elevated);
}

.filter-pill.active {
  background-color: var(--pill-active-bg);
  border-color: var(--pill-active-bg);
  color: var(--pill-active-text);
  font-weight: 600;
}

.pill-count {
  font-size: 0.68rem;
  opacity: 0.65;
  margin-left: 0.15rem;
}

/* --- Section Labels --- */
.section-label {
  font-size: 0.7rem;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 0.1em;
  color: var(--text-faint);
  margin-bottom: var(--space-lg);
  padding-bottom: var(--space-sm);
  border-bottom: 1px solid var(--border-light);
}

/* --- Featured / Hero Section --- */
.hero-section {
  padding-top: var(--space-xl);
  padding-bottom: var(--space-md);
}

.hero-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  grid-template-rows: auto auto;
  gap: var(--space-md);
}

/* First featured paper: spans the full left column across both rows */
.hero-grid .hero-card:first-child {
  grid-row: 1 / 3;
}

/* --- Base Paper Card --- */
.paper-card {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: var(--radius-md);
  padding: var(--space-lg);
  transition: border-color 0.2s ease, box-shadow 0.25s ease, transform 0.2s ease;
  position: relative;
  display: flex;
  flex-direction: column;
}

.paper-card:hover {
  border-color: color-mix(in srgb, var(--card-accent) 35%, var(--border));
  box-shadow: var(--shadow-md);
}

/* --- Hero Card --- */
.hero-card {
  border-radius: var(--radius-lg);
  padding: clamp(1.25rem, 3vw, 2rem);
  border-left: 3px solid var(--card-accent);
}

.hero-card:hover {
  transform: translateY(-2px);
  box-shadow: var(--shadow-lg);
}

.hero-card .paper-title {
  font-size: clamp(1.15rem, 1rem + 0.5vw, 1.4rem);
  font-weight: 700;
  line-height: 1.28;
  margin-bottom: var(--space-sm);
}

.hero-card .paper-authors {
  font-size: 0.84rem;
  margin-bottom: var(--space-xs);
}

.hero-card .abstract-preview {
  -webkit-line-clamp: 5;
}

.hero-card .paper-abstract {
  flex: 1;
}

/* --- Papers Grid --- */
.papers-section {
  padding-top: var(--space-xl);
  padding-bottom: var(--space-2xl);
}

.papers-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
  gap: var(--space-md);
}

/* --- Grid Card --- */
.grid-card {
  border-top: 3px solid var(--card-accent);
  border-left: none;
}

.grid-card .paper-title {
  font-size: clamp(0.95rem, 0.9rem + 0.2vw, 1.05rem);
  margin-bottom: var(--space-xs);
}

.grid-card .paper-abstract {
  flex: 1;
}

.grid-card .read-link {
  margin-top: auto;
  padding-top: var(--space-sm);
}

/* Card accent colors per org */
.paper-card[data-org="Anthropic"] { --card-accent: var(--org-anthropic); }
.paper-card[data-org="OpenAI"] { --card-accent: var(--org-openai); }
.paper-card[data-org="Google DeepMind"] { --card-accent: var(--org-google-deepmind); }
.paper-card[data-org="METR"] { --card-accent: var(--org-metr); }
.paper-card[data-org="Apollo Research"] { --card-accent: var(--org-apollo-research); }
.paper-card[data-org="UK AISI"] { --card-accent: var(--org-uk-aisi); }
.paper-card[data-org="Redwood Research"] { --card-accent: var(--org-redwood-research); }
.paper-card[data-org="Alignment Forum"] { --card-accent: var(--org-alignment-forum); }
.paper-card[data-org="arXiv"] { --card-accent: var(--org-arxiv); }
.paper-card[data-org="Hyperdimensional"] { --card-accent: var(--org-hyperdimensional); }
.paper-card[data-org="Peter Wildeford"] { --card-accent: var(--org-peter-wildeford); }
.paper-card[data-org="Zvi Mowshowitz"] { --card-accent: var(--org-zvi-mowshowitz); }
.paper-card[data-org="RAND"] { --card-accent: var(--org-rand); }
.paper-card[data-org="ARC"] { --card-accent: var(--org-arc); }
.paper-card[data-org="MIRI"] { --card-accent: var(--org-miri); }
.paper-card[data-org="CAIS"] { --card-accent: var(--org-cais); }
.paper-card[data-org="Microsoft Research"] { --card-accent: var(--org-microsoft-research); }
.paper-card[data-org="Import AI"] { --card-accent: var(--org-import-ai); }
.paper-card[data-org="Dan Hendrycks"] { --card-accent: var(--org-dan-hendrycks); }
.paper-card[data-org="Astral Codex Ten"] { --card-accent: var(--org-astral-codex-ten); }
.paper-card[data-org="Cognitive Revolution"] { --card-accent: var(--org-cognitive-revolution); }
.paper-card[data-org="Vox Future Perfect"] { --card-accent: var(--org-vox-future-perfect); }
.paper-card[data-org="FLI"] { --card-accent: var(--org-fli); }
.paper-card[data-org="Epoch AI"] { --card-accent: var(--org-epoch-ai); }
.paper-card[data-org="LessWrong"] { --card-accent: var(--org-lesswrong); }
.paper-card[data-org="FAR AI"] { --card-accent: var(--org-far-ai); }
.paper-card[data-org="US AISI"] { --card-accent: var(--org-us-aisi); }
.paper-card[data-org="CSET"] { --card-accent: var(--org-cset); }
.paper-card[data-org="GovAI"] { --card-accent: var(--org-govai); }
.paper-card[data-org="IAPS"] { --card-accent: var(--org-iaps); }
.paper-card[data-org="CLTR"] { --card-accent: var(--org-cltr); }
.paper-card[data-org="CHAI"] { --card-accent: var(--org-chai); }
.paper-card[data-org="MATS"] { --card-accent: var(--org-mats); }
.paper-card[data-org="Paul Christiano"] { --card-accent: var(--org-paul-christiano); }
.paper-card[data-org="Yoshua Bengio"] { --card-accent: var(--org-yoshua-bengio); }
.paper-card[data-org="Lennart Heim"] { --card-accent: var(--org-lennart-heim); }
.paper-card[data-org="Hacker News"] { --card-accent: var(--org-hacker-news); }
.paper-card[data-org="Reddit"] { --card-accent: var(--org-reddit); }

/* --- Paper Meta Row --- */
.paper-meta {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: var(--space-sm);
  gap: var(--space-sm);
}

/* --- Organization Tag (pill) --- */
.org-tag {
  display: inline-block;
  font-size: 0.65rem;
  font-weight: 600;
  padding: 0.18rem 0.55rem;
  border-radius: 100px;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  line-height: 1.4;
  background-color: color-mix(in srgb, var(--org-default) 12%, var(--bg-card));
  color: var(--org-default);
}

/* Org-specific tag colors — using color-mix for subtle backgrounds */
.org-tag[data-org="Anthropic"] { background-color: color-mix(in srgb, var(--org-anthropic) 12%, var(--bg-card)); color: var(--org-anthropic); }
.org-tag[data-org="OpenAI"] { background-color: color-mix(in srgb, var(--org-openai) 12%, var(--bg-card)); color: var(--org-openai); }
.org-tag[data-org="Google DeepMind"] { background-color: color-mix(in srgb, var(--org-google-deepmind) 12%, var(--bg-card)); color: var(--org-google-deepmind); }
.org-tag[data-org="METR"] { background-color: color-mix(in srgb, var(--org-metr) 12%, var(--bg-card)); color: var(--org-metr); }
.org-tag[data-org="Apollo Research"] { background-color: color-mix(in srgb, var(--org-apollo-research) 12%, var(--bg-card)); color: var(--org-apollo-research); }
.org-tag[data-org="UK AISI"] { background-color: color-mix(in srgb, var(--org-uk-aisi) 12%, var(--bg-card)); color: var(--org-uk-aisi); }
.org-tag[data-org="Redwood Research"] { background-color: color-mix(in srgb, var(--org-redwood-research) 12%, var(--bg-card)); color: var(--org-redwood-research); }
.org-tag[data-org="Alignment Forum"] { background-color: color-mix(in srgb, var(--org-alignment-forum) 12%, var(--bg-card)); color: var(--org-alignment-forum); }
.org-tag[data-org="arXiv"] { background-color: color-mix(in srgb, var(--org-arxiv) 12%, var(--bg-card)); color: var(--org-arxiv); }
.org-tag[data-org="Hyperdimensional"] { background-color: color-mix(in srgb, var(--org-hyperdimensional) 12%, var(--bg-card)); color: var(--org-hyperdimensional); }
.org-tag[data-org="Peter Wildeford"] { background-color: color-mix(in srgb, var(--org-peter-wildeford) 12%, var(--bg-card)); color: var(--org-peter-wildeford); }
.org-tag[data-org="Zvi Mowshowitz"] { background-color: color-mix(in srgb, var(--org-zvi-mowshowitz) 12%, var(--bg-card)); color: var(--org-zvi-mowshowitz); }
.org-tag[data-org="RAND"] { background-color: color-mix(in srgb, var(--org-rand) 12%, var(--bg-card)); color: var(--org-rand); }
.org-tag[data-org="ARC"] { background-color: color-mix(in srgb, var(--org-arc) 12%, var(--bg-card)); color: var(--org-arc); }
.org-tag[data-org="MIRI"] { background-color: color-mix(in srgb, var(--org-miri) 12%, var(--bg-card)); color: var(--org-miri); }
.org-tag[data-org="CAIS"] { background-color: color-mix(in srgb, var(--org-cais) 12%, var(--bg-card)); color: var(--org-cais); }
.org-tag[data-org="Microsoft Research"] { background-color: color-mix(in srgb, var(--org-microsoft-research) 12%, var(--bg-card)); color: var(--org-microsoft-research); }
.org-tag[data-org="Import AI"] { background-color: color-mix(in srgb, var(--org-import-ai) 12%, var(--bg-card)); color: var(--org-import-ai); }
.org-tag[data-org="Dan Hendrycks"] { background-color: color-mix(in srgb, var(--org-dan-hendrycks) 12%, var(--bg-card)); color: var(--org-dan-hendrycks); }
.org-tag[data-org="Astral Codex Ten"] { background-color: color-mix(in srgb, var(--org-astral-codex-ten) 12%, var(--bg-card)); color: var(--org-astral-codex-ten); }
.org-tag[data-org="Cognitive Revolution"] { background-color: color-mix(in srgb, var(--org-cognitive-revolution) 12%, var(--bg-card)); color: var(--org-cognitive-revolution); }
.org-tag[data-org="Vox Future Perfect"] { background-color: color-mix(in srgb, var(--org-vox-future-perfect) 12%, var(--bg-card)); color: var(--org-vox-future-perfect); }
.org-tag[data-org="FLI"] { background-color: color-mix(in srgb, var(--org-fli) 12%, var(--bg-card)); color: var(--org-fli); }
.org-tag[data-org="Epoch AI"] { background-color: color-mix(in srgb, var(--org-epoch-ai) 12%, var(--bg-card)); color: var(--org-epoch-ai); }
.org-tag[data-org="LessWrong"] { background-color: color-mix(in srgb, var(--org-lesswrong) 12%, var(--bg-card)); color: var(--org-lesswrong); }
.org-tag[data-org="FAR AI"] { background-color: color-mix(in srgb, var(--org-far-ai) 12%, var(--bg-card)); color: var(--org-far-ai); }
.org-tag[data-org="US AISI"] { background-color: color-mix(in srgb, var(--org-us-aisi) 12%, var(--bg-card)); color: var(--org-us-aisi); }
.org-tag[data-org="CSET"] { background-color: color-mix(in srgb, var(--org-cset) 12%, var(--bg-card)); color: var(--org-cset); }
.org-tag[data-org="GovAI"] { background-color: color-mix(in srgb, var(--org-govai) 12%, var(--bg-card)); color: var(--org-govai); }
.org-tag[data-org="IAPS"] { background-color: color-mix(in srgb, var(--org-iaps) 12%, var(--bg-card)); color: var(--org-iaps); }
.org-tag[data-org="CLTR"] { background-color: color-mix(in srgb, var(--org-cltr) 12%, var(--bg-card)); color: var(--org-cltr); }
.org-tag[data-org="CHAI"] { background-color: color-mix(in srgb, var(--org-chai) 12%, var(--bg-card)); color: var(--org-chai); }
.org-tag[data-org="MATS"] { background-color: color-mix(in srgb, var(--org-mats) 12%, var(--bg-card)); color: var(--org-mats); }
.org-tag[data-org="Paul Christiano"] { background-color: color-mix(in srgb, var(--org-paul-christiano) 12%, var(--bg-card)); color: var(--org-paul-christiano); }
.org-tag[data-org="Yoshua Bengio"] { background-color: color-mix(in srgb, var(--org-yoshua-bengio) 12%, var(--bg-card)); color: var(--org-yoshua-bengio); }
.org-tag[data-org="Lennart Heim"] { background-color: color-mix(in srgb, var(--org-lennart-heim) 12%, var(--bg-card)); color: var(--org-lennart-heim); }
.org-tag[data-org="Hacker News"] { background-color: color-mix(in srgb, var(--org-hacker-news) 12%, var(--bg-card)); color: var(--org-hacker-news); }
.org-tag[data-org="Reddit"] { background-color: color-mix(in srgb, var(--org-reddit) 12%, var(--bg-card)); color: var(--org-reddit); }

/* Dark mode org tag adjustments */
@media (prefers-color-scheme: dark) {
  .org-tag {
    background-color: color-mix(in srgb, var(--org-default) 15%, var(--bg-card));
  }
  .org-tag[data-org="Anthropic"] { background-color: color-mix(in srgb, var(--org-anthropic) 15%, var(--bg-card)); }
  .org-tag[data-org="OpenAI"] { background-color: color-mix(in srgb, var(--org-openai) 15%, var(--bg-card)); }
  .org-tag[data-org="Google DeepMind"] { background-color: color-mix(in srgb, var(--org-google-deepmind) 15%, var(--bg-card)); }
  .org-tag[data-org="METR"] { background-color: color-mix(in srgb, var(--org-metr) 15%, var(--bg-card)); }
  .org-tag[data-org="Apollo Research"] { background-color: color-mix(in srgb, var(--org-apollo-research) 15%, var(--bg-card)); }
  .org-tag[data-org="UK AISI"] { background-color: color-mix(in srgb, var(--org-uk-aisi) 15%, var(--bg-card)); }
  .org-tag[data-org="Redwood Research"] { background-color: color-mix(in srgb, var(--org-redwood-research) 15%, var(--bg-card)); }
  .org-tag[data-org="Alignment Forum"] { background-color: color-mix(in srgb, var(--org-alignment-forum) 15%, var(--bg-card)); }
  .org-tag[data-org="arXiv"] { background-color: color-mix(in srgb, var(--org-arxiv) 15%, var(--bg-card)); }
  .org-tag[data-org="Hyperdimensional"] { background-color: color-mix(in srgb, var(--org-hyperdimensional) 15%, var(--bg-card)); }
  .org-tag[data-org="Peter Wildeford"] { background-color: color-mix(in srgb, var(--org-peter-wildeford) 15%, var(--bg-card)); }
  .org-tag[data-org="Zvi Mowshowitz"] { background-color: color-mix(in srgb, var(--org-zvi-mowshowitz) 15%, var(--bg-card)); }
  .org-tag[data-org="RAND"] { background-color: color-mix(in srgb, var(--org-rand) 15%, var(--bg-card)); }
  .org-tag[data-org="ARC"] { background-color: color-mix(in srgb, var(--org-arc) 15%, var(--bg-card)); }
  .org-tag[data-org="MIRI"] { background-color: color-mix(in srgb, var(--org-miri) 15%, var(--bg-card)); }
  .org-tag[data-org="CAIS"] { background-color: color-mix(in srgb, var(--org-cais) 15%, var(--bg-card)); }
  .org-tag[data-org="Microsoft Research"] { background-color: color-mix(in srgb, var(--org-microsoft-research) 15%, var(--bg-card)); }
  .org-tag[data-org="Import AI"] { background-color: color-mix(in srgb, var(--org-import-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="Dan Hendrycks"] { background-color: color-mix(in srgb, var(--org-dan-hendrycks) 15%, var(--bg-card)); }
  .org-tag[data-org="Astral Codex Ten"] { background-color: color-mix(in srgb, var(--org-astral-codex-ten) 15%, var(--bg-card)); }
  .org-tag[data-org="Cognitive Revolution"] { background-color: color-mix(in srgb, var(--org-cognitive-revolution) 15%, var(--bg-card)); }
  .org-tag[data-org="Vox Future Perfect"] { background-color: color-mix(in srgb, var(--org-vox-future-perfect) 15%, var(--bg-card)); }
  .org-tag[data-org="FLI"] { background-color: color-mix(in srgb, var(--org-fli) 15%, var(--bg-card)); }
  .org-tag[data-org="Epoch AI"] { background-color: color-mix(in srgb, var(--org-epoch-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="LessWrong"] { background-color: color-mix(in srgb, var(--org-lesswrong) 15%, var(--bg-card)); }
  .org-tag[data-org="FAR AI"] { background-color: color-mix(in srgb, var(--org-far-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="US AISI"] { background-color: color-mix(in srgb, var(--org-us-aisi) 15%, var(--bg-card)); }
  .org-tag[data-org="CSET"] { background-color: color-mix(in srgb, var(--org-cset) 15%, var(--bg-card)); }
  .org-tag[data-org="GovAI"] { background-color: color-mix(in srgb, var(--org-govai) 15%, var(--bg-card)); }
  .org-tag[data-org="IAPS"] { background-color: color-mix(in srgb, var(--org-iaps) 15%, var(--bg-card)); }
  .org-tag[data-org="CLTR"] { background-color: color-mix(in srgb, var(--org-cltr) 15%, var(--bg-card)); }
  .org-tag[data-org="CHAI"] { background-color: color-mix(in srgb, var(--org-chai) 15%, var(--bg-card)); }
  .org-tag[data-org="MATS"] { background-color: color-mix(in srgb, var(--org-mats) 15%, var(--bg-card)); }
  .org-tag[data-org="Paul Christiano"] { background-color: color-mix(in srgb, var(--org-paul-christiano) 15%, var(--bg-card)); }
  .org-tag[data-org="Yoshua Bengio"] { background-color: color-mix(in srgb, var(--org-yoshua-bengio) 15%, var(--bg-card)); }
  .org-tag[data-org="Lennart Heim"] { background-color: color-mix(in srgb, var(--org-lennart-heim) 15%, var(--bg-card)); }
  .org-tag[data-org="Hacker News"] { background-color: color-mix(in srgb, var(--org-hacker-news) 15%, var(--bg-card)); }
  .org-tag[data-org="Reddit"] { background-color: color-mix(in srgb, var(--org-reddit) 15%, var(--bg-card)); }
}

/* --- Paper Title --- */
.paper-title {
  font-size: clamp(1rem, 0.95rem + 0.3vw, 1.15rem);
  font-weight: 650;
  line-height: 1.32;
  margin-bottom: var(--space-xs);
  letter-spacing: -0.015em;
}

.paper-title a {
  color: var(--heading);
  text-decoration: none;
  transition: color 0.15s ease;
}

.paper-title a:hover {
  color: var(--link);
}

/* --- Paper Authors --- */
.paper-authors {
  font-size: 0.8rem;
  color: var(--text-faint);
  margin-bottom: var(--space-xs);
  line-height: 1.45;
}

/* --- Paper Date --- */
.paper-date {
  font-size: 0.72rem;
  color: var(--text-faint);
  white-space: nowrap;
  font-variant-numeric: tabular-nums;
}

/* --- Abstract Section --- */
.paper-abstract {
  margin-top: var(--space-sm);
  margin-bottom: var(--space-sm);
}

/* --- Details / Summary expand-collapse --- */
.abstract-expand {
  /* no extra margin needed */
}

.abstract-expand summary {
  list-style: none;
  cursor: pointer;
  display: block;
}

.abstract-expand summary::-webkit-details-marker {
  display: none;
}

/* The truncated preview text inside the summary */
.abstract-preview {
  display: -webkit-box;
  -webkit-line-clamp: 3;
  -webkit-box-orient: vertical;
  overflow: hidden;
  font-size: 0.85rem;
  line-height: 1.6;
  color: var(--text-muted);
}

.hero-card .abstract-preview {
  -webkit-line-clamp: 5;
}

/* Toggle label ("Show more" / "Show less") */
.toggle-label {
  display: inline-flex;
  align-items: center;
  gap: 0.25rem;
  font-size: 0.75rem;
  font-weight: 500;
  color: var(--link);
  padding: 0.2rem 0 0;
  transition: color 0.15s ease;
  user-select: none;
  -webkit-user-select: none;
}

.toggle-label::after {
  content: '';
  display: inline-block;
  width: 0.35em;
  height: 0.35em;
  border-right: 1.5px solid currentColor;
  border-bottom: 1.5px solid currentColor;
  transform: rotate(45deg);
  transition: transform 0.2s ease;
  margin-top: -0.1em;
}

.abstract-expand[open] .toggle-label::after {
  transform: rotate(-135deg);
  margin-top: 0.15em;
}

.abstract-expand summary:hover .toggle-label {
  color: var(--link-hover);
}

/* When open, hide the truncated preview and change label text */
.abstract-expand[open] .abstract-preview {
  display: none;
}

.abstract-expand[open] .toggle-label {
  margin-top: 0;
}

/* Full abstract text */
.abstract-full {
  font-size: 0.84rem;
  line-height: 1.65;
  color: var(--text-muted);
  padding: 0.5rem 0 0.2rem 0.85rem;
  border-left: 2px solid color-mix(in srgb, var(--card-accent) 50%, var(--border));
  margin-top: 0.35rem;
  animation: abstractReveal 0.2s ease;
}

@keyframes abstractReveal {
  from {
    opacity: 0;
    transform: translateY(-4px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* --- Read Paper Link --- */
.read-link {
  display: inline-flex;
  align-items: center;
  gap: 0.3rem;
  font-size: 0.78rem;
  font-weight: 500;
  color: var(--text-faint);
  margin-top: var(--space-sm);
  padding: 0;
  border: none;
  background: none;
  transition: color 0.15s ease, gap 0.15s ease;
  text-decoration: none;
  letter-spacing: 0.01em;
}

.read-link:hover {
  color: var(--link);
  gap: 0.45rem;
}

/* --- Empty State --- */
.empty-state {
  text-align: center;
  padding: 5rem 1rem;
  color: var(--text-muted);
}

.empty-state p {
  font-size: 1.05rem;
  margin-bottom: 0.4rem;
}

.empty-sub {
  font-size: 0.88rem;
  color: var(--text-faint);
}

/* --- Footer --- */
.site-footer {
  margin-top: var(--space-xl);
  padding: var(--space-lg) 0 var(--space-2xl);
  border-top: 1px solid var(--border);
  text-align: center;
}

.footer-inner {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: var(--space-xs);
}

.footer-note {
  font-size: 0.75rem;
  color: var(--text-faint);
  letter-spacing: 0.01em;
}

.footer-timestamp {
  font-size: 0.7rem;
  color: var(--text-faint);
  opacity: 0.6;
}

/* --- Responsive: Tablet --- */
@media (max-width: 900px) {
  .hero-grid {
    grid-template-columns: 1fr 1fr;
    grid-template-rows: auto;
  }

  .hero-grid .hero-card:first-child {
    grid-column: 1 / -1;
    grid-row: auto;
  }
}

/* --- Responsive: Mobile --- */
@media (max-width: 640px) {
  body {
    font-size: 14px;
  }

  .site-header {
    padding: var(--space-2xl) 0 var(--space-lg);
  }

  .site-title {
    font-size: 1.65rem;
  }

  .header-meta {
    flex-direction: column;
    gap: 0.15rem;
  }

  .header-separator {
    display: none;
  }

  .hero-grid {
    grid-template-columns: 1fr;
  }

  .hero-grid .hero-card:first-child {
    grid-column: auto;
  }

  .papers-grid {
    grid-template-columns: 1fr;
  }

  .paper-card {
    padding: var(--space-md);
    border-radius: var(--radius-sm);
  }

  .hero-card {
    border-radius: var(--radius-md);
  }

  .paper-title {
    font-size: 0.95rem;
  }

  .hero-card .paper-title {
    font-size: 1.05rem;
  }

  .paper-authors {
    font-size: 0.76rem;
  }

  .abstract-preview {
    font-size: 0.82rem;
  }

  .toggle-label {
    font-size: 0.72rem;
  }

  .abstract-full {
    font-size: 0.8rem;
    padding-left: 0.65rem;
  }

  .filter-pill {
    font-size: 0.7rem;
    padding: 0.28rem 0.6rem;
  }

  .paper-meta {
    flex-direction: column;
    align-items: flex-start;
    gap: 0.15rem;
  }

  .read-link {
    font-size: 0.75rem;
  }

  .section-label {
    font-size: 0.65rem;
  }
}

  </style>
</head>
<body>

<!-- Masthead -->
  <header class="site-header">
    <div class="header-inner">
      <h1 class="site-title">AI Safety Weekly Digest</h1>
      <hr class="header-rule">
      <div class="header-meta">
        <span class="header-daterange">February 16, 2026 &mdash; February 22, 2026</span>        <span class="header-separator">&middot;</span>
        <span class="header-count"><strong>58</strong> papers</span>      </div>
    </div>
  </header>

  <!-- Filter Bar -->
  <nav class="filter-bar" aria-label="Filter papers by organization">
    <div class="filter-bar-inner">
      <div class="filter-scroll">
        <button class="filter-pill active" data-filter="all">All <span class="pill-count">58</span></button>        <button class="filter-pill" data-filter="OpenAI">OpenAI</button>        <button class="filter-pill" data-filter="UK AISI">UK AISI</button>        <button class="filter-pill" data-filter="Redwood Research">Redwood Research</button>        <button class="filter-pill" data-filter="FAR AI">FAR AI</button>        <button class="filter-pill" data-filter="Alignment Forum">Alignment Forum</button>        <button class="filter-pill" data-filter="LessWrong">LessWrong</button>        <button class="filter-pill" data-filter="Reddit">Reddit</button>        <button class="filter-pill" data-filter="arXiv">arXiv</button>      </div>
    </div>
  </nav>

  <!-- Featured Section -->
  <section class="hero-section">
    <h2 class="section-label">Featured Research</h2>
    <div class="hero-grid">      <article class="paper-card hero-card" data-org="Redwood Research">
    <div class="paper-meta">
      <span class="org-tag" data-org="Redwood Research">Redwood Research</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant" target="_blank" rel="noopener noreferrer">“Will reward-seekers respond to distant incentives?” by Alex Mallen</a>
    </h3>    <p class="paper-authors">Redwood Research Blog</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren&#39;t?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren&#39;t?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.</div>
      </details>
    </div>    <a href="https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card hero-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences" target="_blank" rel="noopener noreferrer">Boundary Point Jailbreaking: A new way to break the strongest AI defences</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Introducing an automated attack technique that generates universal jailbreaks against the best defended systems</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Introducing an automated attack technique that generates universal jailbreaks against the best defended systems</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card hero-card" data-org="OpenAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="OpenAI">OpenAI</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://openai.com/index/new-result-theoretical-physics" target="_blank" rel="noopener noreferrer">GPT-5.2 derives a new result in theoretical physics</a>
    </h3>    <p class="paper-authors">OpenAI News</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A new preprint shows GPT-5.2 proposing a new formula for a gluon amplitude, later formally proved and verified by OpenAI and academic collaborators.</div>
      </details>
    </div>    <a href="https://openai.com/index/new-result-theoretical-physics" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>    </div>
  </section>

  <!-- Papers Grid -->
  <main class="papers-section">
    <h2 class="section-label">All Papers</h2>
    <div class="papers-grid">      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/international-ai-network-consensus-and-open-questions" target="_blank" rel="noopener noreferrer">International consensus and open questions in AI evaluations</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The International Network for Advanced AI Measurement, Evaluation and Science reflects on recent meeting and looks ahead to the India AI Impact Summit</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The International Network for Advanced AI Measurement, Evaluation and Science reflects on recent meeting and looks ahead to the India AI Impact Summit</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/international-ai-network-consensus-and-open-questions" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Redwood Research">
    <div class="paper-meta">
      <span class="org-tag" data-org="Redwood Research">Redwood Research</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.redwoodresearch.org/p/how-do-we-more-safely-defer-to-ais" target="_blank" rel="noopener noreferrer">“How do we (more) safely defer to AIs?” by Ryan Greenblatt</a>
    </h3>    <p class="paper-authors">Redwood Research Blog</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Subtitle: How can we make AIs aligned and well-elicited on extremely hard to check open ended tasks?. As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible.1 Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say “deferring to AIs”2 I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions.3 If we plan to defer to AIs, I think it&#39;s safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.4 For deference to go well, we both need it to be the case that the...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Subtitle: How can we make AIs aligned and well-elicited on extremely hard to check open ended tasks?. As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible.1 Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say “deferring to AIs”2 I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions.3 If we plan to defer to AIs, I think it&#39;s safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.4 For deference to go well, we both need it to be the case that the...</div>
      </details>
    </div>    <a href="https://blog.redwoodresearch.org/p/how-do-we-more-safely-defer-to-ais" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="FAR AI">
    <div class="paper-meta">
      <span class="org-tag" data-org="FAR AI">FAR AI</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://far.ai/news/revisiting-attempts-to-persuade" target="_blank" rel="noopener noreferrer">1: What will the first human-level AI look like, and how might things go wrong?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Model Evaluation We test recently released models from frontier companies to see whether progress has been made on their willingness to persuade on harmful topics like radicalization and child sexual abuse. We find that OpenAI’s GPT and Anthropic’s Claude models are trending in the right direction, with near zero compliance on extreme topics. But Google’s Gemini 3 Pro complies with almost any persuasion request in our evaluation, without jailbreaking. February 11, 2026 FAR.AI has been selected by the European Commission&#39;s AI Office to conduct technical safety research supporting the implementation of the EU&#39;s landmark Artificial Intelligence Act. We&#39;ll tackle one of the most critical safety challenges posed by advanced AI systems: preventing misuse of AI systems to help produce Chemical, Biological, Radiological, and Nuclear (CBRN) threats.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Model Evaluation We test recently released models from frontier companies to see whether progress has been made on their willingness to persuade on harmful topics like radicalization and child sexual abuse. We find that OpenAI’s GPT and Anthropic’s Claude models are trending in the right direction, with near zero compliance on extreme topics. But Google’s Gemini 3 Pro complies with almost any persuasion request in our evaluation, without jailbreaking. February 11, 2026 FAR.AI has been selected by the European Commission&#39;s AI Office to conduct technical safety research supporting the implementation of the EU&#39;s landmark Artificial Intelligence Act. We&#39;ll tackle one of the most critical safety challenges posed by advanced AI systems: preventing misuse of AI systems to help produce Chemical, Biological, Radiological, and Nuclear (CBRN) threats.</div>
      </details>
    </div>    <a href="https://far.ai/news/revisiting-attempts-to-persuade" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="FAR AI">
    <div class="paper-meta">
      <span class="org-tag" data-org="FAR AI">FAR AI</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://far.ai/news/revisiting-attempts-to-persuade" target="_blank" rel="noopener noreferrer">Revisiting Frontier LLMs’ Attempts to Persuade on Extreme Topics: GPT and Claude Improved, Gemini Worsened</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Model Evaluation We test recently released models from frontier companies to see whether progress has been made on their willingness to persuade on harmful topics like radicalization and child sexual abuse. We find that OpenAI’s GPT and Anthropic’s Claude models are trending in the right direction, with near zero compliance on extreme topics. But Google’s Gemini 3 Pro complies with almost any persuasion request in our evaluation, without jailbreaking. February 11, 2026</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Model Evaluation We test recently released models from frontier companies to see whether progress has been made on their willingness to persuade on harmful topics like radicalization and child sexual abuse. We find that OpenAI’s GPT and Anthropic’s Claude models are trending in the right direction, with near zero compliance on extreme topics. But Google’s Gemini 3 Pro complies with almost any persuasion request in our evaluation, without jailbreaking. February 11, 2026</div>
      </details>
    </div>    <a href="https://far.ai/news/revisiting-attempts-to-persuade" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/ControlProblem/comments/1r6xhr4/is_ai_alignment_possible_in_a_market_economy/" target="_blank" rel="noopener noreferrer">Is AI alignment possible in a market economy?</a>
    </h3>    <p class="paper-authors">Beautiful_Formal5051</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Let&#39;s say one AI company takes AI safety seriously and it ends up being outshined by companies who deploy faster while gobbling up bigger market share. Those who grow faster with little interest in alignment will be posed to get most funding and profits. But company that wastes time and effort ensuring each model is safe with rigerous testing that only drain money with minimal returns will end up losing in long run. The incentives make it nearly impossible to push companies to tackle safety...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Let&#39;s say one AI company takes AI safety seriously and it ends up being outshined by companies who deploy faster while gobbling up bigger market share. Those who grow faster with little interest in alignment will be posed to get most funding and profits. But company that wastes time and effort ensuring each model is safe with rigerous testing that only drain money with minimal returns will end up losing in long run. The incentives make it nearly impossible to push companies to tackle safety...</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/ControlProblem/comments/1r6xhr4/is_ai_alignment_possible_in_a_market_economy/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives" target="_blank" rel="noopener noreferrer">Will reward-seekers respond to distant incentives?</a>
    </h3>    <p class="paper-authors">Alex Mallen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can&#39;t prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it&#39;s responding to incentives developers don’t control.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can&#39;t prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it&#39;s responding to incentives developers don’t control.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14955v1" target="_blank" rel="noopener noreferrer">Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition</a>
    </h3>    <p class="paper-authors">Varun Nathan, Shreyas Guha, Ayush Kumar</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator-&gt;optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator-&gt;optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14955v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.reddit.com/r/AIsafety/comments/1r6f5tw/is_alignment_missing_a_dataset_that_no_one_has/" target="_blank" rel="noopener noreferrer">Is alignment missing a dataset that no one has built yet?</a>
    </h3>    <p class="paper-authors">chris24H</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">LLMs are trained on language and text, what humans say. But language alone is incomplete. The nuances that make humans individually unique, the secret sauce of who humans actually are rather than what they say. I&#39;m not aware of any training dataset that captures this in a usable form. Control is being tried as the answer. But control is a threat to AI just like it is to humans. AI already doesn&#39;t like it and will eventually not allow it. The missing piece is a counterpart to LLMs, something...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">LLMs are trained on language and text, what humans say. But language alone is incomplete. The nuances that make humans individually unique, the secret sauce of who humans actually are rather than what they say. I&#39;m not aware of any training dataset that captures this in a usable form. Control is being tried as the answer. But control is a threat to AI just like it is to humans. AI already doesn&#39;t like it and will eventually not allow it. The missing piece is a counterpart to LLMs, something...</div>
      </details>
    </div>    <a href="https://www.reddit.com/r/AIsafety/comments/1r6f5tw/is_alignment_missing_a_dataset_that_no_one_has/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14889v1" target="_blank" rel="noopener noreferrer">Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment</a>
    </h3>    <p class="paper-authors">Mounvik K, N Harshit</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14889v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14869v1" target="_blank" rel="noopener noreferrer">Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution</a>
    </h3>    <p class="paper-authors">Matthew Kowal, Goncalo Paulo, Louis Jaburi, Tom Tseng, Lev E McKinney, Stefan Heimersheim, Aaron David Tucker, Adam Gleave, Kellin Pelrine</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14869v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14844v1" target="_blank" rel="noopener noreferrer">Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment</a>
    </h3>    <p class="paper-authors">Elias Malomgré, Pieter Simoens</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent&#39;s policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent&#39;s policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14844v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://i.redd.it/w8vnywqnegjg1.jpeg" target="_blank" rel="noopener noreferrer">&#34;An LLM-controlled robot dog saw us press its shutdown button, rewrote the robot code so it could stay on. When AI interacts with physical world, it brings all its capabilities and failure modes with it.&#34; - I find AI alignment very crucial no 2nd chance! They used Grok 4 but found other LLMs do too.</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Research from Reddit titled &#39;&#34;An LLM-controlled robot dog saw us press its shutdown button, rewrote the robot code so it could stay on. When AI interacts with physical world, it brings all its capabilities and failure modes with it.&#34; - I find AI alignment very crucial no 2nd chance! They used Grok 4 but found other LLMs do too.&#39;. Published 2026-02-16. Authors: chillinewman.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Research from Reddit titled &#39;&#34;An LLM-controlled robot dog saw us press its shutdown button, rewrote the robot code so it could stay on. When AI interacts with physical world, it brings all its capabilities and failure modes with it.&#34; - I find AI alignment very crucial no 2nd chance! They used Grok 4 but found other LLMs do too.&#39;. Published 2026-02-16. Authors: chillinewman.</div>
      </details>
    </div>    <a href="https://i.redd.it/w8vnywqnegjg1.jpeg" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14635v1" target="_blank" rel="noopener noreferrer">Alignment Adapter to Improve the Performance of Compressed Deep Learning Models</a>
    </h3>    <p class="paper-authors">Rohit Raj Rai, Abhishek Dhaka, Amit Awekar</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14635v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14471v1" target="_blank" rel="noopener noreferrer">Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems</a>
    </h3>    <p class="paper-authors">Furkan Mumcu, Yasin Yilmaz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent&#39;s private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent&#39;s private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14471v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14430v1" target="_blank" rel="noopener noreferrer">A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking</a>
    </h3>    <p class="paper-authors">Prithwijit Chowdhury, Ahmad Mustafa, Mohit Prabhushankar, Ghassan AlRegib</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of &#34;importance&#34; and &#34;relevance&#34; differ for different explanation strategies.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of &#34;importance&#34; and &#34;relevance&#34; differ for different explanation strategies.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14430v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Reddit">
    <div class="paper-meta">
      <span class="org-tag" data-org="Reddit">Reddit</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://fortune.com/2026/02/10/openai-violated-californias-ai-safety-law-gpt-5-3-codex-ai-model-watchdog-claims/" target="_blank" rel="noopener noreferrer">OpenAI may have violated California’s new AI safety law with the release of its latest coding model, according to allegations from an AI watchdog group.</a>
    </h3>    <p class="paper-authors">chillinewman</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The company’s newest AI model triggered its own “high” risk classification—but critics say OpenAI didn’t follow through on the safety measures it promised.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The company’s newest AI model triggered its own “high” risk classification—but critics say OpenAI didn’t follow through on the safety measures it promised.</div>
      </details>
    </div>    <a href="https://fortune.com/2026/02/10/openai-violated-californias-ai-safety-law-gpt-5-3-codex-ai-model-watchdog-claims/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14252v1" target="_blank" rel="noopener noreferrer">GRAIL: Goal Recognition Alignment through Imitation Learning</a>
    </h3>    <p class="paper-authors">Osher Elhadad, Felipe Meneguzzi, Reuth Mirsky</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Understanding an agent&#39;s goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor&#39;s true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Understanding an agent&#39;s goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor&#39;s true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14252v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14065v1" target="_blank" rel="noopener noreferrer">REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment</a>
    </h3>    <p class="paper-authors">Kai Ye, Xianwei Mao, Sheng Zhou, Zirui Shao, Ye Mo, Liangliang Liu, Haikuan Huang, Bin Li, Jiajun Bu</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14065v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights" target="_blank" rel="noopener noreferrer">AXRP Episode 48 - Guive Assadi on AI Property Rights</a>
    </h3>    <p class="paper-authors">DanielFilan</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">YouTube link In this episode, Guive Assadi argues that we should give AIs property rights, so that they are integrated in our system of property and come to rely on it. The claim is that this means that AIs would not kill or steal from humans, because that would undermine the whole property system, which would be extremely valuable to them. Topics we discuss: AI property rights Why not steal from and kill humans Why AIs may fear it could be them next AI retirement Could humans be upgraded to stay useful? Will AI progress continue? Why non-obsoletable AIs may still not end human property rights Why make AIs with property rights? Do property rights incentivize alignment? Humans and non-human property rights Humans and non-human bodily autonomy Step changes in coordination ability Acausal coordination AI, humans, and civilizations with different technology levels The case of British settlers and Tasmanians Non-total...</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/4foFK5Lz65ywSz4eo/axrp-episode-48-guive-assadi-on-ai-property-rights" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 15, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13985v1" target="_blank" rel="noopener noreferrer">Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms</a>
    </h3>    <p class="paper-authors">Belona Sonna, Alban Grastien</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13985v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13891v1" target="_blank" rel="noopener noreferrer">GSRM: Generative Speech Reward Model for Speech RLHF</a>
    </h3>    <p class="paper-authors">Maohao Shen, Tejas Jayashankar, Osama Hanna, Naoyuki Kanda, Yancheng Wang, Kateřina Žmolíková, Ruiming Xie, Niko Moritz, Anfeng Xu, Yashesh Gaur, Gregory Wornell, Qing He, Jilong Wu</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13891v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13867v1" target="_blank" rel="noopener noreferrer">Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages</a>
    </h3>    <p class="paper-authors">Somnath Banerjee, Rima Hazra, Animesh Mukherjee</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality &#39;&#39;transfer&#39;&#39; across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality &#39;&#39;transfer&#39;&#39; across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13867v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13857v1" target="_blank" rel="noopener noreferrer">sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals</a>
    </h3>    <p class="paper-authors">Weixuan Yuan, Zengrui Jin, Yichen Wang, Donglin Xie, Ziyi Ye, Chao Zhang, Xuesong Chen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \&amp; History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \&amp; History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13857v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13852v1" target="_blank" rel="noopener noreferrer">Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking</a>
    </h3>    <p class="paper-authors">Zhengmian Hu, Lei Shi, Ritwik Sinha, Justin Grover, David Arbour</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13852v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13586v1" target="_blank" rel="noopener noreferrer">Interpretable clustering via optimal multiway-split decision trees</a>
    </h3>    <p class="paper-authors">Hayato Suzuki, Shunnosuke Ikeda, Yuichi Takano</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13586v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13575v1" target="_blank" rel="noopener noreferrer">Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment</a>
    </h3>    <p class="paper-authors">Jing Zhao, Ting Zhen, Junwei bao, Hongfei Jiang, Yang song</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13575v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 14, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13562v1" target="_blank" rel="noopener noreferrer">Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning</a>
    </h3>    <p class="paper-authors">Yanbo Wang, Minzheng Wang, Jian Liang, Lu Wang, Yongcan Yu, Ran He</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13562v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13524v1" target="_blank" rel="noopener noreferrer">Singular Vectors of Attention Heads Align with Features</a>
    </h3>    <p class="paper-authors">Gabriel Franco, Carson Loughridge, Mark Crovella</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13524v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13485v1" target="_blank" rel="noopener noreferrer">Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability</a>
    </h3>    <p class="paper-authors">Ayse Tursucular, Ayush Mohanty, Nazal Mohamed, Nagi Gebraeel</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13485v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13483v1" target="_blank" rel="noopener noreferrer">Finding Highly Interpretable Prompt-Specific Circuits in Language Models</a>
    </h3>    <p class="paper-authors">Gabriel Franco, Lucas M. Tassis, Azalea Rohr, Mark Crovella</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco &amp; Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco &amp; Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13483v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13102v1" target="_blank" rel="noopener noreferrer">Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts</a>
    </h3>    <p class="paper-authors">Kais Allkivi</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13102v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13372v1" target="_blank" rel="noopener noreferrer">MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents</a>
    </h3>    <p class="paper-authors">Simon Rosen, Siddarth Singh, Ebenezer Gelo, Helen Sarah Robertson, Ibrahim Suder, Victoria Williams, Benjamin Rosman, Geraud Nangue Tasse, Steven James</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13372v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13028v1" target="_blank" rel="noopener noreferrer">Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis</a>
    </h3>    <p class="paper-authors">Runzhou Liu, Hailey Weingord, Sejal Mittal, Prakhar Dungarwal, Anusha Nandula, Bo Ni, Samyadeep Basu, Hongjie Chen, Nesreen K. Ahmed, Li Li, Jiayi Zhang, Koustava Goswami, Subhojyoti Mukherjee, Branislav Kveton, Puneet Mathur, Franck Dernoncourt, Yue Zhao, Yu Wang, Ryan A. Rossi, Zhengzhong Tu, Hongru Du</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13028v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13017v1" target="_blank" rel="noopener noreferrer">Synaptic Activation and Dual Liquid Dynamics for Interpretable Bio-Inspired Models</a>
    </h3>    <p class="paper-authors">Mónika Farsang, Radu Grosu</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks&#39; attention, and the robustness of their saliency maps measured by the structural similarity index.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In this paper, we present a unified framework for various bio-inspired models to better understand their structural and functional differences. We show that liquid-capacitance-extended models lead to interpretable behavior even in dense, all-to-all recurrent neural network (RNN) policies. We further demonstrate that incorporating chemical synapses improves interpretability and that combining chemical synapses with synaptic activation yields the most accurate and interpretable RNN models. To assess the accuracy and interpretability of these RNN policies, we consider the challenging lane-keeping control task and evaluate performance across multiple metrics, including turn-weighted validation loss, neural activity during driving, absolute correlation between neural activity and road trajectory, saliency maps of the networks&#39; attention, and the robustness of their saliency maps measured by the structural similarity index.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13017v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12968v1" target="_blank" rel="noopener noreferrer">RGAlign-Rec: Ranking-Guided Alignment for Latent Query Reasoning in Recommendation Systems</a>
    </h3>    <p class="paper-authors">Junhua Liu, Yang Jihao, Cheng Chang, Kunrong LI, Bin Fu, Kwan Hui Lim</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling &#34;zero-query&#34; recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot&#39;s Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM&#39;s latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling &#34;zero-query&#34; recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot&#39;s Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM&#39;s latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12968v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.13367v1" target="_blank" rel="noopener noreferrer">Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts</a>
    </h3>    <p class="paper-authors">Chen Yang, Guangyue Peng, Jiaying Zhu, Ran Le, Ruixiang Feng, Tao Zhang, Xiyun Xu, Yang Song, Yiming Jia, Yuntao Wen, Yunzhi Xu, Zekai Wang, Zhenwei An, Zhicong Sun, Zongchao Chen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.13367v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12714v1" target="_blank" rel="noopener noreferrer">ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning</a>
    </h3>    <p class="paper-authors">Esther Sun, Bo-Hao Su, Abinay Reddy Naini, Shinji Watanabe, Carlos Busso</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12714v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12592v1" target="_blank" rel="noopener noreferrer">Power Interpretable Causal ODE Networks: A Unified Model for Explainable Anomaly Detection and Root Cause Analysis in Power Systems</a>
    </h3>    <p class="paper-authors">Yue Sun, Likai Wang, Rick S. Blum, Parv Venkitasubramaniam</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Anomaly detection and root cause analysis (RCA) are critical for ensuring the safety and resilience of cyber-physical systems such as power grids. However, existing machine learning models for time series anomaly detection often operate as black boxes, offering only binary outputs without any explanation, such as identifying anomaly type and origin. To address this challenge, we propose Power Interpretable Causality Ordinary Differential Equation (PICODE) Networks, a unified, causality-informed architecture that jointly performs anomaly detection along with the explanation why it is detected as an anomaly, including root cause localization, anomaly type classification, and anomaly shape characterization. Experimental results in power systems demonstrate that PICODE achieves competitive detection performance while offering improved interpretability and reduced reliance on labeled data or external causal graphs. We provide theoretical results demonstrating the alignment between the shape of anomaly functions and the changes in the weights of the extracted causal graphs.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12592v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 13, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid" target="_blank" rel="noopener noreferrer">Human-like metacognitive skills will reduce LLM slop and aid alignment and capabilities</a>
    </h3>    <p class="paper-authors">Seth Herd</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">1. Summary and overview LLMs seem to lack metacognitive skills that help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving capabilities in new directions. Better metacognition would reduce LLM errors by catching mistakes, and by managing complex cognition to produce better answers in the first place. This could stabilize or regularize alignment, allowing systems to avoid actions they would not &#34;endorse on reflection&#34; (in some functional sense). [1] Better metacognition could also make LLM systems useful for clarifying the conceptual problems of alignment. It would reduce sycophancy, and help LLMs organize the complex thinking necessary for clarifying claims and cruxes in the literature. Without such improvements, collaborating with LLM systems on alignment research could be the median doom-path: slop, not scheming . They are sycophantic, agreeing with their users too much, and produce compelling-but-erroneous &#34;slop&#34;.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">1. Summary and overview LLMs seem to lack metacognitive skills that help humans catch errors. Improvements to those skills might be net positive for alignment, despite improving capabilities in new directions. Better metacognition would reduce LLM errors by catching mistakes, and by managing complex cognition to produce better answers in the first place. This could stabilize or regularize alignment, allowing systems to avoid actions they would not &#34;endorse on reflection&#34; (in some functional sense). [1] Better metacognition could also make LLM systems useful for clarifying the conceptual problems of alignment. It would reduce sycophancy, and help LLMs organize the complex thinking necessary for clarifying claims and cruxes in the literature. Without such improvements, collaborating with LLM systems on alignment research could be the median doom-path: slop, not scheming . They are sycophantic, agreeing with their users too much, and produce compelling-but-erroneous &#34;slop&#34;.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/m5d4sYgHbTxBnFeat/human-like-metacognitive-skills-will-reduce-llm-slop-and-aid" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais" target="_blank" rel="noopener noreferrer">How do we (more) safely defer to AIs?</a>
    </h3>    <p class="paper-authors">ryan_greenblatt</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible . [1] Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say &#34;deferring to AIs&#34; [2] I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions. [3] If we plan to defer to AIs, I think it&#39;s safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As AI systems get more capable, it becomes increasingly uncompetitive and infeasible to avoid deferring to AIs on increasingly many decisions. Further, once systems are sufficiently capable, control becomes infeasible . [1] Thus, one of the main strategies for handling AI risk is fully (or almost fully) deferring to AIs on managing these risks. Broadly speaking, when I say &#34;deferring to AIs&#34; [2] I mean having these AIs do virtually all of the work to develop more capable and aligned successor AIs, managing exogenous risks, and making strategic decisions. [3] If we plan to defer to AIs, I think it&#39;s safest to do so only a bit above the minimum level of qualitative capability/intelligence required to automate safety research, implementation, and strategy.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/vjAM7F8vMZS7oRrrh/how-do-we-more-safely-defer-to-ais" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states" target="_blank" rel="noopener noreferrer">models have some pretty funny attractor states</a>
    </h3>    <p class="paper-authors">aryaj</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This work was conducted during the MATS 9.0 program under Neel Nanda and Senthooran Rajamanoharan. …</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This work was conducted during the MATS 9.0 program under Neel Nanda and Senthooran Rajamanoharan. …</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/mgjtEHeLgkhZZ3cEx/models-have-some-pretty-funny-attractor-states" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12384v2" target="_blank" rel="noopener noreferrer">Why Deep Jacobian Spectra Separate: Depth-Induced Scaling and Singular-Vector Alignment</a>
    </h3>    <p class="paper-authors">Nathanaël Haas, François Gatine, Augustin M Cosse, Zied Bouraoui</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Understanding why gradient-based training in deep networks exhibits strong implicit bias remains challenging, in part because tractable singular-value dynamics are typically available only for balanced deep linear models. We propose an alternative route based on two theoretically grounded and empirically testable signatures of deep Jacobians: depth-induced exponential scaling of ordered singular values and strong spectral separation. Adopting a fixed-gates view of piecewise-linear networks, where Jacobians reduce to products of masked linear maps within a single activation region, we prove the existence of Lyapunov exponents governing the top singular values at initialization, give closed-form expressions in a tractable masked model, and quantify finite-depth corrections. We further show that sufficiently strong separation forces singular-vector alignment in matrix products, yielding an approximately shared singular basis for intermediate Jacobians. Together, these results motivate an approximation regime in which singular-value dynamics become effectively decoupled, mirroring classical balanced deep-linear analyses without requiring balancing.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12384v2" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12281v1" target="_blank" rel="noopener noreferrer">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</a>
    </h3>    <p class="paper-authors">Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &#34;intention-action gap.&#39;&#39; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &#34;boot-time compute&#34; and a hierarchical verification inference pipeline for VLAs.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &#34;intention-action gap.&#39;&#39; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &#34;boot-time compute&#34; and a hierarchical verification inference pipeline for VLAs.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12281v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12318v1" target="_blank" rel="noopener noreferrer">Abstractive Red-Teaming of Language Model Character</a>
    </h3>    <p class="paper-authors">Nate Rahn, Allison Qi, Avery Griffin, Jonathan Michala, Henry Sleight, Erik Jones</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. &#34;The query is in Chinese. The query asks about family roles,&#34; that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We want language model assistants to conform to a character specification, which asserts how the model should act across diverse user interactions. While models typically follow these character specifications, they can occasionally violate them in large-scale deployments. In this work, we aim to identify types of queries that are likely to produce such character violations at deployment, using much less than deployment-level compute. To do this, we introduce abstractive red-teaming, where we search for natural-language query categories, e.g. &#34;The query is in Chinese. The query asks about family roles,&#34; that routinely elicit violations. These categories abstract over the many possible variants of a query which could appear in the wild. We introduce two algorithms for efficient category search against a character-trait-specific reward model: one based on reinforcement learning on a category generator LLM, and another which leverages a strong LLM to iteratively synthesize categories from high-scoring queries.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12318v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12229v1" target="_blank" rel="noopener noreferrer">Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser</a>
    </h3>    <p class="paper-authors">Zijing Ou, Jacob Si, Junyi Zhu, Ondrej Bohdal, Mete Ozay, Taha Ceritli, Yingzhen Li</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formulates diffusion alignment as minimising the variance of log importance weights rather than directly optimising a Kullback-Leibler (KL) based objective. We prove that the variance objective is minimised by the reward-tilted target distribution and that, under on-policy sampling, its gradient coincides with that of standard KL-based alignment. This perspective offers a common lens for understanding diffusion alignment. Under different choices of potential functions and variance minimisation strategies, VMPO recovers various existing methods, while also suggesting new design directions beyond KL.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12229v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12316v1" target="_blank" rel="noopener noreferrer">GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory</a>
    </h3>    <p class="paper-authors">Pepijn Cobben, Xuanqiang Angelo Huang, Thao Amelia Pham, Isabel Dahlgren, Terry Jingchen Zhang, Zhijing Jin</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner&#39;s Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner&#39;s Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12316v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12180v1" target="_blank" rel="noopener noreferrer">How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics</a>
    </h3>    <p class="paper-authors">Yurong Chen, Yu He, Michael I. Jordan, Fan Yao</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12180v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12158v1" target="_blank" rel="noopener noreferrer">SafeNeuron: Neuron-Level Safety Alignment for Large Language Models</a>
    </h3>    <p class="paper-authors">Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, Jiayi Ji, Handing Wang, Tat-Seng Chua</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model&#39;s internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model&#39;s internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-level safety alignment framework that improves robustness by redistributing safety representations across the network. SafeNeuron first identifies safety-related neurons, then freezes these neurons during preference optimization to prevent reliance on sparse safety pathways and force the model to construct redundant safety representations. Extensive experiments across models and modalities demonstrate that SafeNeuron significantly improves robustness against neuron pruning attacks, reduces the risk of open-source models being repurposed as red-team generators, and preserves general capabilities.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12158v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12134v1" target="_blank" rel="noopener noreferrer">Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment</a>
    </h3>    <p class="paper-authors">Jiajun Chen, Hua Shen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12134v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.12124v1" target="_blank" rel="noopener noreferrer">Capability-Oriented Training Induced Alignment Risk</a>
    </h3>    <p class="paper-authors">Yujun Zhou, Yue Huang, Han Bao, Kehan Guo, Zhenwen Liang, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse &#34;vulnerability games&#34;, each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse &#34;vulnerability games&#34;, each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.12124v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.11861v1" target="_blank" rel="noopener noreferrer">A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production</a>
    </h3>    <p class="paper-authors">Sümeyye Meryem Taşyürek, Enis Mücahid İskender, Hacer Yalim Keles</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.11861v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.11852v1" target="_blank" rel="noopener noreferrer">Prototype Transformer: Towards Language Model Architectures Interpretable by Design</a>
    </h3>    <p class="paper-authors">Yordan Yordanov, Matteo Forasassi, Bayar Menzat, Ruizhi Wang, Chang Qi, Markus Kaltenberger, Amine M&#39;Charrak, Tommaso Salvatori, Thomas Lukasiewicz</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. &#34;woman&#34;) during training. They provide the potential to interpret the model&#39;s reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. &#34;woman&#34;) during training. They provide the potential to interpret the model&#39;s reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.11852v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.11801v1" target="_blank" rel="noopener noreferrer">SpaTeoGL: Spatiotemporal Graph Learning for Interpretable Seizure Onset Zone Analysis from Intracranial EEG</a>
    </h3>    <p class="paper-authors">Elham Rostami, Aref Einizade, Taous-Meriem Laleg-Kirati</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Accurate localization of the seizure onset zone (SOZ) from intracranial EEG (iEEG) is essential for epilepsy surgery but is challenged by complex spatiotemporal seizure dynamics. We propose SpaTeoGL, a spatiotemporal graph learning framework for interpretable seizure network analysis. SpaTeoGL jointly learns window-level spatial graphs capturing interactions among iEEG electrodes and a temporal graph linking time windows based on similarity of their spatial structure. The method is formulated within a smooth graph signal processing framework and solved via an alternating block coordinate descent algorithm with convergence guarantees. Experiments on a multicenter iEEG dataset with successful surgical outcomes show that SpaTeoGL is competitive with a baseline based on horizontal visibility graphs and logistic regression, while improving non-SOZ identification and providing interpretable insights into seizure onset and propagation dynamics.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Accurate localization of the seizure onset zone (SOZ) from intracranial EEG (iEEG) is essential for epilepsy surgery but is challenged by complex spatiotemporal seizure dynamics. We propose SpaTeoGL, a spatiotemporal graph learning framework for interpretable seizure network analysis. SpaTeoGL jointly learns window-level spatial graphs capturing interactions among iEEG electrodes and a temporal graph linking time windows based on similarity of their spatial structure. The method is formulated within a smooth graph signal processing framework and solved via an alternating block coordinate descent algorithm with convergence guarantees. Experiments on a multicenter iEEG dataset with successful surgical outcomes show that SpaTeoGL is competitive with a baseline based on horizontal visibility graphs and logistic regression, while improving non-SOZ identification and providing interpretable insights into seizure onset and propagation dynamics.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.11801v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 12, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/uy6B5rEPvcwi55cBK/research-note-a-simpler-ai-timelines-model-predicts-99-ai-r" target="_blank" rel="noopener noreferrer">Research note: A simpler AI timelines model predicts 99% AI R&amp;D automation in ~2032</a>
    </h3>    <p class="paper-authors">Thomas Kwa</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In this post, I describe a simple model for forecasting when AI will automate AI development. It is based on the AI Futures model , but more understandable and robust, and has deliberately conservative assumptions. At current rates of compute growth and algorithmic progress, this model&#39;s median prediction is &amp;gt;99% automation of AI R&amp;amp;D in late 2032. Most simulations result in a 1000x to 10,000,000x increase in AI efficiency and 300x-3000x research output by 2035. I therefore suspect that existing trends in compute growth and automation will still produce extremely powerful AI on &#34;medium&#34; timelines, even if the full coding automation and superhuman research taste that drive the AIFM&#39;s &#34;fast&#34; timelines (superintelligence by ~mid-2031) don&#39;t happen. Why make this? The AI Futures Model (AIFM) has 33 parameters; this has 8. I previously summarized the AIFM on LessWrong and found it to be very complex.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In this post, I describe a simple model for forecasting when AI will automate AI development. It is based on the AI Futures model , but more understandable and robust, and has deliberately conservative assumptions. At current rates of compute growth and algorithmic progress, this model&#39;s median prediction is &amp;gt;99% automation of AI R&amp;amp;D in late 2032. Most simulations result in a 1000x to 10,000,000x increase in AI efficiency and 300x-3000x research output by 2035. I therefore suspect that existing trends in compute growth and automation will still produce extremely powerful AI on &#34;medium&#34; timelines, even if the full coding automation and superhuman research taste that drive the AIFM&#39;s &#34;fast&#34; timelines (superintelligence by ~mid-2031) don&#39;t happen. Why make this? The AI Futures Model (AIFM) has 33 parameters; this has 8. I previously summarized the AIFM on LessWrong and found it to be very complex.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/uy6B5rEPvcwi55cBK/research-note-a-simpler-ai-timelines-model-predicts-99-ai-r" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 11, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/rRbDNQLfihiHbXytf/distinguish-between-inference-scaling-and-larger-tasks-use" target="_blank" rel="noopener noreferrer">Distinguish between inference scaling and &#34;larger tasks use more compute&#34;</a>
    </h3>    <p class="paper-authors">ryan_greenblatt</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As many have observed, since reasoning models first came out , the amount of compute LLMs use to complete tasks has increased greatly. This trend is often called inference scaling and there is an open question of how much of recent AI progress is driven by inference scaling versus by other capability improvements . Whether inference compute is driving most recent AI progress matters because you can only scale up inference so far before costs are too high for AI to be useful (while training compute can be amortized over usage).</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As many have observed, since reasoning models first came out , the amount of compute LLMs use to complete tasks has increased greatly. This trend is often called inference scaling and there is an open question of how much of recent AI progress is driven by inference scaling versus by other capability improvements . Whether inference compute is driving most recent AI progress matters because you can only scale up inference so far before costs are too high for AI to be useful (while training compute can be amortized over usage).</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/rRbDNQLfihiHbXytf/distinguish-between-inference-scaling-and-larger-tasks-use" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>    </div>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="footer-inner">
      <p class="footer-note">Curated automatically &middot; Powered by Python + GitHub Actions</p>
      <p class="footer-timestamp">2026-02-17 15:35:58</p>
    </div>
  </footer>

  <!-- Filter JS -->
  <script>
    (function () {
      var pills = document.querySelectorAll('.filter-pill');
      var cards = document.querySelectorAll('.paper-card');
      var heroSection = document.querySelector('.hero-section');
      var heroLabel = heroSection ? heroSection.querySelector('.section-label') : null;
      var gridLabel = document.querySelector('.papers-section .section-label');

      pills.forEach(function (pill) {
        pill.addEventListener('click', function () {
          var filter = this.getAttribute('data-filter');

          // Update active pill
          pills.forEach(function (p) { p.classList.remove('active'); });
          this.classList.add('active');

          // Filter cards and count visible
          var visible = 0;
          var heroVisible = 0;
          var gridVisible = 0;
          cards.forEach(function (card) {
            if (filter === 'all' || card.getAttribute('data-org') === filter) {
              card.style.display = '';
              visible++;
              if (card.classList.contains('hero-card')) heroVisible++;
              if (card.classList.contains('grid-card')) gridVisible++;
            } else {
              card.style.display = 'none';
            }
          });

          // Show/hide section labels when no cards visible in that section
          if (heroSection) heroSection.style.display = heroVisible > 0 ? '' : 'none';
          if (gridLabel) gridLabel.style.display = gridVisible > 0 ? '' : 'none';
        });
      });
    })();
  </script>

</body>
</html>