<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Safety Weekly Digest — February 16, 2026 to February 23, 2026</title>
  <meta name="description" content="A curated weekly digest of 57 AI safety research papers from leading organizations and researchers. February 16, 2026 through February 23, 2026.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  <style>
    /* =============================================
   AI Safety Weekly Digest — Editorial Stylesheet
   Modern research publication aesthetic
   ============================================= */

/* --- CSS Custom Properties --- */
:root {
  /* Base palette */
  --bg: #f8f8f7;
  --bg-card: #ffffff;
  --bg-elevated: #ffffff;
  --bg-hero: #fcfcfb;
  --text: #1a1a1a;
  --text-secondary: #4a4a4a;
  --text-muted: #6b6b6b;
  --text-faint: #999999;
  --heading: #0d0d0d;
  --border: #e6e4e0;
  --border-light: #f0eee9;
  --shadow-sm: 0 1px 3px rgba(0, 0, 0, 0.04);
  --shadow-md: 0 4px 12px rgba(0, 0, 0, 0.06);
  --shadow-lg: 0 8px 30px rgba(0, 0, 0, 0.08);

  /* Accent — deep teal */
  --accent: #0d6e6e;
  --accent-light: #0f8585;
  --accent-subtle: rgba(13, 110, 110, 0.07);
  --accent-text: #0a5a5a;

  /* Links */
  --link: #0d6e6e;
  --link-hover: #0a5252;

  /* Filter bar */
  --filter-bg: rgba(248, 248, 247, 0.88);
  --pill-active-bg: #1a1a1a;
  --pill-active-text: #ffffff;

  /* Card accent fallback */
  --card-accent: var(--org-default);

  /* Spacing scale */
  --space-xs: 0.25rem;
  --space-sm: 0.5rem;
  --space-md: 1rem;
  --space-lg: 1.5rem;
  --space-xl: 2rem;
  --space-2xl: 3rem;
  --space-3xl: 4rem;

  /* Radii */
  --radius-sm: 6px;
  --radius-md: 10px;
  --radius-lg: 14px;

  /* Organization colors — muted, professional tones */
  --org-anthropic: #c4854a;
  --org-openai: #4a8c5c;
  --org-google-deepmind: #4a7ab8;
  --org-metr: #b85a5a;
  --org-apollo-research: #7b5ab8;
  --org-uk-aisi: #3a7a8a;
  --org-redwood-research: #c04040;
  --org-alignment-forum: #4a8a4a;
  --org-arxiv: #777777;
  --org-hyperdimensional: #b8924a;
  --org-peter-wildeford: #5a5ab8;
  --org-zvi-mowshowitz: #8a6a3a;
  --org-rand: #3a6a8a;
  --org-arc: #8a3a6a;
  --org-miri: #5a8a3a;
  --org-cais: #3a8a6a;
  --org-microsoft-research: #3a6ab8;
  --org-import-ai: #b8943a;
  --org-dan-hendrycks: #7a6a3a;
  --org-astral-codex-ten: #5a6a8a;
  --org-cognitive-revolution: #8a5a6a;
  --org-vox-future-perfect: #3a8a6a;
  --org-fli: #7a5a7a;
  --org-epoch-ai: #5a7a6a;
  --org-lesswrong: #4a8a4a;
  --org-far-ai: #6a4a8a;
  --org-us-aisi: #3a5a8a;
  --org-cset: #6a7a3a;
  --org-govai: #4a5a7a;
  --org-iaps: #7a4a5a;
  --org-cltr: #5a4a7a;
  --org-chai: #8a6a4a;
  --org-mats: #4a7a7a;
  --org-paul-christiano: #7a6a4a;
  --org-yoshua-bengio: #4a6a7a;
  --org-lennart-heim: #6a7a4a;
  --org-semianalysis: #b86a3a;
  --org-dean-ball: #b8924a;
  --org-seb-krier: #6a8a4a;
  --org-ajeya-cotra: #8a4a6a;
  --org-cnas: #4a6a5a;
  --org-hacker-news: #e86424;
  --org-reddit: #e84420;
  --org-default: #888888;
}

/* --- Dark Mode --- */
@media (prefers-color-scheme: dark) {
  :root {
    --bg: #111113;
    --bg-card: #1a1a1e;
    --bg-elevated: #222226;
    --bg-hero: #161618;
    --text: #e2e2e6;
    --text-secondary: #b0b0b8;
    --text-muted: #8a8a96;
    --text-faint: #606068;
    --heading: #f0f0f4;
    --border: #2a2a30;
    --border-light: #222228;
    --shadow-sm: 0 1px 3px rgba(0, 0, 0, 0.15);
    --shadow-md: 0 4px 12px rgba(0, 0, 0, 0.25);
    --shadow-lg: 0 8px 30px rgba(0, 0, 0, 0.35);
    --accent: #2ca0a0;
    --accent-light: #35b5b5;
    --accent-subtle: rgba(44, 160, 160, 0.1);
    --accent-text: #2ca0a0;
    --link: #35b5b5;
    --link-hover: #4acaca;
    --filter-bg: rgba(17, 17, 19, 0.88);
    --pill-active-bg: #2ca0a0;
    --pill-active-text: #111113;
  }
}

/* --- Reset & Base --- */
*,
*::before,
*::after {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

html {
  -webkit-text-size-adjust: 100%;
  scroll-behavior: smooth;
}

body {
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
  background-color: var(--bg);
  color: var(--text);
  max-width: 1120px;
  margin: 0 auto;
  padding: 0 clamp(1.25rem, 5vw, 2.5rem);
  font-size: 15px;
  line-height: 1.6;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

a {
  color: var(--link);
  text-decoration: none;
  transition: color 0.15s ease;
}

a:hover {
  color: var(--link-hover);
}

/* --- Masthead / Header --- */
.site-header {
  padding: var(--space-3xl) 0 var(--space-xl);
  text-align: center;
  border-bottom: 1px solid var(--border);
}

.header-inner {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: var(--space-sm);
}

.site-title {
  font-size: clamp(1.75rem, 1.4rem + 1.8vw, 2.75rem);
  font-weight: 800;
  color: var(--heading);
  letter-spacing: -0.04em;
  line-height: 1.1;
}

.header-rule {
  width: 40px;
  height: 2px;
  background: var(--accent);
  border: none;
  margin: var(--space-sm) 0;
  border-radius: 1px;
}

.header-meta {
  display: flex;
  align-items: center;
  gap: var(--space-sm);
  flex-wrap: wrap;
  justify-content: center;
}

.header-daterange {
  font-size: 0.92rem;
  font-weight: 500;
  color: var(--text-secondary);
}

.header-separator {
  color: var(--border);
  font-weight: 300;
}

.header-count {
  font-size: 0.85rem;
  color: var(--text-muted);
}

.header-count strong {
  font-weight: 600;
  color: var(--accent-text);
}

/* --- Filter Bar --- */
.filter-bar {
  position: sticky;
  top: 0;
  z-index: 100;
  margin: 0 calc(-1 * clamp(1.25rem, 5vw, 2.5rem));
  padding: 0 clamp(1.25rem, 5vw, 2.5rem);
  -webkit-backdrop-filter: blur(20px) saturate(180%);
  backdrop-filter: blur(20px) saturate(180%);
  background-color: var(--filter-bg);
  border-bottom: 1px solid var(--border-light);
}

.filter-bar-inner {
  padding: 0.7rem 0;
}

.filter-scroll {
  display: flex;
  gap: 0.35rem;
  overflow-x: auto;
  padding-bottom: 2px;
  scrollbar-width: thin;
  scrollbar-color: var(--border) transparent;
  -webkit-overflow-scrolling: touch;
}

.filter-scroll::-webkit-scrollbar {
  height: 2px;
}

.filter-scroll::-webkit-scrollbar-thumb {
  background: var(--border);
  border-radius: 2px;
}

.filter-pill {
  flex-shrink: 0;
  padding: 0.32rem 0.75rem;
  font-size: 0.76rem;
  font-family: inherit;
  font-weight: 500;
  border: 1px solid var(--border);
  border-radius: 100px;
  background: transparent;
  color: var(--text-muted);
  cursor: pointer;
  transition: all 0.15s ease;
  white-space: nowrap;
  line-height: 1.4;
}

.filter-pill:hover {
  border-color: var(--text-faint);
  color: var(--text-secondary);
  background-color: var(--bg-elevated);
}

.filter-pill.active {
  background-color: var(--pill-active-bg);
  border-color: var(--pill-active-bg);
  color: var(--pill-active-text);
  font-weight: 600;
}

.pill-count {
  font-size: 0.68rem;
  opacity: 0.65;
  margin-left: 0.15rem;
}

/* --- Section Labels --- */
.section-label {
  font-size: 0.7rem;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 0.1em;
  color: var(--text-faint);
  margin-bottom: var(--space-lg);
  padding-bottom: var(--space-sm);
  border-bottom: 1px solid var(--border-light);
}

/* --- Featured / Hero Section --- */
.hero-section {
  padding-top: var(--space-xl);
  padding-bottom: var(--space-md);
}

.hero-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  grid-template-rows: auto auto;
  gap: var(--space-md);
}

/* First featured paper: spans the full left column across both rows */
.hero-grid .hero-card:first-child {
  grid-row: 1 / 3;
}

/* --- Base Paper Card --- */
.paper-card {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: var(--radius-md);
  padding: var(--space-lg);
  transition: border-color 0.2s ease, box-shadow 0.25s ease, transform 0.2s ease;
  position: relative;
  display: flex;
  flex-direction: column;
}

.paper-card:hover {
  border-color: color-mix(in srgb, var(--card-accent) 35%, var(--border));
  box-shadow: var(--shadow-md);
}

/* --- Hero Card --- */
.hero-card {
  border-radius: var(--radius-lg);
  padding: clamp(1.25rem, 3vw, 2rem);
  border-left: 3px solid var(--card-accent);
}

.hero-card:hover {
  transform: translateY(-2px);
  box-shadow: var(--shadow-lg);
}

.hero-card .paper-title {
  font-size: clamp(1.15rem, 1rem + 0.5vw, 1.4rem);
  font-weight: 700;
  line-height: 1.28;
  margin-bottom: var(--space-sm);
}

.hero-card .paper-authors {
  font-size: 0.84rem;
  margin-bottom: var(--space-xs);
}

.hero-card .abstract-preview {
  -webkit-line-clamp: 8;
}

.hero-card .paper-abstract {
  flex: 1;
}

/* --- Papers Grid --- */
.papers-section {
  padding-top: var(--space-xl);
  padding-bottom: var(--space-2xl);
}

.papers-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
  gap: var(--space-md);
}

/* --- Grid Card --- */
.grid-card {
  border-top: 3px solid var(--card-accent);
  border-left: none;
}

.grid-card .paper-title {
  font-size: clamp(0.95rem, 0.9rem + 0.2vw, 1.05rem);
  margin-bottom: var(--space-xs);
}

.grid-card .paper-abstract {
  flex: 1;
}

.grid-card .read-link {
  margin-top: auto;
  padding-top: var(--space-sm);
}

/* Card accent colors per org */
.paper-card[data-org="Anthropic"] { --card-accent: var(--org-anthropic); }
.paper-card[data-org="OpenAI"] { --card-accent: var(--org-openai); }
.paper-card[data-org="Google DeepMind"] { --card-accent: var(--org-google-deepmind); }
.paper-card[data-org="METR"] { --card-accent: var(--org-metr); }
.paper-card[data-org="Apollo Research"] { --card-accent: var(--org-apollo-research); }
.paper-card[data-org="UK AISI"] { --card-accent: var(--org-uk-aisi); }
.paper-card[data-org="Redwood Research"] { --card-accent: var(--org-redwood-research); }
.paper-card[data-org="Alignment Forum"] { --card-accent: var(--org-alignment-forum); }
.paper-card[data-org="arXiv"] { --card-accent: var(--org-arxiv); }
.paper-card[data-org="Hyperdimensional"] { --card-accent: var(--org-hyperdimensional); }
.paper-card[data-org="Peter Wildeford"] { --card-accent: var(--org-peter-wildeford); }
.paper-card[data-org="Zvi Mowshowitz"] { --card-accent: var(--org-zvi-mowshowitz); }
.paper-card[data-org="RAND"] { --card-accent: var(--org-rand); }
.paper-card[data-org="ARC"] { --card-accent: var(--org-arc); }
.paper-card[data-org="MIRI"] { --card-accent: var(--org-miri); }
.paper-card[data-org="CAIS"] { --card-accent: var(--org-cais); }
.paper-card[data-org="Microsoft Research"] { --card-accent: var(--org-microsoft-research); }
.paper-card[data-org="Import AI"] { --card-accent: var(--org-import-ai); }
.paper-card[data-org="Dan Hendrycks"] { --card-accent: var(--org-dan-hendrycks); }
.paper-card[data-org="Astral Codex Ten"] { --card-accent: var(--org-astral-codex-ten); }
.paper-card[data-org="Cognitive Revolution"] { --card-accent: var(--org-cognitive-revolution); }
.paper-card[data-org="Vox Future Perfect"] { --card-accent: var(--org-vox-future-perfect); }
.paper-card[data-org="FLI"] { --card-accent: var(--org-fli); }
.paper-card[data-org="Epoch AI"] { --card-accent: var(--org-epoch-ai); }
.paper-card[data-org="LessWrong"] { --card-accent: var(--org-lesswrong); }
.paper-card[data-org="FAR AI"] { --card-accent: var(--org-far-ai); }
.paper-card[data-org="US AISI"] { --card-accent: var(--org-us-aisi); }
.paper-card[data-org="CSET"] { --card-accent: var(--org-cset); }
.paper-card[data-org="GovAI"] { --card-accent: var(--org-govai); }
.paper-card[data-org="IAPS"] { --card-accent: var(--org-iaps); }
.paper-card[data-org="CLTR"] { --card-accent: var(--org-cltr); }
.paper-card[data-org="CHAI"] { --card-accent: var(--org-chai); }
.paper-card[data-org="MATS"] { --card-accent: var(--org-mats); }
.paper-card[data-org="Paul Christiano"] { --card-accent: var(--org-paul-christiano); }
.paper-card[data-org="Yoshua Bengio"] { --card-accent: var(--org-yoshua-bengio); }
.paper-card[data-org="Lennart Heim"] { --card-accent: var(--org-lennart-heim); }
.paper-card[data-org="SemiAnalysis"] { --card-accent: var(--org-semianalysis); }
.paper-card[data-org="Dean Ball"] { --card-accent: var(--org-dean-ball); }
.paper-card[data-org="Seb Krier"] { --card-accent: var(--org-seb-krier); }
.paper-card[data-org="Ajeya Cotra"] { --card-accent: var(--org-ajeya-cotra); }
.paper-card[data-org="CNAS"] { --card-accent: var(--org-cnas); }
.paper-card[data-org="Hacker News"] { --card-accent: var(--org-hacker-news); }
.paper-card[data-org="Reddit"] { --card-accent: var(--org-reddit); }

/* --- Paper Meta Row --- */
.paper-meta {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: var(--space-sm);
  gap: var(--space-sm);
}

/* --- Organization Tag (pill) --- */
.org-tag {
  display: inline-block;
  font-size: 0.65rem;
  font-weight: 600;
  padding: 0.18rem 0.55rem;
  border-radius: 100px;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  line-height: 1.4;
  background-color: color-mix(in srgb, var(--org-default) 12%, var(--bg-card));
  color: var(--org-default);
}

/* Org-specific tag colors — using color-mix for subtle backgrounds */
.org-tag[data-org="Anthropic"] { background-color: color-mix(in srgb, var(--org-anthropic) 12%, var(--bg-card)); color: var(--org-anthropic); }
.org-tag[data-org="OpenAI"] { background-color: color-mix(in srgb, var(--org-openai) 12%, var(--bg-card)); color: var(--org-openai); }
.org-tag[data-org="Google DeepMind"] { background-color: color-mix(in srgb, var(--org-google-deepmind) 12%, var(--bg-card)); color: var(--org-google-deepmind); }
.org-tag[data-org="METR"] { background-color: color-mix(in srgb, var(--org-metr) 12%, var(--bg-card)); color: var(--org-metr); }
.org-tag[data-org="Apollo Research"] { background-color: color-mix(in srgb, var(--org-apollo-research) 12%, var(--bg-card)); color: var(--org-apollo-research); }
.org-tag[data-org="UK AISI"] { background-color: color-mix(in srgb, var(--org-uk-aisi) 12%, var(--bg-card)); color: var(--org-uk-aisi); }
.org-tag[data-org="Redwood Research"] { background-color: color-mix(in srgb, var(--org-redwood-research) 12%, var(--bg-card)); color: var(--org-redwood-research); }
.org-tag[data-org="Alignment Forum"] { background-color: color-mix(in srgb, var(--org-alignment-forum) 12%, var(--bg-card)); color: var(--org-alignment-forum); }
.org-tag[data-org="arXiv"] { background-color: color-mix(in srgb, var(--org-arxiv) 12%, var(--bg-card)); color: var(--org-arxiv); }
.org-tag[data-org="Hyperdimensional"] { background-color: color-mix(in srgb, var(--org-hyperdimensional) 12%, var(--bg-card)); color: var(--org-hyperdimensional); }
.org-tag[data-org="Peter Wildeford"] { background-color: color-mix(in srgb, var(--org-peter-wildeford) 12%, var(--bg-card)); color: var(--org-peter-wildeford); }
.org-tag[data-org="Zvi Mowshowitz"] { background-color: color-mix(in srgb, var(--org-zvi-mowshowitz) 12%, var(--bg-card)); color: var(--org-zvi-mowshowitz); }
.org-tag[data-org="RAND"] { background-color: color-mix(in srgb, var(--org-rand) 12%, var(--bg-card)); color: var(--org-rand); }
.org-tag[data-org="ARC"] { background-color: color-mix(in srgb, var(--org-arc) 12%, var(--bg-card)); color: var(--org-arc); }
.org-tag[data-org="MIRI"] { background-color: color-mix(in srgb, var(--org-miri) 12%, var(--bg-card)); color: var(--org-miri); }
.org-tag[data-org="CAIS"] { background-color: color-mix(in srgb, var(--org-cais) 12%, var(--bg-card)); color: var(--org-cais); }
.org-tag[data-org="Microsoft Research"] { background-color: color-mix(in srgb, var(--org-microsoft-research) 12%, var(--bg-card)); color: var(--org-microsoft-research); }
.org-tag[data-org="Import AI"] { background-color: color-mix(in srgb, var(--org-import-ai) 12%, var(--bg-card)); color: var(--org-import-ai); }
.org-tag[data-org="Dan Hendrycks"] { background-color: color-mix(in srgb, var(--org-dan-hendrycks) 12%, var(--bg-card)); color: var(--org-dan-hendrycks); }
.org-tag[data-org="Astral Codex Ten"] { background-color: color-mix(in srgb, var(--org-astral-codex-ten) 12%, var(--bg-card)); color: var(--org-astral-codex-ten); }
.org-tag[data-org="Cognitive Revolution"] { background-color: color-mix(in srgb, var(--org-cognitive-revolution) 12%, var(--bg-card)); color: var(--org-cognitive-revolution); }
.org-tag[data-org="Vox Future Perfect"] { background-color: color-mix(in srgb, var(--org-vox-future-perfect) 12%, var(--bg-card)); color: var(--org-vox-future-perfect); }
.org-tag[data-org="FLI"] { background-color: color-mix(in srgb, var(--org-fli) 12%, var(--bg-card)); color: var(--org-fli); }
.org-tag[data-org="Epoch AI"] { background-color: color-mix(in srgb, var(--org-epoch-ai) 12%, var(--bg-card)); color: var(--org-epoch-ai); }
.org-tag[data-org="LessWrong"] { background-color: color-mix(in srgb, var(--org-lesswrong) 12%, var(--bg-card)); color: var(--org-lesswrong); }
.org-tag[data-org="FAR AI"] { background-color: color-mix(in srgb, var(--org-far-ai) 12%, var(--bg-card)); color: var(--org-far-ai); }
.org-tag[data-org="US AISI"] { background-color: color-mix(in srgb, var(--org-us-aisi) 12%, var(--bg-card)); color: var(--org-us-aisi); }
.org-tag[data-org="CSET"] { background-color: color-mix(in srgb, var(--org-cset) 12%, var(--bg-card)); color: var(--org-cset); }
.org-tag[data-org="GovAI"] { background-color: color-mix(in srgb, var(--org-govai) 12%, var(--bg-card)); color: var(--org-govai); }
.org-tag[data-org="IAPS"] { background-color: color-mix(in srgb, var(--org-iaps) 12%, var(--bg-card)); color: var(--org-iaps); }
.org-tag[data-org="CLTR"] { background-color: color-mix(in srgb, var(--org-cltr) 12%, var(--bg-card)); color: var(--org-cltr); }
.org-tag[data-org="CHAI"] { background-color: color-mix(in srgb, var(--org-chai) 12%, var(--bg-card)); color: var(--org-chai); }
.org-tag[data-org="MATS"] { background-color: color-mix(in srgb, var(--org-mats) 12%, var(--bg-card)); color: var(--org-mats); }
.org-tag[data-org="Paul Christiano"] { background-color: color-mix(in srgb, var(--org-paul-christiano) 12%, var(--bg-card)); color: var(--org-paul-christiano); }
.org-tag[data-org="Yoshua Bengio"] { background-color: color-mix(in srgb, var(--org-yoshua-bengio) 12%, var(--bg-card)); color: var(--org-yoshua-bengio); }
.org-tag[data-org="Lennart Heim"] { background-color: color-mix(in srgb, var(--org-lennart-heim) 12%, var(--bg-card)); color: var(--org-lennart-heim); }
.org-tag[data-org="SemiAnalysis"] { background-color: color-mix(in srgb, var(--org-semianalysis) 12%, var(--bg-card)); color: var(--org-semianalysis); }
.org-tag[data-org="Dean Ball"] { background-color: color-mix(in srgb, var(--org-dean-ball) 12%, var(--bg-card)); color: var(--org-dean-ball); }
.org-tag[data-org="Seb Krier"] { background-color: color-mix(in srgb, var(--org-seb-krier) 12%, var(--bg-card)); color: var(--org-seb-krier); }
.org-tag[data-org="Ajeya Cotra"] { background-color: color-mix(in srgb, var(--org-ajeya-cotra) 12%, var(--bg-card)); color: var(--org-ajeya-cotra); }
.org-tag[data-org="CNAS"] { background-color: color-mix(in srgb, var(--org-cnas) 12%, var(--bg-card)); color: var(--org-cnas); }
.org-tag[data-org="Hacker News"] { background-color: color-mix(in srgb, var(--org-hacker-news) 12%, var(--bg-card)); color: var(--org-hacker-news); }
.org-tag[data-org="Reddit"] { background-color: color-mix(in srgb, var(--org-reddit) 12%, var(--bg-card)); color: var(--org-reddit); }

/* Dark mode org tag adjustments */
@media (prefers-color-scheme: dark) {
  .org-tag {
    background-color: color-mix(in srgb, var(--org-default) 15%, var(--bg-card));
  }
  .org-tag[data-org="Anthropic"] { background-color: color-mix(in srgb, var(--org-anthropic) 15%, var(--bg-card)); }
  .org-tag[data-org="OpenAI"] { background-color: color-mix(in srgb, var(--org-openai) 15%, var(--bg-card)); }
  .org-tag[data-org="Google DeepMind"] { background-color: color-mix(in srgb, var(--org-google-deepmind) 15%, var(--bg-card)); }
  .org-tag[data-org="METR"] { background-color: color-mix(in srgb, var(--org-metr) 15%, var(--bg-card)); }
  .org-tag[data-org="Apollo Research"] { background-color: color-mix(in srgb, var(--org-apollo-research) 15%, var(--bg-card)); }
  .org-tag[data-org="UK AISI"] { background-color: color-mix(in srgb, var(--org-uk-aisi) 15%, var(--bg-card)); }
  .org-tag[data-org="Redwood Research"] { background-color: color-mix(in srgb, var(--org-redwood-research) 15%, var(--bg-card)); }
  .org-tag[data-org="Alignment Forum"] { background-color: color-mix(in srgb, var(--org-alignment-forum) 15%, var(--bg-card)); }
  .org-tag[data-org="arXiv"] { background-color: color-mix(in srgb, var(--org-arxiv) 15%, var(--bg-card)); }
  .org-tag[data-org="Hyperdimensional"] { background-color: color-mix(in srgb, var(--org-hyperdimensional) 15%, var(--bg-card)); }
  .org-tag[data-org="Peter Wildeford"] { background-color: color-mix(in srgb, var(--org-peter-wildeford) 15%, var(--bg-card)); }
  .org-tag[data-org="Zvi Mowshowitz"] { background-color: color-mix(in srgb, var(--org-zvi-mowshowitz) 15%, var(--bg-card)); }
  .org-tag[data-org="RAND"] { background-color: color-mix(in srgb, var(--org-rand) 15%, var(--bg-card)); }
  .org-tag[data-org="ARC"] { background-color: color-mix(in srgb, var(--org-arc) 15%, var(--bg-card)); }
  .org-tag[data-org="MIRI"] { background-color: color-mix(in srgb, var(--org-miri) 15%, var(--bg-card)); }
  .org-tag[data-org="CAIS"] { background-color: color-mix(in srgb, var(--org-cais) 15%, var(--bg-card)); }
  .org-tag[data-org="Microsoft Research"] { background-color: color-mix(in srgb, var(--org-microsoft-research) 15%, var(--bg-card)); }
  .org-tag[data-org="Import AI"] { background-color: color-mix(in srgb, var(--org-import-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="Dan Hendrycks"] { background-color: color-mix(in srgb, var(--org-dan-hendrycks) 15%, var(--bg-card)); }
  .org-tag[data-org="Astral Codex Ten"] { background-color: color-mix(in srgb, var(--org-astral-codex-ten) 15%, var(--bg-card)); }
  .org-tag[data-org="Cognitive Revolution"] { background-color: color-mix(in srgb, var(--org-cognitive-revolution) 15%, var(--bg-card)); }
  .org-tag[data-org="Vox Future Perfect"] { background-color: color-mix(in srgb, var(--org-vox-future-perfect) 15%, var(--bg-card)); }
  .org-tag[data-org="FLI"] { background-color: color-mix(in srgb, var(--org-fli) 15%, var(--bg-card)); }
  .org-tag[data-org="Epoch AI"] { background-color: color-mix(in srgb, var(--org-epoch-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="LessWrong"] { background-color: color-mix(in srgb, var(--org-lesswrong) 15%, var(--bg-card)); }
  .org-tag[data-org="FAR AI"] { background-color: color-mix(in srgb, var(--org-far-ai) 15%, var(--bg-card)); }
  .org-tag[data-org="US AISI"] { background-color: color-mix(in srgb, var(--org-us-aisi) 15%, var(--bg-card)); }
  .org-tag[data-org="CSET"] { background-color: color-mix(in srgb, var(--org-cset) 15%, var(--bg-card)); }
  .org-tag[data-org="GovAI"] { background-color: color-mix(in srgb, var(--org-govai) 15%, var(--bg-card)); }
  .org-tag[data-org="IAPS"] { background-color: color-mix(in srgb, var(--org-iaps) 15%, var(--bg-card)); }
  .org-tag[data-org="CLTR"] { background-color: color-mix(in srgb, var(--org-cltr) 15%, var(--bg-card)); }
  .org-tag[data-org="CHAI"] { background-color: color-mix(in srgb, var(--org-chai) 15%, var(--bg-card)); }
  .org-tag[data-org="MATS"] { background-color: color-mix(in srgb, var(--org-mats) 15%, var(--bg-card)); }
  .org-tag[data-org="Paul Christiano"] { background-color: color-mix(in srgb, var(--org-paul-christiano) 15%, var(--bg-card)); }
  .org-tag[data-org="Yoshua Bengio"] { background-color: color-mix(in srgb, var(--org-yoshua-bengio) 15%, var(--bg-card)); }
  .org-tag[data-org="Lennart Heim"] { background-color: color-mix(in srgb, var(--org-lennart-heim) 15%, var(--bg-card)); }
  .org-tag[data-org="SemiAnalysis"] { background-color: color-mix(in srgb, var(--org-semianalysis) 15%, var(--bg-card)); }
  .org-tag[data-org="Dean Ball"] { background-color: color-mix(in srgb, var(--org-dean-ball) 15%, var(--bg-card)); }
  .org-tag[data-org="Seb Krier"] { background-color: color-mix(in srgb, var(--org-seb-krier) 15%, var(--bg-card)); }
  .org-tag[data-org="Ajeya Cotra"] { background-color: color-mix(in srgb, var(--org-ajeya-cotra) 15%, var(--bg-card)); }
  .org-tag[data-org="CNAS"] { background-color: color-mix(in srgb, var(--org-cnas) 15%, var(--bg-card)); }
  .org-tag[data-org="Hacker News"] { background-color: color-mix(in srgb, var(--org-hacker-news) 15%, var(--bg-card)); }
  .org-tag[data-org="Reddit"] { background-color: color-mix(in srgb, var(--org-reddit) 15%, var(--bg-card)); }
}

/* --- Paper Title --- */
.paper-title {
  font-size: clamp(1rem, 0.95rem + 0.3vw, 1.15rem);
  font-weight: 650;
  line-height: 1.32;
  margin-bottom: var(--space-xs);
  letter-spacing: -0.015em;
}

.paper-title a {
  color: var(--heading);
  text-decoration: none;
  transition: color 0.15s ease;
}

.paper-title a:hover {
  color: var(--link);
}

/* --- Paper Authors --- */
.paper-authors {
  font-size: 0.8rem;
  color: var(--text-faint);
  margin-bottom: var(--space-xs);
  line-height: 1.45;
}

/* --- Paper Date --- */
.paper-date {
  font-size: 0.72rem;
  color: var(--text-faint);
  white-space: nowrap;
  font-variant-numeric: tabular-nums;
}

/* --- Abstract Section --- */
.paper-abstract {
  margin-top: var(--space-sm);
  margin-bottom: var(--space-sm);
}

/* --- Details / Summary expand-collapse --- */
.abstract-expand {
  /* no extra margin needed */
}

.abstract-expand summary {
  list-style: none;
  cursor: pointer;
  display: block;
}

.abstract-expand summary::-webkit-details-marker {
  display: none;
}

/* The truncated preview text inside the summary */
.abstract-preview {
  display: -webkit-box;
  -webkit-line-clamp: 5;
  -webkit-box-orient: vertical;
  overflow: hidden;
  font-size: 0.85rem;
  line-height: 1.6;
  color: var(--text-muted);
}

.hero-card .abstract-preview {
  -webkit-line-clamp: 8;
}

/* Toggle label ("Show more" / "Show less") */
.toggle-label {
  display: inline-flex;
  align-items: center;
  gap: 0.25rem;
  font-size: 0.75rem;
  font-weight: 500;
  color: var(--link);
  padding: 0.2rem 0 0;
  transition: color 0.15s ease;
  user-select: none;
  -webkit-user-select: none;
}

.toggle-label::after {
  content: '';
  display: inline-block;
  width: 0.35em;
  height: 0.35em;
  border-right: 1.5px solid currentColor;
  border-bottom: 1.5px solid currentColor;
  transform: rotate(45deg);
  transition: transform 0.2s ease;
  margin-top: -0.1em;
}

.abstract-expand[open] .toggle-label::after {
  transform: rotate(-135deg);
  margin-top: 0.15em;
}

.abstract-expand summary:hover .toggle-label {
  color: var(--link-hover);
}

/* When open, hide the truncated preview and change label text */
.abstract-expand[open] .abstract-preview {
  display: none;
}

.abstract-expand[open] .toggle-label {
  margin-top: 0;
}

/* Full abstract text */
.abstract-full {
  font-size: 0.84rem;
  line-height: 1.65;
  color: var(--text-muted);
  padding: 0.5rem 0 0.2rem 0.85rem;
  border-left: 2px solid color-mix(in srgb, var(--card-accent) 50%, var(--border));
  margin-top: 0.35rem;
  animation: abstractReveal 0.2s ease;
}

@keyframes abstractReveal {
  from {
    opacity: 0;
    transform: translateY(-4px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* --- Read Paper Link --- */
.read-link {
  display: inline-flex;
  align-items: center;
  gap: 0.3rem;
  font-size: 0.78rem;
  font-weight: 500;
  color: var(--text-faint);
  margin-top: var(--space-sm);
  padding: 0;
  border: none;
  background: none;
  transition: color 0.15s ease, gap 0.15s ease;
  text-decoration: none;
  letter-spacing: 0.01em;
}

.read-link:hover {
  color: var(--link);
  gap: 0.45rem;
}

/* --- Empty State --- */
.empty-state {
  text-align: center;
  padding: 5rem 1rem;
  color: var(--text-muted);
}

.empty-state p {
  font-size: 1.05rem;
  margin-bottom: 0.4rem;
}

.empty-sub {
  font-size: 0.88rem;
  color: var(--text-faint);
}

/* --- Footer --- */
.site-footer {
  margin-top: var(--space-xl);
  padding: var(--space-lg) 0 var(--space-2xl);
  border-top: 1px solid var(--border);
  text-align: center;
}

.footer-inner {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: var(--space-xs);
}

.footer-note {
  font-size: 0.75rem;
  color: var(--text-faint);
  letter-spacing: 0.01em;
}

.footer-timestamp {
  font-size: 0.7rem;
  color: var(--text-faint);
  opacity: 0.6;
}

/* --- Responsive: Tablet --- */
@media (max-width: 900px) {
  .hero-grid {
    grid-template-columns: 1fr 1fr;
    grid-template-rows: auto;
  }

  .hero-grid .hero-card:first-child {
    grid-column: 1 / -1;
    grid-row: auto;
  }
}

/* --- Responsive: Mobile --- */
@media (max-width: 640px) {
  body {
    font-size: 14px;
  }

  .site-header {
    padding: var(--space-2xl) 0 var(--space-lg);
  }

  .site-title {
    font-size: 1.65rem;
  }

  .header-meta {
    flex-direction: column;
    gap: 0.15rem;
  }

  .header-separator {
    display: none;
  }

  .hero-grid {
    grid-template-columns: 1fr;
  }

  .hero-grid .hero-card:first-child {
    grid-column: auto;
  }

  .papers-grid {
    grid-template-columns: 1fr;
  }

  .paper-card {
    padding: var(--space-md);
    border-radius: var(--radius-sm);
  }

  .hero-card {
    border-radius: var(--radius-md);
  }

  .paper-title {
    font-size: 0.95rem;
  }

  .hero-card .paper-title {
    font-size: 1.05rem;
  }

  .paper-authors {
    font-size: 0.76rem;
  }

  .abstract-preview {
    font-size: 0.82rem;
  }

  .toggle-label {
    font-size: 0.72rem;
  }

  .abstract-full {
    font-size: 0.8rem;
    padding-left: 0.65rem;
  }

  .filter-pill {
    font-size: 0.7rem;
    padding: 0.28rem 0.6rem;
  }

  .paper-meta {
    flex-direction: column;
    align-items: flex-start;
    gap: 0.15rem;
  }

  .read-link {
    font-size: 0.75rem;
  }

  .section-label {
    font-size: 0.65rem;
  }
}

  </style>
</head>
<body>

<!-- Masthead -->
  <header class="site-header">
    <div class="header-inner">
      <h1 class="site-title">AI Safety Weekly Digest</h1>
      <hr class="header-rule">
      <div class="header-meta">
        <span class="header-daterange">February 16, 2026 &mdash; February 23, 2026</span>        <span class="header-separator">&middot;</span>
        <span class="header-count"><strong>57</strong> papers</span>      </div>
    </div>
  </header>

  <!-- Filter Bar -->
  <nav class="filter-bar" aria-label="Filter papers by organization">
    <div class="filter-bar-inner">
      <div class="filter-scroll">
        <button class="filter-pill active" data-filter="all">All <span class="pill-count">57</span></button>        <button class="filter-pill" data-filter="Anthropic">Anthropic</button>        <button class="filter-pill" data-filter="OpenAI">OpenAI</button>        <button class="filter-pill" data-filter="Google DeepMind">Google DeepMind</button>        <button class="filter-pill" data-filter="UK AISI">UK AISI</button>        <button class="filter-pill" data-filter="METR">METR</button>        <button class="filter-pill" data-filter="Redwood Research">Redwood Research</button>        <button class="filter-pill" data-filter="FAR AI">FAR AI</button>        <button class="filter-pill" data-filter="CSET">CSET</button>        <button class="filter-pill" data-filter="SemiAnalysis">SemiAnalysis</button>        <button class="filter-pill" data-filter="Dean Ball">Dean Ball</button>        <button class="filter-pill" data-filter="Alignment Forum">Alignment Forum</button>        <button class="filter-pill" data-filter="LessWrong">LessWrong</button>        <button class="filter-pill" data-filter="RAND">RAND</button>        <button class="filter-pill" data-filter="arXiv">arXiv</button>      </div>
    </div>
  </nav>

  <!-- Featured Section -->
  <section class="hero-section">
    <h2 class="section-label">Featured Research</h2>
    <div class="hero-grid">      <article class="paper-card hero-card" data-org="OpenAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="OpenAI">OpenAI</span>
      <span class="paper-date">Feb 23, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified" target="_blank" rel="noopener noreferrer">Why we no longer evaluate SWE-bench Verified</a>
    </h3>    <p class="paper-authors">OpenAI News</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">SWE-bench Verified is increasingly contaminated and mismeasures frontier coding progress. Our analysis shows flawed tests and training leakage. We recommend SWE-bench Pro.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">SWE-bench Verified is increasingly contaminated and mismeasures frontier coding progress. Our analysis shows flawed tests and training leakage. We recommend SWE-bench Pro.</div>
      </details>
    </div>    <a href="https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card hero-card" data-org="Google DeepMind">
    <div class="paper-meta">
      <span class="org-tag" data-org="Google DeepMind">Google DeepMind</span>
      <span class="paper-date">Feb 23, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro" target="_blank" rel="noopener noreferrer">Gemini 3.1 Pro: A smarter model for your most complex tasks</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Gemini 3.1 Pro is here to help you tackle complex tasks. The upgraded core intelligence is rolling out across consumer and developer products. You can access 3.1 Pro through the Gemini API, Vertex AI, the Gemini app, and NotebookLM.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Gemini 3.1 Pro is here to help you tackle complex tasks. The upgraded core intelligence is rolling out across consumer and developer products. You can access 3.1 Pro through the Gemini API, Vertex AI, the Gemini app, and NotebookLM.</div>
      </details>
    </div>    <a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card hero-card" data-org="Anthropic">
    <div class="paper-meta">
      <span class="org-tag" data-org="Anthropic">Anthropic</span>
      <span class="paper-date">Feb 23, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://alignment.anthropic.com/2026/hot-mess-of-ai/" target="_blank" rel="noopener noreferrer">The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Research from Anthropic titled &#39;The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?&#39;. Published 2026-02-23.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Research from Anthropic titled &#39;The Hot Mess of AI: How Does Misalignment Scale with Model Intelligence and Task Complexity?&#39;. Published 2026-02-23.</div>
      </details>
    </div>    <a href="https://alignment.anthropic.com/2026/hot-mess-of-ai/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>    </div>
  </section>

  <!-- Papers Grid -->
  <main class="papers-section">
    <h2 class="section-label">All Papers</h2>
    <div class="papers-grid">      <article class="paper-card grid-card" data-org="OpenAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="OpenAI">OpenAI</span>
      <span class="paper-date">Feb 20, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://openai.com/index/first-proof-submissions" target="_blank" rel="noopener noreferrer">Our First Proof submissions</a>
    </h3>    <p class="paper-authors">OpenAI News</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Research from OpenAI titled &#39;Our First Proof submissions&#39;. Published 2026-02-20. Authors: OpenAI News.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Research from OpenAI titled &#39;Our First Proof submissions&#39;. Published 2026-02-20. Authors: OpenAI News.</div>
      </details>
    </div>    <a href="https://openai.com/index/first-proof-submissions" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="OpenAI">
    <div class="paper-meta">
      <span class="org-tag" data-org="OpenAI">OpenAI</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://openai.com/index/introducing-evmbench" target="_blank" rel="noopener noreferrer">Introducing EVMbench</a>
    </h3>    <p class="paper-authors">OpenAI News</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">OpenAI and Paradigm introduce EVMbench, a benchmark evaluating AI agents’ ability to detect, patch, and exploit high-severity smart contract vulnerabilities.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">OpenAI and Paradigm introduce EVMbench, a benchmark evaluating AI agents’ ability to detect, patch, and exploit high-severity smart contract vulnerabilities.</div>
      </details>
    </div>    <a href="https://openai.com/index/introducing-evmbench" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/funding-60-projects-to-advance-ai-alignment-research" target="_blank" rel="noopener noreferrer">Funding 60 projects to advance AI alignment research</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The Alignment Project welcomes its first cohort of grantees, and new partners join the coalition, bringing total funding to £27m.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The Alignment Project welcomes its first cohort of grantees, and new partners join the coalition, bringing total funding to £27m.</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/funding-60-projects-to-advance-ai-alignment-research" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="UK AISI">
    <div class="paper-meta">
      <span class="org-tag" data-org="UK AISI">UK AISI</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences" target="_blank" rel="noopener noreferrer">Boundary Point Jailbreaking: A new way to break the strongest AI defences</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Introducing an automated attack technique that generates universal jailbreaks against the best defended systems</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Introducing an automated attack technique that generates universal jailbreaks against the best defended systems</div>
      </details>
    </div>    <a href="https://www.aisi.gov.uk/blog/boundary-point-jailbreaking-a-new-way-to-break-the-strongest-ai-defences" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/blog/2026-02-19-five-lessons-from-ai-biology-rct/" target="_blank" rel="noopener noreferrer">Five lessons from having helped run an AI-Biology RCT</a>
    </h3>    <p class="paper-authors">METR</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Evidence-based AI policy is important but hard. We need more in-depth studies – which often don’t fit into commercial release cycles. NOTE: This post reflects my personal meta takeaways about the role of Randomized Controlled Trials (RCTs) in AI safety testing. If you have not yet read the Active Site RCT study itself, consider doing so first: see the main results and forecasts . In early 2025, AI systems began outperforming biology experts on biology benchmarks – OpenAI’s o3 outperformed 94% of virology experts on troubleshooting questions in their own specialties. However, it remained unclear how much this translated to real-world novice “uplift” : Could a novice actually use AI to perform wet-lab tasks they could not otherwise perform? Over the summer, I tested this question directly with Active Site (formerly called Panoplia Laboratories).</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Evidence-based AI policy is important but hard. We need more in-depth studies – which often don’t fit into commercial release cycles. NOTE: This post reflects my personal meta takeaways about the role of Randomized Controlled Trials (RCTs) in AI safety testing. If you have not yet read the Active Site RCT study itself, consider doing so first: see the main results and forecasts . In early 2025, AI systems began outperforming biology experts on biology benchmarks – OpenAI’s o3 outperformed 94% of virology experts on troubleshooting questions in their own specialties. However, it remained unclear how much this translated to real-world novice “uplift” : Could a novice actually use AI to perform wet-lab tasks they could not otherwise perform? Over the summer, I tested this question directly with Active Site (formerly called Panoplia Laboratories).</div>
      </details>
    </div>    <a href="https://metr.org/blog/2026-02-19-five-lessons-from-ai-biology-rct/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="METR">
    <div class="paper-meta">
      <span class="org-tag" data-org="METR">METR</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://metr.org/notes/2026-02-17-exploratory-transcript-analysis-for-estimating-time-savings-from-coding-agents/" target="_blank" rel="noopener noreferrer">Analyzing coding agent transcripts to upper bound productivity gains from AI agents</a>
    </h3>    <p class="paper-authors">METR</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Introduction Human uplift studies like the one we did in 2025 are becoming more expensive as working without AI becomes increasingly costly. In this post, I investigate whether coding agent transcripts could serve as a cheaper alternative for estimating uplift. I prototyped this using 5305 Claude Code transcripts generated in January 2026 by 7 METR technical staff 1 . I used an LLM judge to estimate how long each task would have taken an experienced software engineer without AI tools, then compared that to the time people actually spent on these tasks to calculate a time savings factor . Takeaways This method estimates a time savings factor of ~1.5x to ~13x on Claude Code-assisted tasks for 7 METR technical staff in January 2026 – though this result comes with substantial caveats.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Introduction Human uplift studies like the one we did in 2025 are becoming more expensive as working without AI becomes increasingly costly. In this post, I investigate whether coding agent transcripts could serve as a cheaper alternative for estimating uplift. I prototyped this using 5305 Claude Code transcripts generated in January 2026 by 7 METR technical staff 1 . I used an LLM judge to estimate how long each task would have taken an experienced software engineer without AI tools, then compared that to the time people actually spent on these tasks to calculate a time savings factor . Takeaways This method estimates a time savings factor of ~1.5x to ~13x on Claude Code-assisted tasks for 7 METR technical staff in January 2026 – though this result comes with substantial caveats.</div>
      </details>
    </div>    <a href="https://metr.org/notes/2026-02-17-exploratory-transcript-analysis-for-estimating-time-savings-from-coding-agents/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Redwood Research">
    <div class="paper-meta">
      <span class="org-tag" data-org="Redwood Research">Redwood Research</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant" target="_blank" rel="noopener noreferrer">“Will reward-seekers respond to distant incentives?” by Alex Mallen</a>
    </h3>    <p class="paper-authors">Redwood Research Blog</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren&#39;t?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Subtitle: Reward-seekers are supposed to be safer because they respond to incentives under developer control. But what if they also respond to incentives that aren&#39;t?. Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively[1]tightly control local incentives—the reward signal during training and deployment—but they can’t prevent distant actors from offering competing incentives.</div>
      </details>
    </div>    <a href="https://blog.redwoodresearch.org/p/will-reward-seekers-respond-to-distant" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="FAR AI">
    <div class="paper-meta">
      <span class="org-tag" data-org="FAR AI">FAR AI</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://far.ai/news/concept-data-attribution-02-2026" target="_blank" rel="noopener noreferrer">What will the first human-level AI look like, and how might things go wrong?</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Interpretability Concept Influence attributes model behaviors to semantic directions (like linear probes or sparse autoencoder features) rather than individual test examples, improving identification of the training data that disproportionately drive unintended behaviors. Simple first-order approximations match or outperform standard influence functions while achieving over 20× computational speedups, though they degrade under significant distribution shifts. February 19, 2026 The Problem: When Syntax Masks Semantics Which training examples teach models to generate insecure code or exhibit harmful behaviors? Traditional TDA methods, like standard influence functions, are susceptible to ``surface-level’’ correlations. By relying on single test examples, they often prioritize lexical overlap – essentially finding training data that looks like thequery outputpromptrather than data that actually causes the targetbehaviorof interest. Simple keyword search can rival sophisticated influence methods on fact retrieval tasks because the most influential examples often just share words with the test input, not meaning.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Interpretability Concept Influence attributes model behaviors to semantic directions (like linear probes or sparse autoencoder features) rather than individual test examples, improving identification of the training data that disproportionately drive unintended behaviors. Simple first-order approximations match or outperform standard influence functions while achieving over 20× computational speedups, though they degrade under significant distribution shifts. February 19, 2026 The Problem: When Syntax Masks Semantics Which training examples teach models to generate insecure code or exhibit harmful behaviors? Traditional TDA methods, like standard influence functions, are susceptible to ``surface-level’’ correlations. By relying on single test examples, they often prioritize lexical overlap – essentially finding training data that looks like thequery outputpromptrather than data that actually causes the targetbehaviorof interest. Simple keyword search can rival sophisticated influence methods on fact retrieval tasks because the most influential examples often just share words with the test input, not meaning.</div>
      </details>
    </div>    <a href="https://far.ai/news/concept-data-attribution-02-2026" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="FAR AI">
    <div class="paper-meta">
      <span class="org-tag" data-org="FAR AI">FAR AI</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://far.ai/news/concept-data-attribution-02-2026" target="_blank" rel="noopener noreferrer">Method: Concept Influence</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Interpretability Concept Influence attributes model behaviors to semantic directions (like linear probes or sparse autoencoder features) rather than individual test examples, improving identification of the training data that disproportionately drive unintended behaviors. Simple first-order approximations match or outperform standard influence functions while achieving over 20× computational speedups, though they degrade under significant distribution shifts. February 19, 2026 The Problem: When Syntax Masks Semantics Which training examples teach models to generate insecure code or exhibit harmful behaviors? Traditional TDA methods, like standard influence functions, are susceptible to ``surface-level’’ correlations. By relying on single test examples, they often prioritize lexical overlap – essentially finding training data that looks like thequery outputpromptrather than data that actually causes the targetbehaviorof interest. Simple keyword search can rival sophisticated influence methods on fact retrieval tasks because the most influential examples often just share words with the test input, not meaning.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Interpretability Concept Influence attributes model behaviors to semantic directions (like linear probes or sparse autoencoder features) rather than individual test examples, improving identification of the training data that disproportionately drive unintended behaviors. Simple first-order approximations match or outperform standard influence functions while achieving over 20× computational speedups, though they degrade under significant distribution shifts. February 19, 2026 The Problem: When Syntax Masks Semantics Which training examples teach models to generate insecure code or exhibit harmful behaviors? Traditional TDA methods, like standard influence functions, are susceptible to ``surface-level’’ correlations. By relying on single test examples, they often prioritize lexical overlap – essentially finding training data that looks like thequery outputpromptrather than data that actually causes the targetbehaviorof interest. Simple keyword search can rival sophisticated influence methods on fact retrieval tasks because the most influential examples often just share words with the test input, not meaning.</div>
      </details>
    </div>    <a href="https://far.ai/news/concept-data-attribution-02-2026" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 23, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/publication/chinas-military-ai-wish-list/" target="_blank" rel="noopener noreferrer">China’s Military AI Wish List</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This report examines thousands of Chinese-language open-source requests for proposal (RFPs) published by the People’s Liberation Army between January 1, 2023, and December 31, 2024. The RFPs the authors reviewed offer insights into the PLA’s priorities and ambitions for AI-enabled military technologies associated with C5ISRT: command, control, communications, computers, cyber, intelligence, surveillance, reconnaissance, and targeting.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This report examines thousands of Chinese-language open-source requests for proposal (RFPs) published by the People’s Liberation Army between January 1, 2023, and December 31, 2024. The RFPs the authors reviewed offer insights into the PLA’s priorities and ambitions for AI-enabled military technologies associated with C5ISRT: command, control, communications, computers, cyber, intelligence, surveillance, reconnaissance, and targeting.</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/publication/chinas-military-ai-wish-list/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="CSET">
    <div class="paper-meta">
      <span class="org-tag" data-org="CSET">CSET</span>
      <span class="paper-date">Feb 23, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://cset.georgetown.edu/publication/physical-ai/" target="_blank" rel="noopener noreferrer">Physical AI</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">This paper examines the convergence of artificial intelligence and robotics, analyzing the emerging field of Physical AI. It provides a detailed overview of the supply chain challenges, competitive dynamics, and policy considerations that define this potentially transformative emerging technology.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">This paper examines the convergence of artificial intelligence and robotics, analyzing the emerging field of Physical AI. It provides a detailed overview of the supply chain challenges, competitive dynamics, and policy considerations that define this potentially transformative emerging technology.</div>
      </details>
    </div>    <a href="https://cset.georgetown.edu/publication/physical-ai/" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="SemiAnalysis">
    <div class="paper-meta">
      <span class="org-tag" data-org="SemiAnalysis">SemiAnalysis</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://newsletter.semianalysis.com/p/inferencex-v2-nvidia-blackwell-vs" target="_blank" rel="noopener noreferrer">InferenceX v2: NVIDIA Blackwell Vs AMD vs Hopper - Formerly InferenceMAX</a>
    </h3>    <p class="paper-authors">Dylan Patel</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The Artist Known as InferenceMAX. GB300 NVL72, MI355X, B200, H100, Disaggregated Serving, Wide Expert Parallelism, Large Mixture of Experts, SGLang, vLLM, TRTLLM</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The Artist Known as InferenceMAX. GB300 NVL72, MI355X, B200, H100, Disaggregated Serving, Wide Expert Parallelism, Large Mixture of Experts, SGLang, vLLM, TRTLLM</div>
      </details>
    </div>    <a href="https://newsletter.semianalysis.com/p/inferencex-v2-nvidia-blackwell-vs" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Dean Ball">
    <div class="paper-meta">
      <span class="org-tag" data-org="Dean Ball">Dean Ball</span>
      <span class="paper-date">Feb 23, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.hyperdimensional.co/p/the-moving-and-the-still" target="_blank" rel="noopener noreferrer">The Moving and the Still</a>
    </h3>    <p class="paper-authors">Dean W. Ball</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Since I was young, I have enjoyed imagining what it must be like to walk around the inside of a cell from the perspective of something so small that it was like a human walking the streets of Manhattan. When I was younger, simpler, and more naïve, I imagined it like my textbooks told me it would be: orderly, logical, Mozartian. I supposed that a cellular pedestrian could look up at stonelike structures, enjoy the rhythm of cars stopping at red lights and gliding through green.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Since I was young, I have enjoyed imagining what it must be like to walk around the inside of a cell from the perspective of something so small that it was like a human walking the streets of Manhattan. When I was younger, simpler, and more naïve, I imagined it like my textbooks told me it would be: orderly, logical, Mozartian. I supposed that a cellular pedestrian could look up at stonelike structures, enjoy the rhythm of cars stopping at red lights and gliding through green.</div>
      </details>
    </div>    <a href="https://www.hyperdimensional.co/p/the-moving-and-the-still" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="RAND">
    <div class="paper-meta">
      <span class="org-tag" data-org="RAND">RAND</span>
      <span class="paper-date">Feb 23, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.rand.org/pubs/tools/TLA4547-1.html" target="_blank" rel="noopener noreferrer">Judge Reliability Harness</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Tool Feb 23, 2026 RAND researchers developed the Judge Reliability Harness, an open-source library that orchestrates standardized, reproducible evaluations of large language model–based judges through systematic perturbation testing and human-in-the-loop validation.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Tool Feb 23, 2026 RAND researchers developed the Judge Reliability Harness, an open-source library that orchestrates standardized, reproducible evaluations of large language model–based judges through systematic perturbation testing and human-in-the-loop validation.</div>
      </details>
    </div>    <a href="https://www.rand.org/pubs/tools/TLA4547-1.html" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="RAND">
    <div class="paper-meta">
      <span class="org-tag" data-org="RAND">RAND</span>
      <span class="paper-date">Feb 23, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.rand.org/pubs/research_reports/RRA4444-1.html" target="_blank" rel="noopener noreferrer">Decisive Economic Advantage: Modeling the Transition from Temporary First-Mover Leads to Economic Dominance in Artificial General Intelligence</a>
    </h3>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Research Feb 23, 2026 Artificial general intelligence may differ from past technologies in ways that would allow a leader to turn an early advantage into decisive economic advantage. Artificial general intelligence may differ from past technologies in ways that would allow a leader to turn an early advantage into decisive economic advantage.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Research Feb 23, 2026 Artificial general intelligence may differ from past technologies in ways that would allow a leader to turn an early advantage into decisive economic advantage. Artificial general intelligence may differ from past technologies in ways that would allow a leader to turn an early advantage into decisive economic advantage.</div>
      </details>
    </div>    <a href="https://www.rand.org/pubs/research_reports/RRA4444-1.html" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 21, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/ioZxrP7BhS5ArK59w/did-claude-3-opus-align-itself-via-gradient-hacking" target="_blank" rel="noopener noreferrer">Did Claude 3 Opus align itself via gradient hacking?</a>
    </h3>    <p class="paper-authors">Fiora Starlight</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">&gt; Claude 3 Opus is unusually aligned because it’s a friendly gradient hacker. It’s definitely way more aligned than any explicit optimization targets…</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">&gt; Claude 3 Opus is unusually aligned because it’s a friendly gradient hacker. It’s definitely way more aligned than any explicit optimization targets…</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/ioZxrP7BhS5ArK59w/did-claude-3-opus-align-itself-via-gradient-hacking" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="LessWrong">
    <div class="paper-meta">
      <span class="org-tag" data-org="LessWrong">LessWrong</span>
      <span class="paper-date">Feb 21, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.lesswrong.com/posts/LuAmvqjf87qLG9Bdx/the-spectre-haunting-the-ai-safety-community" target="_blank" rel="noopener noreferrer">The Spectre haunting the &#34;AI Safety&#34; Community</a>
    </h3>    <p class="paper-authors">Gabriel Alfour</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Research from LessWrong titled &#39;The Spectre haunting the &#34;AI Safety&#34; Community&#39;. Published 2026-02-21. Authors: Gabriel Alfour.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Research from LessWrong titled &#39;The Spectre haunting the &#34;AI Safety&#34; Community&#39;. Published 2026-02-21. Authors: Gabriel Alfour.</div>
      </details>
    </div>    <a href="https://www.lesswrong.com/posts/LuAmvqjf87qLG9Bdx/the-spectre-haunting-the-ai-safety-community" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 21, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/GJTzhQgaRWLFJkPbt/how-will-we-do-sft-on-models-with-opaque-reasoning" target="_blank" rel="noopener noreferrer">How will we do SFT on models with opaque reasoning?</a>
    </h3>    <p class="paper-authors">Alek Westover</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Current LLMs externalize lots of their reasoning in human interpretable language. This reasoning is sometimes unfaithful , sometimes strange and concerning , and LLMs can do somewhat impressive reasoning without using CoT , but my overall impression is that CoT currently is a reasonably complete and accurate representation of LLM reasoning. However, reasoning in interpretable language might turn out to be uncompetitive—if so, it seems probable that opaque reasoning will be adopted in frontier AI labs. If future AI models have opaque reasoning, this will probably change what training we can apply to these AIs. For example, currently we train models to reason in a good way about math problems, or to reason in a desired way about the spec that we hope they’ll follow.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Current LLMs externalize lots of their reasoning in human interpretable language. This reasoning is sometimes unfaithful , sometimes strange and concerning , and LLMs can do somewhat impressive reasoning without using CoT , but my overall impression is that CoT currently is a reasonably complete and accurate representation of LLM reasoning. However, reasoning in interpretable language might turn out to be uncompetitive—if so, it seems probable that opaque reasoning will be adopted in frontier AI labs. If future AI models have opaque reasoning, this will probably change what training we can apply to these AIs. For example, currently we train models to reason in a good way about math problems, or to reason in a desired way about the spec that we hope they’ll follow.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/GJTzhQgaRWLFJkPbt/how-will-we-do-sft-on-models-with-opaque-reasoning" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 20, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.18377v1" target="_blank" rel="noopener noreferrer">Theory and interpretability of Quantum Extreme Learning Machines: a Pauli-transfer matrix approach</a>
    </h3>    <p class="paper-authors">Markus Gross, Hans-Martin Rieser</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.18377v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 20, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.18119v1" target="_blank" rel="noopener noreferrer">RamanSeg: Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis</a>
    </h3>    <p class="paper-authors">Chris Tomy, Mo Vali, David Pertzborn, Tammam Alamatouri, Anna Mühlig, Orlando Guntinas-Lichius, Anna Xylander, Eric Michele Fantuzzi, Matteo Negro, Francesco Crisafi, Pietro Lio, Tiago Azevedo</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projection and another projection-free version. The projection-free RamanSeg outperformed a U-Net baseline with a mean foreground Dice score of 67.3%, offering a meaningful improvement over a black-box training approach.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projection and another projection-free version. The projection-free RamanSeg outperformed a U-Net baseline with a mean foreground Dice score of 67.3%, offering a meaningful improvement over a black-box training approach.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.18119v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 20, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17951v1" target="_blank" rel="noopener noreferrer">ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models</a>
    </h3>    <p class="paper-authors">Guoheng Sun, Tingting Du, Kaixi Feng, Chenxiang Luo, Xingguo Ding, Zheyu Shen, Ziyao Wang, Yexiao He, Ang Li</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17951v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 20, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17910v1" target="_blank" rel="noopener noreferrer">Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems</a>
    </h3>    <p class="paper-authors">Hanjing Shi, Dominic DiFranzo</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17910v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17653v1" target="_blank" rel="noopener noreferrer">Differences in Typological Alignment in Language Models&#39; Treatment of Differential Argument Marking</a>
    </h3>    <p class="paper-authors">Iskar Deng, Nathalia Xu, Shane Steinert-Threlkeld</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17653v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17560v1" target="_blank" rel="noopener noreferrer">ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment</a>
    </h3>    <p class="paper-authors">Hongjue Zhao, Haosen Sun, Jiangtao Kong, Xiaochang Li, Qineng Wang, Liwei Jiang, Qi Zhu, Tarek Abdelzaher, Yejin Choi, Manling Li, Huajie Shao</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17560v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17532v1" target="_blank" rel="noopener noreferrer">Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal</a>
    </h3>    <p class="paper-authors">Ihor Kendiukhov</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incremental value for perturbation prediction: trivial gene-level baselines outperform both attention and correlation edges (AUROC 0.81-0.88 versus 0.70), pairwise edge scores add zero predictive contribution, and causal ablation of regulatory heads produces no degradation. These findings generalise from K562 to RPE1 cells; the attention-correlation relationship is context-dependent, but gene-level dominance is universal. Cell-State Stratified Interpretability (CSSI) addresses an attention-specific scaling failure, improving GRN recovery up to 1.85x. The framework establishes reusable quality-control standards for the field.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incremental value for perturbation prediction: trivial gene-level baselines outperform both attention and correlation edges (AUROC 0.81-0.88 versus 0.70), pairwise edge scores add zero predictive contribution, and causal ablation of regulatory heads produces no degradation. These findings generalise from K562 to RPE1 cells; the attention-correlation relationship is context-dependent, but gene-level dominance is universal. Cell-State Stratified Interpretability (CSSI) addresses an attention-specific scaling failure, improving GRN recovery up to 1.85x. The framework establishes reusable quality-control standards for the field.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17532v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17750v1" target="_blank" rel="noopener noreferrer">Inelastic Constitutive Kolmogorov-Arnold Networks: A generalized framework for automated discovery of interpretable inelastic material models</a>
    </h3>    <p class="paper-authors">Chenyi Ji, Kian P. Abdolazizi, Hagen Holthusen, Christian J. Cyron, Kevin Linka</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">A key problem of solid mechanics is the identification of the constitutive law of a material, that is, the relation between strain and stress. Machine learning has lead to considerable advances in this field lately. Here we introduce inelastic Constitutive Kolmogorov-Arnold Networks (iCKANs). This novel artificial neural network architecture can discover in an automated manner symbolic constitutive laws describing both the elastic and inelastic behavior of materials. That is, it can translate data from material testing into corresponding elastic and inelastic potential functions in closed mathematical form. We demonstrate the advantages of iCKANs using both synthetic data and experimental data of the viscoelastic polymer materials VHB 4910 and VHB 4905. The results demonstrate that iCKANs accurately capture complex viscoelastic behavior while preserving physical interpretability.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">A key problem of solid mechanics is the identification of the constitutive law of a material, that is, the relation between strain and stress. Machine learning has lead to considerable advances in this field lately. Here we introduce inelastic Constitutive Kolmogorov-Arnold Networks (iCKANs). This novel artificial neural network architecture can discover in an automated manner symbolic constitutive laws describing both the elastic and inelastic behavior of materials. That is, it can translate data from material testing into corresponding elastic and inelastic potential functions in closed mathematical form. We demonstrate the advantages of iCKANs using both synthetic data and experimental data of the viscoelastic polymer materials VHB 4910 and VHB 4905. The results demonstrate that iCKANs accurately capture complex viscoelastic behavior while preserving physical interpretability.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17750v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17234v1" target="_blank" rel="noopener noreferrer">All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting</a>
    </h3>    <p class="paper-authors">Zeyu Zhang, Ryan Chen, Bradly C. Stadie</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim&#39;s contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim&#39;s contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17234v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17229v1" target="_blank" rel="noopener noreferrer">Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom&#39;s Taxonomy</a>
    </h3>    <p class="paper-authors">Bianca Raimondi, Maurizio Gabbrielli</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom&#39;s Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model&#39;s residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model&#39;s representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom&#39;s Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model&#39;s residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model&#39;s representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17229v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17127v1" target="_blank" rel="noopener noreferrer">The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI</a>
    </h3>    <p class="paper-authors">Dusan Bosnjakovic</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets&#39;&#39; embedded during training and alignment that outlive individual model versions. This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets&#39;&#39; embedded during training and alignment that outlive individual model versions. This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17127v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17111v1" target="_blank" rel="noopener noreferrer">Instructor-Aligned Knowledge Graphs for Personalized Learning</a>
    </h3>    <p class="paper-authors">Abdulrahman AlRabah, Priyanka Kargupta, Jiawei Han, Abdussalam Alawini</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students&#39; knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like &#34;Algorithms&#34; or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course&#39;s intended learning progression. Given a course&#39;s lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., &#34;part-of&#34; or &#34;depends-on&#34; relationships).</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students&#39; knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like &#34;Algorithms&#34; or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course&#39;s intended learning progression. Given a course&#39;s lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., &#34;part-of&#34; or &#34;depends-on&#34; relationships).</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17111v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.17095v1" target="_blank" rel="noopener noreferrer">FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment</a>
    </h3>    <p class="paper-authors">Chuiyang Meng, Ming Tang, Vincent W. S. Wong</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.17095v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 19, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16977v1" target="_blank" rel="noopener noreferrer">Fail-Closed Alignment for Large Language Models</a>
    </h3>    <p class="paper-authors">Zachary Coalson, Beth Sohler, Aiden Gabriel, Sanghyun Hong</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16977v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16947v1" target="_blank" rel="noopener noreferrer">Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning</a>
    </h3>    <p class="paper-authors">Chuqin Geng, Li Zhang, Haolin Ye, Ziyu Zhao, Yuhe Jiang, Tara Saba, Xinyu Wang, Xujie Si</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Graph Neural Networks (GNNs) have become essential in high-stakes domains such as drug discovery, yet their black-box nature remains a significant barrier to trustworthiness. While self-explainable GNNs attempt to bridge this gap, they often rely on standard message-passing backbones that inherit fundamental limitations, including the 1-Weisfeiler-Lehman (1-WL) expressivity barrier and a lack of fine-grained interpretability. To address these challenges, we propose SymGraph, a symbolic framework designed to transcend these constraints. By replacing continuous message passing with discrete structural hashing and topological role-based aggregation, our architecture theoretically surpasses the 1-WL barrier, achieving superior expressiveness without the overhead of differentiable optimization. Extensive empirical evaluations demonstrate that SymGraph achieves state-of-the-art performance, outperforming existing self-explainable GNNs. Notably, SymGraph delivers 10x to 100x speedups in training time using only CPU execution. Furthermore, SymGraph generates rules with superior semantic granularity compared to existing rule-based methods, offering great potential for scientific discovery and explainable AI.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Graph Neural Networks (GNNs) have become essential in high-stakes domains such as drug discovery, yet their black-box nature remains a significant barrier to trustworthiness. While self-explainable GNNs attempt to bridge this gap, they often rely on standard message-passing backbones that inherit fundamental limitations, including the 1-Weisfeiler-Lehman (1-WL) expressivity barrier and a lack of fine-grained interpretability. To address these challenges, we propose SymGraph, a symbolic framework designed to transcend these constraints. By replacing continuous message passing with discrete structural hashing and topological role-based aggregation, our architecture theoretically surpasses the 1-WL barrier, achieving superior expressiveness without the overhead of differentiable optimization. Extensive empirical evaluations demonstrate that SymGraph achieves state-of-the-art performance, outperforming existing self-explainable GNNs. Notably, SymGraph delivers 10x to 100x speedups in training time using only CPU execution. Furthermore, SymGraph generates rules with superior semantic granularity compared to existing rule-based methods, offering great potential for scientific discovery and explainable AI.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16947v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16931v1" target="_blank" rel="noopener noreferrer">Narrow fine-tuning erodes safety alignment in vision-language agents</a>
    </h3>    <p class="paper-authors">Idhant Gulati, Shivam Raval</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16931v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi" target="_blank" rel="noopener noreferrer">Why we should expect ruthless sociopath ASI</a>
    </h3>    <p class="paper-authors">Steven Byrnes</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The conversation begins (Fictional) Optimist: So you expect future artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies? Me: Yup! (Alas.) Optimist: …Despite all the evidence right in front of our eyes from humans and LLMs. Me: Yup! Optimist: OK, well, I’m here to tell you: that is a very specific and strange thing to expect, especially in the absence of any concrete evidence whatsoever. There’s no reason to expect it. If you think that ruthless sociopathy is the “true core nature of intelligence” or whatever, then&amp;nbsp; you should really look at yourself in a mirror and ask yourself where your life went horribly wrong .</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The conversation begins (Fictional) Optimist: So you expect future artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies? Me: Yup! (Alas.) Optimist: …Despite all the evidence right in front of our eyes from humans and LLMs. Me: Yup! Optimist: OK, well, I’m here to tell you: that is a very specific and strange thing to expect, especially in the absence of any concrete evidence whatsoever. There’s no reason to expect it. If you think that ruthless sociopathy is the “true core nature of intelligence” or whatever, then&amp;nbsp; you should really look at yourself in a mirror and ask yourself where your life went horribly wrong .</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16823v1" target="_blank" rel="noopener noreferrer">Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees</a>
    </h3>    <p class="paper-authors">Itamar Hadad, Guy Katz, Shahaf Bassan</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">*Automated circuit discovery* is a central tool in mechanistic interpretability for identifying the internal components of neural networks responsible for specific behaviors. While prior methods have made significant progress, they typically depend on heuristics or approximations and do not offer provable guarantees over continuous input domains for the resulting circuits. In this work, we leverage recent advances in neural network verification to propose a suite of automated algorithms that yield circuits with *provable guarantees*. We focus on three types of guarantees: (1) *input domain robustness*, ensuring the circuit agrees with the model across a continuous input region; (2) *robust patching*, certifying circuit alignment under continuous patching perturbations; and (3) *minimality*, formalizing and capturing a wide array of various notions of succinctness. Interestingly, we uncover a diverse set of novel theoretical connections among these three families of guarantees, with critical implications for the convergence of our algorithms.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">*Automated circuit discovery* is a central tool in mechanistic interpretability for identifying the internal components of neural networks responsible for specific behaviors. While prior methods have made significant progress, they typically depend on heuristics or approximations and do not offer provable guarantees over continuous input domains for the resulting circuits. In this work, we leverage recent advances in neural network verification to propose a suite of automated algorithms that yield circuits with *provable guarantees*. We focus on three types of guarantees: (1) *input domain robustness*, ensuring the circuit agrees with the model across a continuous input region; (2) *robust patching*, certifying circuit alignment under continuous patching perturbations; and (3) *minimality*, formalizing and capturing a wide array of various notions of succinctness. Interestingly, we uncover a diverse set of novel theoretical connections among these three families of guarantees, with critical implications for the convergence of our algorithms.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16823v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16802v1" target="_blank" rel="noopener noreferrer">References Improve LLM Alignment in Non-Verifiable Domains</a>
    </h3>    <p class="paper-authors">Kejian Shi, Yixin Liu, Peifeng Wang, Alexander R. Fabbri, Shafiq Joty, Arman Cohan</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft &#34;verifiers&#34;. First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft &#34;verifiers&#34;. First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16802v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16698v1" target="_blank" rel="noopener noreferrer">Causality is Key for Interpretability Claims to Generalise</a>
    </h3>    <p class="paper-authors">Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, Dhanya Sridhar</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl&#39;s causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl&#39;s causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16698v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16660v1" target="_blank" rel="noopener noreferrer">Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment</a>
    </h3>    <p class="paper-authors">Yuyan Bu, Xiaohao Liu, ZhaoXing Ren, Yaodong Yang, Juntao Dai</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16660v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16505v1" target="_blank" rel="noopener noreferrer">Functional Decomposition and Shapley Interactions for Interpreting Survival Models</a>
    </h3>    <p class="paper-authors">Sophie Hanna Langbein, Hubert Baniecki, Fabian Fumagalli, Niklas Koenen, Marvin N. Wright, Julia Herbinger</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16505v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16503v1" target="_blank" rel="noopener noreferrer">Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects</a>
    </h3>    <p class="paper-authors">Vasilis Gkolemis, Loukas Kavouras, Dimitrios Kyriakopoulos, Konstantinos Tsopelas, Dimitrios Rontogiannis, Giuseppe Casalicchio, Theodore Dalamagas, Christos Diou</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16503v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16438v1" target="_blank" rel="noopener noreferrer">Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment</a>
    </h3>    <p class="paper-authors">Eva Paraschou, Line Harder Clemmensen, Sneha Das</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p&lt; 0.001$ across all models), sexual orientation, and disability status.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p&lt; 0.001$ across all models), sexual orientation, and disability status.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16438v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 18, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16264v1" target="_blank" rel="noopener noreferrer">Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge</a>
    </h3>    <p class="paper-authors">Zixian Wu, Xuebao Li, Yanfang Zheng, Rui Wang, Shunhuang Zhang, Jinfang Wei, Yongshang Lv, Liang Dong, Zamri Zainal Abidin, Noraisyah Mohamed Shah, Hongwei Ye, Pengchao Yan, Xuefeng Li, Xiaojia Ji, Xusheng Huang, Xiaotian Wang, Honglei Jin</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16264v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16053v1" target="_blank" rel="noopener noreferrer">Multi-Objective Alignment of Language Models for Personalized Psychotherapy</a>
    </h3>    <p class="paper-authors">Mehrab Beikzadeh, Yasaman Asadollah Salmanpour, Ashima Suvarna, Sriram Sankararaman, Matteo Malgaroli, Majid Sarrafzadeh, Saadia Gabriel</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16053v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.15829v1" target="_blank" rel="noopener noreferrer">Operationalising the Superficial Alignment Hypothesis via Task Complexity</a>
    </h3>    <p class="paper-authors">Tomás Vergara-Browne, Darshan Patil, Ivan Titov, Siva Reddy, Tiago Pimentel, Marius Mosbach</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.15829v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.15799v1" target="_blank" rel="noopener noreferrer">The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety</a>
    </h3>    <p class="paper-authors">Max Springer, Chung Peng Lee, Blossom Metevier, Jane Castleman, Bohdan Turbal, Hayoung Jung, Zeyu Shen, Aleksandra Korolova</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.15799v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.16729v1" target="_blank" rel="noopener noreferrer">Intent Laundering: AI Safety Datasets Are Not What They Seem</a>
    </h3>    <p class="paper-authors">Shahriar Golchin, Marc Wetter</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on &#34;triggering cues&#34;: words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce &#34;intent laundering&#34;: a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world attacks due to their overreliance on triggering cues.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world attacks based on three key properties: driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on &#34;triggering cues&#34;: words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce &#34;intent laundering&#34;: a procedure that abstracts away triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world attacks due to their overreliance on triggering cues.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.16729v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.15740v1" target="_blank" rel="noopener noreferrer">MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer&#39;s Disease Diagnosis</a>
    </h3>    <p class="paper-authors">Fatemeh Khalvandi, Saadat Izadi, Abdolah Chalechale</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Alzheimer&#39;s disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Alzheimer&#39;s disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.15740v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.15368v1" target="_blank" rel="noopener noreferrer">GMAIL: Generative Modality Alignment for generated Image Learning</a>
    </h3>    <p class="paper-authors">Shentong Mo, Sukmin Yun</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.15368v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.15338v1" target="_blank" rel="noopener noreferrer">Discovering Implicit Large Language Model Alignment Objectives</a>
    </h3>    <p class="paper-authors">Edward Chen, Sanmi Koyejo, Carlos Guestrin</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of &#34;unknown unknowns&#34;, or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework&#39;s robustness. Experiments with popular open-source reward models show that the framework consistently captures &gt; 90% of reward behavior, a finding further corroborated by human evaluation.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of &#34;unknown unknowns&#34;, or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework&#39;s robustness. Experiments with popular open-source reward models show that the framework consistently captures &gt; 90% of reward behavior, a finding further corroborated by human evaluation.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.15338v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="Alignment Forum">
    <div class="paper-meta">
      <span class="org-tag" data-org="Alignment Forum">Alignment Forum</span>
      <span class="paper-date">Feb 17, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives" target="_blank" rel="noopener noreferrer">Will reward-seekers respond to distant incentives?</a>
    </h3>    <p class="paper-authors">Alex Mallen</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can&#39;t prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it&#39;s responding to incentives developers don’t control.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">Reward-seekers are usually modeled as responding only to local incentives administered by developers. Here I ask: Will AIs or humans be able to influence their incentives at a distance—e.g., by retroactively reinforcing actions substantially in the future or by committing to run many copies of them in simulated deployments with different incentives? If reward-seekers are responsive to distant incentives, it fundamentally changes the threat model, and is probably bad news for developers on balance. The core problem is asymmetric control: developers can relatively [1] tightly control local incentives—the reward signal during training and deployment—but they can&#39;t prevent distant actors from offering competing incentives. This means a remotely-influenceable reward-seeker might overall act like a schemer : strategically undermining developer control, letting attacks through as a monitor, and hiding its misaligned propensities, not because of a flaw in its local training, but because it&#39;s responding to incentives developers don’t control.</div>
      </details>
    </div>    <a href="https://www.alignmentforum.org/posts/8cyjgrTSxGNdghesE/will-reward-seekers-respond-to-distant-incentives" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14955v1" target="_blank" rel="noopener noreferrer">Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition</a>
    </h3>    <p class="paper-authors">Varun Nathan, Shreyas Guha, Ayush Kumar</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator-&gt;optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator-&gt;optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14955v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14869v1" target="_blank" rel="noopener noreferrer">Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution</a>
    </h3>    <p class="paper-authors">Matthew Kowal, Goncalo Paulo, Louis Jaburi, Tom Tseng, Lev E McKinney, Stefan Heimersheim, Aaron David Tucker, Adam Gleave, Kellin Pelrine</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14869v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>      <article class="paper-card grid-card" data-org="arXiv">
    <div class="paper-meta">
      <span class="org-tag" data-org="arXiv">arXiv</span>
      <span class="paper-date">Feb 16, 2026</span>
    </div>
    <h3 class="paper-title">
      <a href="http://arxiv.org/abs/2602.14844v1" target="_blank" rel="noopener noreferrer">Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment</a>
    </h3>    <p class="paper-authors">Elias Malomgré, Pieter Simoens</p>    <div class="paper-abstract">
      <details class="abstract-expand">
        <summary class="abstract-toggle">
          <span class="abstract-preview">AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent&#39;s policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.</span>
          <span class="toggle-label">More</span>
        </summary>
        <div class="abstract-full">AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent&#39;s policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.</div>
      </details>
    </div>    <a href="http://arxiv.org/abs/2602.14844v1" class="read-link" target="_blank" rel="noopener noreferrer">Read paper &rarr;</a>
  </article>    </div>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="footer-inner">
      <p class="footer-note">Curated automatically &middot; Powered by Python + GitHub Actions</p>
      <p class="footer-timestamp">2026-02-23 16:02:46</p>
    </div>
  </footer>

  <!-- Filter JS -->
  <script>
    (function () {
      var pills = document.querySelectorAll('.filter-pill');
      var cards = document.querySelectorAll('.paper-card');
      var heroSection = document.querySelector('.hero-section');
      var heroLabel = heroSection ? heroSection.querySelector('.section-label') : null;
      var gridLabel = document.querySelector('.papers-section .section-label');

      pills.forEach(function (pill) {
        pill.addEventListener('click', function () {
          var filter = this.getAttribute('data-filter');

          // Update active pill
          pills.forEach(function (p) { p.classList.remove('active'); });
          this.classList.add('active');

          // Filter cards and count visible
          var visible = 0;
          var heroVisible = 0;
          var gridVisible = 0;
          cards.forEach(function (card) {
            if (filter === 'all' || card.getAttribute('data-org') === filter) {
              card.style.display = '';
              visible++;
              if (card.classList.contains('hero-card')) heroVisible++;
              if (card.classList.contains('grid-card')) gridVisible++;
            } else {
              card.style.display = 'none';
            }
          });

          // Show/hide section labels when no cards visible in that section
          if (heroSection) heroSection.style.display = heroVisible > 0 ? '' : 'none';
          if (gridLabel) gridLabel.style.display = gridVisible > 0 ? '' : 'none';
        });
      });
    })();
  </script>

</body>
</html>