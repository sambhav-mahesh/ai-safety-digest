"""Scrape research pages for papers and articles using requests + BeautifulSoup."""

from __future__ import annotations

import logging
import re
from datetime import datetime, timezone
from typing import Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

from scripts.models import Paper

logger = logging.getLogger(__name__)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (compatible; AISafetyDigestBot/1.0; "
        "+https://github.com/ai-safety-digest)"
    ),
    "Accept": "text/html,application/xhtml+xml",
}

REQUEST_TIMEOUT = 15  # seconds

# Regex for dates like "2026-01-15", "Jan 15, 2026", "15 January 2026", etc.
DATE_PATTERNS = [
    # ISO-style
    re.compile(r"\b(\d{4}-\d{2}-\d{2})\b"),
    # "January 15, 2026" or "January 15 2026"
    re.compile(
        r"\b((?:January|February|March|April|May|June|July|August|September|"
        r"October|November|December)\s+\d{1,2},?\s+\d{4})\b",
        re.IGNORECASE,
    ),
    # "Jan 15, 2026"
    re.compile(
        r"\b((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)"
        r"\s+\d{1,2},?\s+\d{4})\b",
        re.IGNORECASE,
    ),
    # "15 January 2026"
    re.compile(
        r"\b(\d{1,2}\s+(?:January|February|March|April|May|June|July|"
        r"August|September|October|November|December)\s+\d{4})\b",
        re.IGNORECASE,
    ),
    # "February 2026" (month + year only, no day)
    re.compile(
        r"\b((?:January|February|March|April|May|June|July|August|September|"
        r"October|November|December)\s+\d{4})\b",
        re.IGNORECASE,
    ),
]

DATE_FORMATS = [
    "%Y-%m-%d",
    "%B %d, %Y",
    "%B %d %Y",
    "%b %d, %Y",
    "%b %d %Y",
    "%d %B %Y",
    "%B %Y",
]


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _parse_date_string(text: str) -> Optional[str]:
    """
    Attempt to extract and parse a date from an arbitrary string.

    Returns an ISO-formatted date string on success, or None.
    """
    for pattern in DATE_PATTERNS:
        match = pattern.search(text)
        if match:
            date_str = match.group(1)
            for fmt in DATE_FORMATS:
                try:
                    dt = datetime.strptime(date_str, fmt).replace(
                        tzinfo=timezone.utc
                    )
                    # For month-only dates (e.g. "February 2026"), use the
                    # current day-of-month if it's the current month so papers
                    # aren't excluded by the 7-day filter late in the month.
                    if fmt == "%B %Y":
                        now = datetime.now(timezone.utc)
                        if dt.year == now.year and dt.month == now.month:
                            dt = dt.replace(day=now.day)
                        else:
                            dt = dt.replace(day=15)
                    return dt.isoformat()
                except ValueError:
                    continue
    return None


def _extract_date(element) -> Optional[str]:
    """
    Try to find a date associated with a DOM element.

    Checks <time> tags first, then searches visible text for date patterns.
    """
    # <time datetime="...">
    time_tag = element.find("time")
    if time_tag:
        dt_attr = time_tag.get("datetime", "")
        if dt_attr:
            parsed = _parse_date_string(dt_attr)
            if parsed:
                return parsed
        # Try the text content of <time>
        parsed = _parse_date_string(time_tag.get_text(strip=True))
        if parsed:
            return parsed

    # Try short metadata elements (spans, small tags) before full-text scan.
    # This avoids picking up dates from paragraph content (e.g. "between
    # January 1, 2023 and...") instead of the actual publication date.
    for tag in element.find_all(["span", "small"]):
        tag_text = tag.get_text(strip=True)
        if tag_text and len(tag_text) < 60:
            parsed = _parse_date_string(tag_text)
            if parsed:
                return parsed

    # Fall back to scanning the element text
    text = element.get_text(" ", strip=True)
    parsed = _parse_date_string(text)
    if parsed:
        return parsed

    # Check previous siblings for date (common pattern: date div before article links)
    for sibling in element.previous_siblings:
        if hasattr(sibling, "get_text"):
            sib_text = sibling.get_text(strip=True)
            if sib_text:
                parsed = _parse_date_string(sib_text)
                if parsed:
                    return parsed
                # Stop after checking a few siblings to avoid scanning the whole page
                break

    return None


def _strip_list_numbering(title: str) -> str:
    """Strip leading list-numbering patterns like '1: ', '2. ' from titles."""
    return re.sub(r"^\d+[\.:]\s*", "", title)


def _extract_title(element) -> str:
    """Extract a title from headings or the first prominent link."""
    for tag in ("h1", "h2", "h3", "h4"):
        heading = element.find(tag)
        if heading:
            return _strip_list_numbering(heading.get_text(strip=True))

    # Try the first link text
    link = element.find("a")
    if link:
        text = link.get_text(strip=True)
        if text:
            return _strip_list_numbering(text)

    return ""


def _extract_abstract(element) -> str:
    """Extract a description from <p> tags within the element."""
    paragraphs = element.find_all("p")
    texts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
    return " ".join(texts) if texts else ""


def _extract_link(element, base_url: str) -> str:
    """Extract the most relevant href from an element."""
    # If the element itself is an <a> tag, use its href directly
    if element.name == "a" and element.get("href"):
        return urljoin(base_url, element["href"])
    link = element.find("a", href=True)
    if link:
        href = link["href"]
        return urljoin(base_url, href)
    return base_url


ARTICLE_CLASSES = re.compile(
    r"post|card|entry|research|publication|article|blog|item|teaser", re.IGNORECASE
)

# Titles that are obviously not papers/articles
_JUNK_TITLES = {
    "home", "about", "about us", "careers", "donate", "contact", "team",
    "privacy", "privacy policy", "transparency", "news", "blog", "research",
    "events", "programs", "faq", "alumni", "save & accept", "necessary",
    "privacy overview", "policies", "work", "publications",
}

# Minimum title length to avoid single-word nav items
_MIN_TITLE_LENGTH = 10

# Maximum title length — titles beyond this are likely garbled (concatenated
# heading + date + subtitle from scraper extraction)
_MAX_TITLE_LENGTH = 150


def _is_junk_title(title: str) -> bool:
    """Return True if the title looks like navigation / non-paper content."""
    stripped = title.strip().rstrip(".")
    if stripped.lower() in _JUNK_TITLES:
        return True
    if len(stripped) < _MIN_TITLE_LENGTH:
        return True
    if len(stripped) > _MAX_TITLE_LENGTH:
        return True
    # Reject titles that are just domain names or URLs
    if stripped.startswith("http") or ".gov" in stripped or ".com" in stripped:
        return True
    # Reject garbled titles where text nodes were concatenated without spaces
    # e.g. "Some Title17 February 2026Some subtitleRead more"
    if re.search(r"20\d{2}[A-Z]", stripped):
        return True
    if re.search(r"[Rr]ead more\s*$", stripped):
        return True
    return False


def _find_article_elements(soup: BeautifulSoup) -> list:
    """
    Locate article-like DOM elements using multiple heuristics.

    Returns a list of BeautifulSoup Tag objects.
    """
    # 1. Explicit <article> tags
    articles = soup.find_all("article")
    if articles:
        return articles

    # 2. Divs / sections / list-items whose class matches typical patterns
    candidates = soup.find_all(
        ["div", "section", "li", "a"],
        class_=ARTICLE_CLASSES,
    )
    if candidates:
        return candidates

    # 3. <a> tags that contain headings — common pattern for article cards
    heading_links = [
        a for a in soup.find_all("a", href=True)
        if a.find(["h2", "h3", "h4"])
    ]
    if heading_links:
        return heading_links

    # 4. Links inside known container ids / classes
    for container_attr in ("main", "content", "posts", "articles", "blog"):
        container = soup.find(id=re.compile(container_attr, re.IGNORECASE))
        if container is None:
            container = soup.find(
                class_=re.compile(container_attr, re.IGNORECASE)
            )
        if container:
            links = container.find_all("a", href=True)
            if links:
                return links

    return []


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------

def fetch_scraped(scrapers_config: list[dict]) -> list[Paper]:
    """
    Scrape a list of research pages for articles.

    Parameters
    ----------
    scrapers_config : list[dict]
        Each dict must contain ``url`` and ``org``, and optionally ``name``.
        Optional ``link_must_contain`` restricts results to links whose URL
        contains the given substring.

    Returns
    -------
    list[Paper]
    """
    papers: list[Paper] = []

    for site_cfg in scrapers_config:
        site_url = site_cfg.get("url", "")
        org = site_cfg.get("org", "Unknown")
        site_name = site_cfg.get("name", site_url)
        link_must_contain = site_cfg.get("link_must_contain", "")
        keywords = [k.lower() for k in site_cfg.get("keywords", [])]
        article_class = site_cfg.get("article_class", "")

        logger.info("Scraping: %s (%s)", site_name, site_url)

        try:
            response = requests.get(
                site_url, headers=HEADERS, timeout=REQUEST_TIMEOUT
            )
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            if article_class:
                elements = soup.find_all(class_=article_class)
            else:
                elements = _find_article_elements(soup)

            if not elements:
                logger.info("No article elements found on %s", site_name)
                continue

            site_count = 0
            for elem in elements:
                title = _extract_title(elem)
                if not title:
                    continue

                # Filter obvious non-paper content
                if _is_junk_title(title):
                    continue

                link = _extract_link(elem, site_url)

                # Apply URL filter if configured
                if link_must_contain and link_must_contain not in link:
                    continue

                # Skip links that point back to the exact page we're scraping
                if link.rstrip("/") == site_url.rstrip("/"):
                    continue

                abstract = _extract_abstract(elem)
                pub_date = _extract_date(elem)

                # Skip papers without a parseable date — they are likely
                # old papers from a research listing page.
                if pub_date is None:
                    continue

                # Keyword filter: if configured, skip items that don't
                # match any keyword in title + abstract
                if keywords:
                    searchable = (title + " " + abstract).lower()
                    if not any(kw in searchable for kw in keywords):
                        continue

                papers.append(
                    Paper(
                        title=title,
                        authors=[],
                        organization=org,
                        abstract=abstract,
                        url=link,
                        published_date=pub_date,
                        source_type="scrape",
                        source_url=site_url,
                    )
                )
                site_count += 1

            logger.info(
                "Scraper %s: found %d items from %d elements",
                site_name,
                site_count,
                len(elements),
            )

        except Exception:
            logger.warning("Failed to scrape %s", site_name, exc_info=True)
            continue

    logger.info("Scraper total: %d papers collected", len(papers))
    return papers
